{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is enabled)\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST example\n",
    "\n",
    "We will now build a convolutional network for the MNIST data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}))\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 25))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 100))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 1, 28, 28) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 1, 28, 28) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4('X')\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix('Y', dtype='uint8')\n",
    "\n",
    "#The tag values are useful during debugging the creation of Theano graphs\n",
    "\n",
    "X_test_value, Y_test_value = next(mnist_train_stream.get_epoch_iterator())\n",
    "#\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "#\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values\n",
    "\n",
    "\n",
    "X.tag.test_value = X_test_value[:3]\n",
    "Y.tag.test_value = Y_test_value[:3]\n",
    "\n",
    "print \"X shape: %s\" % (X.tag.test_value.shape,)\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "#The first convolutional layer\n",
    "#The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = 10 #we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1,1,5,5), dtype='float32'),\n",
    "                   name='CW1')\n",
    "#please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype='float32'),\n",
    "                    name='CB1')\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2,2), ignore_border=True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_filters_2 = 25 #we will compute ten convolution filters in the first layer\n",
    "CW2 = theano.shared(np.zeros((num_filters_2,num_filters_1,5,5), dtype='float32'),\n",
    "                   name='CW2')\n",
    "CW2.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype='float32'),\n",
    "                    name='CB2')\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(\n",
    "    0.0,\n",
    "    theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle('x',0,'x','x')\n",
    "    )\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2,2), ignore_border=True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "#Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden=500\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 4 * 4, num_fw3_hidden), dtype='float32'),\n",
    "                   name='FW3')\n",
    "FW3.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype='float32'),\n",
    "                    name='FB3')\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle('x',0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_fw4_hidden=10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype='float32'),\n",
    "                   name='FW4')\n",
    "FW4.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype='float32'),\n",
    "                    name='FB4')\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle('x',0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis=1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions,Y.ravel()).mean()\n",
    "nll = - theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1]=='W':\n",
    "        weight_decay = weight_decay + 1e-3 * (p**2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "#At this point stop computing test values\n",
    "theano.config.compute_test_value = 'off' # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "#\n",
    "# svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar('lrate',dtype='float32')\n",
    "momentum = theano.tensor.scalar('momentum',dtype='float32')\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name='V_%s' %(p.name, )) for p in model_parameters]\n",
    "\n",
    "for p,g,v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v,v_new), (p, p_new)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile theano functions\n",
    "\n",
    "#each call to train step will make one SGD step\n",
    "train_step = theano.function([X,Y,lrate,momentum],[cost, error_rate, nll, weight_decay],updates=updates)\n",
    "#each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X)!=Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs/num_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow=False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i=0\n",
    "e=0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 1.546369, batch nll 1.016520, batch error rate 32.000000%\n",
      "At minibatch 200, batch loss 1.207390, batch nll 0.681141, batch error rate 20.000000%\n",
      "At minibatch 300, batch loss 0.670582, batch nll 0.149439, batch error rate 4.000000%\n",
      "At minibatch 400, batch loss 0.648750, batch nll 0.133522, batch error rate 8.000000%\n",
      "At minibatch 500, batch loss 0.565698, batch nll 0.056337, batch error rate 0.000000%\n",
      "At minibatch 600, batch loss 0.559566, batch nll 0.056646, batch error rate 4.000000%\n",
      "At minibatch 700, batch loss 0.507729, batch nll 0.010957, batch error rate 0.000000%\n",
      "At minibatch 800, batch loss 0.557928, batch nll 0.067653, batch error rate 0.000000%\n",
      "At minibatch 900, batch loss 0.505403, batch nll 0.021722, batch error rate 0.000000%\n",
      "At minibatch 1000, batch loss 0.528816, batch nll 0.051599, batch error rate 0.000000%\n",
      "At minibatch 1100, batch loss 0.500660, batch nll 0.029455, batch error rate 0.000000%\n",
      "At minibatch 1200, batch loss 0.515430, batch nll 0.050542, batch error rate 0.000000%\n",
      "At minibatch 1300, batch loss 0.524578, batch nll 0.065865, batch error rate 4.000000%\n",
      "At minibatch 1400, batch loss 0.483790, batch nll 0.031002, batch error rate 0.000000%\n",
      "At minibatch 1500, batch loss 0.459200, batch nll 0.012467, batch error rate 0.000000%\n",
      "At minibatch 1600, batch loss 0.447290, batch nll 0.006632, batch error rate 0.000000%\n",
      "At minibatch 1700, batch loss 0.457805, batch nll 0.023105, batch error rate 0.000000%\n",
      "At minibatch 1800, batch loss 0.465583, batch nll 0.036822, batch error rate 0.000000%\n",
      "At minibatch 1900, batch loss 0.463298, batch nll 0.040108, batch error rate 0.000000%\n",
      "At minibatch 2000, batch loss 0.473562, batch nll 0.055869, batch error rate 4.000000%\n",
      "After epoch 1: valid_err_rate: 1.980000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 6.704000% averaged train nll: 0.213142 averaged train loss: 0.690469\n",
      "At minibatch 2100, batch loss 0.509870, batch nll 0.097694, batch error rate 4.000000%\n",
      "At minibatch 2200, batch loss 0.605298, batch nll 0.198321, batch error rate 8.000000%\n",
      "At minibatch 2300, batch loss 0.428693, batch nll 0.026736, batch error rate 0.000000%\n",
      "At minibatch 2400, batch loss 0.482520, batch nll 0.085138, batch error rate 0.000000%\n",
      "At minibatch 2500, batch loss 0.409180, batch nll 0.016269, batch error rate 0.000000%\n",
      "At minibatch 2600, batch loss 0.510687, batch nll 0.122173, batch error rate 4.000000%\n",
      "At minibatch 2700, batch loss 0.388633, batch nll 0.004028, batch error rate 0.000000%\n",
      "At minibatch 2800, batch loss 0.396423, batch nll 0.015584, batch error rate 0.000000%\n",
      "At minibatch 2900, batch loss 0.399866, batch nll 0.022697, batch error rate 0.000000%\n",
      "At minibatch 3000, batch loss 0.440339, batch nll 0.066550, batch error rate 4.000000%\n",
      "At minibatch 3100, batch loss 0.433101, batch nll 0.062688, batch error rate 4.000000%\n",
      "At minibatch 3200, batch loss 0.378028, batch nll 0.010908, batch error rate 0.000000%\n",
      "At minibatch 3300, batch loss 0.403744, batch nll 0.039696, batch error rate 0.000000%\n",
      "At minibatch 3400, batch loss 0.463707, batch nll 0.102593, batch error rate 4.000000%\n",
      "At minibatch 3500, batch loss 0.368064, batch nll 0.009890, batch error rate 0.000000%\n",
      "At minibatch 3600, batch loss 0.421818, batch nll 0.066410, batch error rate 0.000000%\n",
      "At minibatch 3700, batch loss 0.363291, batch nll 0.010544, batch error rate 0.000000%\n",
      "At minibatch 3800, batch loss 0.356698, batch nll 0.006638, batch error rate 0.000000%\n",
      "At minibatch 3900, batch loss 0.355586, batch nll 0.008093, batch error rate 0.000000%\n",
      "At minibatch 4000, batch loss 0.371545, batch nll 0.026481, batch error rate 0.000000%\n",
      "After epoch 2: valid_err_rate: 1.400000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 1.696000% averaged train nll: 0.054630 averaged train loss: 0.430815\n",
      "At minibatch 4100, batch loss 0.344986, batch nll 0.002153, batch error rate 0.000000%\n",
      "At minibatch 4200, batch loss 0.350742, batch nll 0.010168, batch error rate 0.000000%\n",
      "At minibatch 4300, batch loss 0.436285, batch nll 0.097922, batch error rate 4.000000%\n",
      "At minibatch 4400, batch loss 0.353087, batch nll 0.016872, batch error rate 0.000000%\n",
      "At minibatch 4500, batch loss 0.337543, batch nll 0.003412, batch error rate 0.000000%\n",
      "At minibatch 4600, batch loss 0.344037, batch nll 0.011952, batch error rate 0.000000%\n",
      "At minibatch 4700, batch loss 0.332958, batch nll 0.002856, batch error rate 0.000000%\n",
      "At minibatch 4800, batch loss 0.342892, batch nll 0.014670, batch error rate 0.000000%\n",
      "At minibatch 4900, batch loss 0.328778, batch nll 0.002348, batch error rate 0.000000%\n",
      "At minibatch 5000, batch loss 0.337317, batch nll 0.012747, batch error rate 0.000000%\n",
      "At minibatch 5100, batch loss 0.339973, batch nll 0.017158, batch error rate 0.000000%\n",
      "At minibatch 5200, batch loss 0.386898, batch nll 0.065827, batch error rate 4.000000%\n",
      "At minibatch 5300, batch loss 0.331104, batch nll 0.011622, batch error rate 0.000000%\n",
      "At minibatch 5400, batch loss 0.345381, batch nll 0.027492, batch error rate 0.000000%\n",
      "At minibatch 5500, batch loss 0.323769, batch nll 0.007463, batch error rate 0.000000%\n",
      "At minibatch 5600, batch loss 0.326477, batch nll 0.011631, batch error rate 0.000000%\n",
      "At minibatch 5700, batch loss 0.330464, batch nll 0.017135, batch error rate 0.000000%\n",
      "At minibatch 5800, batch loss 0.330774, batch nll 0.018989, batch error rate 0.000000%\n",
      "At minibatch 5900, batch loss 0.324133, batch nll 0.013798, batch error rate 0.000000%\n",
      "At minibatch 6000, batch loss 0.315206, batch nll 0.006324, batch error rate 0.000000%\n",
      "After epoch 3: valid_err_rate: 1.330000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 1.138000% averaged train nll: 0.038731 averaged train loss: 0.364134\n",
      "At minibatch 6100, batch loss 0.311665, batch nll 0.004097, batch error rate 0.000000%\n",
      "At minibatch 6200, batch loss 0.312759, batch nll 0.006509, batch error rate 0.000000%\n",
      "At minibatch 6300, batch loss 0.310610, batch nll 0.005713, batch error rate 0.000000%\n",
      "At minibatch 6400, batch loss 0.307305, batch nll 0.003669, batch error rate 0.000000%\n",
      "At minibatch 6500, batch loss 0.369551, batch nll 0.067191, batch error rate 4.000000%\n",
      "At minibatch 6600, batch loss 0.457856, batch nll 0.156667, batch error rate 4.000000%\n",
      "At minibatch 6700, batch loss 0.341877, batch nll 0.041900, batch error rate 4.000000%\n",
      "At minibatch 6800, batch loss 0.323922, batch nll 0.025100, batch error rate 0.000000%\n",
      "At minibatch 6900, batch loss 0.302008, batch nll 0.004354, batch error rate 0.000000%\n",
      "At minibatch 7000, batch loss 0.387459, batch nll 0.090976, batch error rate 4.000000%\n",
      "At minibatch 7100, batch loss 0.309616, batch nll 0.014271, batch error rate 0.000000%\n",
      "At minibatch 7200, batch loss 0.424902, batch nll 0.130698, batch error rate 4.000000%\n",
      "At minibatch 7300, batch loss 0.328469, batch nll 0.035407, batch error rate 0.000000%\n",
      "At minibatch 7400, batch loss 0.335671, batch nll 0.043725, batch error rate 4.000000%\n",
      "At minibatch 7500, batch loss 0.316356, batch nll 0.025455, batch error rate 0.000000%\n",
      "At minibatch 7600, batch loss 0.306186, batch nll 0.016294, batch error rate 0.000000%\n",
      "At minibatch 7700, batch loss 0.308659, batch nll 0.019789, batch error rate 0.000000%\n",
      "At minibatch 7800, batch loss 0.386787, batch nll 0.098908, batch error rate 4.000000%\n",
      "At minibatch 7900, batch loss 0.361464, batch nll 0.074634, batch error rate 4.000000%\n",
      "At minibatch 8000, batch loss 0.355822, batch nll 0.069986, batch error rate 4.000000%\n",
      "After epoch 4: valid_err_rate: 1.250000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 0.922000% averaged train nll: 0.032271 averaged train loss: 0.329020\n",
      "At minibatch 8100, batch loss 0.293082, batch nll 0.008200, batch error rate 0.000000%\n",
      "At minibatch 8200, batch loss 0.347119, batch nll 0.063166, batch error rate 4.000000%\n",
      "At minibatch 8300, batch loss 0.310024, batch nll 0.026971, batch error rate 0.000000%\n",
      "At minibatch 8400, batch loss 0.290066, batch nll 0.007906, batch error rate 0.000000%\n",
      "At minibatch 8500, batch loss 0.284018, batch nll 0.002704, batch error rate 0.000000%\n",
      "At minibatch 8600, batch loss 0.289038, batch nll 0.008599, batch error rate 0.000000%\n",
      "At minibatch 8700, batch loss 0.303195, batch nll 0.023618, batch error rate 0.000000%\n",
      "At minibatch 8800, batch loss 0.283349, batch nll 0.004607, batch error rate 0.000000%\n",
      "At minibatch 8900, batch loss 0.374575, batch nll 0.096657, batch error rate 4.000000%\n",
      "At minibatch 9000, batch loss 0.412918, batch nll 0.135846, batch error rate 8.000000%\n",
      "At minibatch 9100, batch loss 0.361450, batch nll 0.085212, batch error rate 4.000000%\n",
      "At minibatch 9200, batch loss 0.337364, batch nll 0.061939, batch error rate 0.000000%\n",
      "At minibatch 9300, batch loss 0.354315, batch nll 0.079692, batch error rate 4.000000%\n",
      "At minibatch 9400, batch loss 0.281262, batch nll 0.007431, batch error rate 0.000000%\n",
      "At minibatch 9500, batch loss 0.502828, batch nll 0.229758, batch error rate 8.000000%\n",
      "At minibatch 9600, batch loss 0.275630, batch nll 0.003342, batch error rate 0.000000%\n",
      "At minibatch 9700, batch loss 0.291558, batch nll 0.020075, batch error rate 0.000000%\n",
      "At minibatch 9800, batch loss 0.473931, batch nll 0.203193, batch error rate 8.000000%\n",
      "At minibatch 9900, batch loss 0.272157, batch nll 0.002160, batch error rate 0.000000%\n",
      "At minibatch 10000, batch loss 0.272586, batch nll 0.003330, batch error rate 0.000000%\n",
      "After epoch 5: valid_err_rate: 1.140000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 0.858000% averaged train nll: 0.029547 averaged train loss: 0.306758\n",
      "At minibatch 10100, batch loss 0.289646, batch nll 0.021064, batch error rate 0.000000%\n",
      "At minibatch 10200, batch loss 0.289458, batch nll 0.021554, batch error rate 0.000000%\n",
      "At minibatch 10300, batch loss 0.284694, batch nll 0.017483, batch error rate 0.000000%\n",
      "At minibatch 10400, batch loss 0.270229, batch nll 0.003667, batch error rate 0.000000%\n",
      "At minibatch 10500, batch loss 0.572096, batch nll 0.306186, batch error rate 8.000000%\n",
      "At minibatch 10600, batch loss 0.273348, batch nll 0.008113, batch error rate 0.000000%\n",
      "At minibatch 10700, batch loss 0.279202, batch nll 0.014592, batch error rate 0.000000%\n",
      "At minibatch 10800, batch loss 0.367316, batch nll 0.103413, batch error rate 4.000000%\n",
      "At minibatch 10900, batch loss 0.295984, batch nll 0.032695, batch error rate 0.000000%\n",
      "At minibatch 11000, batch loss 0.267571, batch nll 0.004897, batch error rate 0.000000%\n",
      "At minibatch 11100, batch loss 0.285923, batch nll 0.023873, batch error rate 0.000000%\n",
      "At minibatch 11200, batch loss 0.347499, batch nll 0.086049, batch error rate 4.000000%\n",
      "At minibatch 11300, batch loss 0.262164, batch nll 0.001326, batch error rate 0.000000%\n",
      "At minibatch 11400, batch loss 0.262123, batch nll 0.001901, batch error rate 0.000000%\n",
      "At minibatch 11500, batch loss 0.262106, batch nll 0.002485, batch error rate 0.000000%\n",
      "At minibatch 11600, batch loss 0.261463, batch nll 0.002435, batch error rate 0.000000%\n",
      "At minibatch 11700, batch loss 0.360132, batch nll 0.101691, batch error rate 4.000000%\n",
      "At minibatch 11800, batch loss 0.262016, batch nll 0.004161, batch error rate 0.000000%\n",
      "At minibatch 11900, batch loss 0.279547, batch nll 0.022255, batch error rate 0.000000%\n",
      "At minibatch 12000, batch loss 0.267834, batch nll 0.011119, batch error rate 0.000000%\n",
      "After epoch 6: valid_err_rate: 1.060000% currently going to do 10 epochs\n",
      "After epoch 6: averaged train_err_rate: 0.746000% averaged train nll: 0.027085 averaged train loss: 0.289865\n",
      "At minibatch 12100, batch loss 0.277822, batch nll 0.021652, batch error rate 0.000000%\n",
      "At minibatch 12200, batch loss 0.261033, batch nll 0.005388, batch error rate 0.000000%\n",
      "At minibatch 12300, batch loss 0.266155, batch nll 0.011050, batch error rate 0.000000%\n",
      "At minibatch 12400, batch loss 0.269840, batch nll 0.015268, batch error rate 0.000000%\n",
      "At minibatch 12500, batch loss 0.255924, batch nll 0.001886, batch error rate 0.000000%\n",
      "At minibatch 12600, batch loss 0.258052, batch nll 0.004558, batch error rate 0.000000%\n",
      "At minibatch 12700, batch loss 0.334132, batch nll 0.081151, batch error rate 4.000000%\n",
      "At minibatch 12800, batch loss 0.313726, batch nll 0.061245, batch error rate 4.000000%\n",
      "At minibatch 12900, batch loss 0.257993, batch nll 0.006024, batch error rate 0.000000%\n",
      "At minibatch 13000, batch loss 0.407324, batch nll 0.155852, batch error rate 8.000000%\n",
      "At minibatch 13100, batch loss 0.285103, batch nll 0.034136, batch error rate 0.000000%\n",
      "At minibatch 13200, batch loss 0.256349, batch nll 0.005874, batch error rate 0.000000%\n",
      "At minibatch 13300, batch loss 0.253245, batch nll 0.003275, batch error rate 0.000000%\n",
      "At minibatch 13400, batch loss 0.256250, batch nll 0.006779, batch error rate 0.000000%\n",
      "At minibatch 13500, batch loss 0.295165, batch nll 0.046163, batch error rate 0.000000%\n",
      "At minibatch 13600, batch loss 0.254377, batch nll 0.005855, batch error rate 0.000000%\n",
      "At minibatch 13700, batch loss 0.257621, batch nll 0.009593, batch error rate 0.000000%\n",
      "At minibatch 13800, batch loss 0.258806, batch nll 0.011254, batch error rate 0.000000%\n",
      "At minibatch 13900, batch loss 0.282419, batch nll 0.035349, batch error rate 4.000000%\n",
      "At minibatch 14000, batch loss 0.380209, batch nll 0.133616, batch error rate 4.000000%\n",
      "After epoch 7: valid_err_rate: 1.200000% currently going to do 10 epochs\n",
      "After epoch 7: averaged train_err_rate: 0.682000% averaged train nll: 0.025545 averaged train loss: 0.277073\n",
      "At minibatch 14100, batch loss 0.251927, batch nll 0.005792, batch error rate 0.000000%\n",
      "At minibatch 14200, batch loss 0.251515, batch nll 0.005832, batch error rate 0.000000%\n",
      "At minibatch 14300, batch loss 0.253501, batch nll 0.008269, batch error rate 0.000000%\n",
      "At minibatch 14400, batch loss 0.250482, batch nll 0.005677, batch error rate 0.000000%\n",
      "At minibatch 14500, batch loss 0.250804, batch nll 0.006463, batch error rate 0.000000%\n",
      "At minibatch 14600, batch loss 0.271234, batch nll 0.027304, batch error rate 0.000000%\n",
      "At minibatch 14700, batch loss 0.244661, batch nll 0.001146, batch error rate 0.000000%\n",
      "At minibatch 14800, batch loss 0.248807, batch nll 0.005716, batch error rate 0.000000%\n",
      "At minibatch 14900, batch loss 0.254422, batch nll 0.011743, batch error rate 0.000000%\n",
      "At minibatch 15000, batch loss 0.250892, batch nll 0.008630, batch error rate 0.000000%\n",
      "At minibatch 15100, batch loss 0.261007, batch nll 0.019145, batch error rate 0.000000%\n",
      "At minibatch 15200, batch loss 0.250539, batch nll 0.009103, batch error rate 0.000000%\n",
      "At minibatch 15300, batch loss 0.262908, batch nll 0.021875, batch error rate 0.000000%\n",
      "At minibatch 15400, batch loss 0.303408, batch nll 0.062781, batch error rate 4.000000%\n",
      "At minibatch 15500, batch loss 0.260318, batch nll 0.020100, batch error rate 0.000000%\n",
      "At minibatch 15600, batch loss 0.247890, batch nll 0.008082, batch error rate 0.000000%\n",
      "At minibatch 15700, batch loss 0.288687, batch nll 0.049289, batch error rate 4.000000%\n",
      "At minibatch 15800, batch loss 0.272020, batch nll 0.033017, batch error rate 0.000000%\n",
      "At minibatch 15900, batch loss 0.256576, batch nll 0.017963, batch error rate 0.000000%\n",
      "At minibatch 16000, batch loss 0.255607, batch nll 0.017384, batch error rate 0.000000%\n",
      "After epoch 8: valid_err_rate: 1.160000% currently going to do 10 epochs\n",
      "After epoch 8: averaged train_err_rate: 0.642000% averaged train nll: 0.024538 averaged train loss: 0.266839\n",
      "At minibatch 16100, batch loss 0.304776, batch nll 0.066912, batch error rate 4.000000%\n",
      "At minibatch 16200, batch loss 0.241476, batch nll 0.003999, batch error rate 0.000000%\n",
      "At minibatch 16300, batch loss 0.251736, batch nll 0.014641, batch error rate 0.000000%\n",
      "At minibatch 16400, batch loss 0.244440, batch nll 0.007718, batch error rate 0.000000%\n",
      "At minibatch 16500, batch loss 0.249657, batch nll 0.013308, batch error rate 0.000000%\n",
      "At minibatch 16600, batch loss 0.240099, batch nll 0.004111, batch error rate 0.000000%\n",
      "At minibatch 16700, batch loss 0.253287, batch nll 0.017649, batch error rate 0.000000%\n",
      "At minibatch 16800, batch loss 0.272448, batch nll 0.037183, batch error rate 0.000000%\n",
      "At minibatch 16900, batch loss 0.240622, batch nll 0.005717, batch error rate 0.000000%\n",
      "At minibatch 17000, batch loss 0.238915, batch nll 0.004389, batch error rate 0.000000%\n",
      "At minibatch 17100, batch loss 0.246221, batch nll 0.012044, batch error rate 0.000000%\n",
      "At minibatch 17200, batch loss 0.297362, batch nll 0.063529, batch error rate 4.000000%\n",
      "At minibatch 17300, batch loss 0.249653, batch nll 0.016162, batch error rate 0.000000%\n",
      "At minibatch 17400, batch loss 0.293564, batch nll 0.060434, batch error rate 4.000000%\n",
      "At minibatch 17500, batch loss 0.236253, batch nll 0.003467, batch error rate 0.000000%\n",
      "At minibatch 17600, batch loss 0.250485, batch nll 0.018042, batch error rate 0.000000%\n",
      "At minibatch 17700, batch loss 0.236232, batch nll 0.004123, batch error rate 0.000000%\n",
      "At minibatch 17800, batch loss 0.232760, batch nll 0.000969, batch error rate 0.000000%\n",
      "At minibatch 17900, batch loss 0.286538, batch nll 0.055088, batch error rate 4.000000%\n",
      "At minibatch 18000, batch loss 0.247164, batch nll 0.016044, batch error rate 0.000000%\n",
      "After epoch 9: valid_err_rate: 1.050000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 0.588000% averaged train nll: 0.023737 averaged train loss: 0.258320\n",
      "At minibatch 18100, batch loss 0.242332, batch nll 0.011550, batch error rate 0.000000%\n",
      "At minibatch 18200, batch loss 0.246230, batch nll 0.015761, batch error rate 0.000000%\n",
      "At minibatch 18300, batch loss 0.233601, batch nll 0.003450, batch error rate 0.000000%\n",
      "At minibatch 18400, batch loss 0.232902, batch nll 0.003081, batch error rate 0.000000%\n",
      "At minibatch 18500, batch loss 0.247838, batch nll 0.018330, batch error rate 0.000000%\n",
      "At minibatch 18600, batch loss 0.234764, batch nll 0.005555, batch error rate 0.000000%\n",
      "At minibatch 18700, batch loss 0.229488, batch nll 0.000601, batch error rate 0.000000%\n",
      "At minibatch 18800, batch loss 0.242664, batch nll 0.014074, batch error rate 0.000000%\n",
      "At minibatch 18900, batch loss 0.233940, batch nll 0.005665, batch error rate 0.000000%\n",
      "At minibatch 19000, batch loss 0.241714, batch nll 0.013743, batch error rate 0.000000%\n",
      "At minibatch 19100, batch loss 0.252187, batch nll 0.024505, batch error rate 0.000000%\n",
      "At minibatch 19200, batch loss 0.229366, batch nll 0.001978, batch error rate 0.000000%\n",
      "At minibatch 19300, batch loss 0.248252, batch nll 0.021169, batch error rate 0.000000%\n",
      "At minibatch 19400, batch loss 0.229697, batch nll 0.002918, batch error rate 0.000000%\n",
      "At minibatch 19500, batch loss 0.235116, batch nll 0.008640, batch error rate 0.000000%\n",
      "At minibatch 19600, batch loss 0.250048, batch nll 0.023861, batch error rate 0.000000%\n",
      "At minibatch 19700, batch loss 0.242876, batch nll 0.016982, batch error rate 0.000000%\n",
      "At minibatch 19800, batch loss 0.230789, batch nll 0.005196, batch error rate 0.000000%\n",
      "At minibatch 19900, batch loss 0.243031, batch nll 0.017720, batch error rate 0.000000%\n",
      "At minibatch 20000, batch loss 0.253225, batch nll 0.028193, batch error rate 0.000000%\n",
      "After epoch 10: valid_err_rate: 1.050000% currently going to do 14 epochs\n",
      "After epoch 10: averaged train_err_rate: 0.598000% averaged train nll: 0.022811 averaged train loss: 0.250815\n",
      "At minibatch 20100, batch loss 0.231461, batch nll 0.006705, batch error rate 0.000000%\n",
      "At minibatch 20200, batch loss 0.252144, batch nll 0.027672, batch error rate 0.000000%\n",
      "At minibatch 20300, batch loss 0.226561, batch nll 0.002381, batch error rate 0.000000%\n",
      "At minibatch 20400, batch loss 0.245183, batch nll 0.021296, batch error rate 0.000000%\n",
      "At minibatch 20500, batch loss 0.233505, batch nll 0.009887, batch error rate 0.000000%\n",
      "At minibatch 20600, batch loss 0.229527, batch nll 0.006180, batch error rate 0.000000%\n",
      "At minibatch 20700, batch loss 0.227752, batch nll 0.004695, batch error rate 0.000000%\n",
      "At minibatch 20800, batch loss 0.228865, batch nll 0.006062, batch error rate 0.000000%\n",
      "At minibatch 20900, batch loss 0.252327, batch nll 0.029791, batch error rate 0.000000%\n",
      "At minibatch 21000, batch loss 0.245485, batch nll 0.023214, batch error rate 0.000000%\n",
      "At minibatch 21100, batch loss 0.260262, batch nll 0.038245, batch error rate 0.000000%\n",
      "At minibatch 21200, batch loss 0.233978, batch nll 0.012218, batch error rate 0.000000%\n",
      "At minibatch 21300, batch loss 0.226982, batch nll 0.005472, batch error rate 0.000000%\n",
      "At minibatch 21400, batch loss 0.225562, batch nll 0.004317, batch error rate 0.000000%\n",
      "At minibatch 21500, batch loss 0.223941, batch nll 0.002959, batch error rate 0.000000%\n",
      "At minibatch 21600, batch loss 0.231404, batch nll 0.010677, batch error rate 0.000000%\n",
      "At minibatch 21700, batch loss 0.232259, batch nll 0.011795, batch error rate 0.000000%\n",
      "At minibatch 21800, batch loss 0.222343, batch nll 0.002159, batch error rate 0.000000%\n",
      "At minibatch 21900, batch loss 0.222583, batch nll 0.002655, batch error rate 0.000000%\n",
      "At minibatch 22000, batch loss 0.263935, batch nll 0.044258, batch error rate 4.000000%\n",
      "After epoch 11: valid_err_rate: 1.050000% currently going to do 14 epochs\n",
      "After epoch 11: averaged train_err_rate: 0.564000% averaged train nll: 0.022323 averaged train loss: 0.244628\n",
      "At minibatch 22100, batch loss 0.226547, batch nll 0.007099, batch error rate 0.000000%\n",
      "At minibatch 22200, batch loss 0.229361, batch nll 0.010165, batch error rate 0.000000%\n",
      "At minibatch 22300, batch loss 0.238510, batch nll 0.019561, batch error rate 0.000000%\n",
      "At minibatch 22400, batch loss 0.346550, batch nll 0.127859, batch error rate 8.000000%\n",
      "At minibatch 22500, batch loss 0.231981, batch nll 0.013529, batch error rate 0.000000%\n",
      "At minibatch 22600, batch loss 0.289133, batch nll 0.070934, batch error rate 4.000000%\n",
      "At minibatch 22700, batch loss 0.219292, batch nll 0.001340, batch error rate 0.000000%\n",
      "At minibatch 22800, batch loss 0.275525, batch nll 0.057811, batch error rate 0.000000%\n",
      "At minibatch 22900, batch loss 0.309656, batch nll 0.092179, batch error rate 4.000000%\n",
      "At minibatch 23000, batch loss 0.225280, batch nll 0.008042, batch error rate 0.000000%\n",
      "At minibatch 23100, batch loss 0.256510, batch nll 0.039515, batch error rate 0.000000%\n",
      "At minibatch 23200, batch loss 0.220864, batch nll 0.004097, batch error rate 0.000000%\n",
      "At minibatch 23300, batch loss 0.233084, batch nll 0.016545, batch error rate 0.000000%\n",
      "At minibatch 23400, batch loss 0.219613, batch nll 0.003295, batch error rate 0.000000%\n",
      "At minibatch 23500, batch loss 0.219223, batch nll 0.003150, batch error rate 0.000000%\n",
      "At minibatch 23600, batch loss 0.303041, batch nll 0.087194, batch error rate 4.000000%\n",
      "At minibatch 23700, batch loss 0.256072, batch nll 0.040461, batch error rate 4.000000%\n",
      "At minibatch 23800, batch loss 0.268764, batch nll 0.053385, batch error rate 0.000000%\n",
      "At minibatch 23900, batch loss 0.221877, batch nll 0.006716, batch error rate 0.000000%\n",
      "At minibatch 24000, batch loss 0.220798, batch nll 0.005861, batch error rate 0.000000%\n",
      "After epoch 12: valid_err_rate: 1.130000% currently going to do 14 epochs\n",
      "After epoch 12: averaged train_err_rate: 0.530000% averaged train nll: 0.021718 averaged train loss: 0.238983\n",
      "At minibatch 24100, batch loss 0.219860, batch nll 0.005147, batch error rate 0.000000%\n",
      "At minibatch 24200, batch loss 0.217382, batch nll 0.002889, batch error rate 0.000000%\n",
      "At minibatch 24300, batch loss 0.225027, batch nll 0.010741, batch error rate 0.000000%\n",
      "At minibatch 24400, batch loss 0.235412, batch nll 0.021341, batch error rate 0.000000%\n",
      "At minibatch 24500, batch loss 0.215407, batch nll 0.001555, batch error rate 0.000000%\n",
      "At minibatch 24600, batch loss 0.221702, batch nll 0.008071, batch error rate 0.000000%\n",
      "At minibatch 24700, batch loss 0.219494, batch nll 0.006080, batch error rate 0.000000%\n",
      "At minibatch 24800, batch loss 0.228489, batch nll 0.015297, batch error rate 0.000000%\n",
      "At minibatch 24900, batch loss 0.216095, batch nll 0.003110, batch error rate 0.000000%\n",
      "At minibatch 25000, batch loss 0.229565, batch nll 0.016788, batch error rate 0.000000%\n",
      "At minibatch 25100, batch loss 0.213233, batch nll 0.000673, batch error rate 0.000000%\n",
      "At minibatch 25200, batch loss 0.221888, batch nll 0.009539, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.266332, batch nll 0.054185, batch error rate 0.000000%\n",
      "At minibatch 25400, batch loss 0.217740, batch nll 0.005817, batch error rate 0.000000%\n",
      "At minibatch 25500, batch loss 0.228873, batch nll 0.017172, batch error rate 0.000000%\n",
      "At minibatch 25600, batch loss 0.212699, batch nll 0.001202, batch error rate 0.000000%\n",
      "At minibatch 25700, batch loss 0.218644, batch nll 0.007353, batch error rate 0.000000%\n",
      "At minibatch 25800, batch loss 0.216727, batch nll 0.005640, batch error rate 0.000000%\n",
      "At minibatch 25900, batch loss 0.231176, batch nll 0.020286, batch error rate 0.000000%\n",
      "At minibatch 26000, batch loss 0.217586, batch nll 0.006907, batch error rate 0.000000%\n",
      "After epoch 13: valid_err_rate: 1.110000% currently going to do 14 epochs\n",
      "After epoch 13: averaged train_err_rate: 0.546000% averaged train nll: 0.021329 averaged train loss: 0.234112\n",
      "At minibatch 26100, batch loss 0.250513, batch nll 0.040033, batch error rate 0.000000%\n",
      "At minibatch 26200, batch loss 0.225846, batch nll 0.015576, batch error rate 0.000000%\n",
      "At minibatch 26300, batch loss 0.233418, batch nll 0.023352, batch error rate 0.000000%\n",
      "At minibatch 26400, batch loss 0.267339, batch nll 0.057467, batch error rate 4.000000%\n",
      "At minibatch 26500, batch loss 0.212618, batch nll 0.002936, batch error rate 0.000000%\n",
      "At minibatch 26600, batch loss 0.213717, batch nll 0.004233, batch error rate 0.000000%\n",
      "At minibatch 26700, batch loss 0.239852, batch nll 0.030569, batch error rate 0.000000%\n",
      "At minibatch 26800, batch loss 0.212320, batch nll 0.003227, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.215769, batch nll 0.006862, batch error rate 0.000000%\n",
      "At minibatch 27000, batch loss 0.275286, batch nll 0.066569, batch error rate 4.000000%\n",
      "At minibatch 27100, batch loss 0.211512, batch nll 0.002983, batch error rate 0.000000%\n",
      "At minibatch 27200, batch loss 0.211779, batch nll 0.003450, batch error rate 0.000000%\n",
      "At minibatch 27300, batch loss 0.209709, batch nll 0.001575, batch error rate 0.000000%\n",
      "At minibatch 27400, batch loss 0.249746, batch nll 0.041794, batch error rate 0.000000%\n",
      "At minibatch 27500, batch loss 0.209138, batch nll 0.001362, batch error rate 0.000000%\n",
      "At minibatch 27600, batch loss 0.253208, batch nll 0.045623, batch error rate 4.000000%\n",
      "At minibatch 27700, batch loss 0.209908, batch nll 0.002506, batch error rate 0.000000%\n",
      "At minibatch 27800, batch loss 0.214512, batch nll 0.007297, batch error rate 0.000000%\n",
      "At minibatch 27900, batch loss 0.212880, batch nll 0.005845, batch error rate 0.000000%\n",
      "At minibatch 28000, batch loss 0.436883, batch nll 0.230035, batch error rate 4.000000%\n",
      "After epoch 14: valid_err_rate: 1.030000% currently going to do 22 epochs\n",
      "After epoch 14: averaged train_err_rate: 0.548000% averaged train nll: 0.020992 averaged train loss: 0.229719\n",
      "At minibatch 28100, batch loss 0.213156, batch nll 0.006499, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.256424, batch nll 0.049944, batch error rate 0.000000%\n",
      "At minibatch 28300, batch loss 0.223040, batch nll 0.016753, batch error rate 0.000000%\n",
      "At minibatch 28400, batch loss 0.208634, batch nll 0.002516, batch error rate 0.000000%\n",
      "At minibatch 28500, batch loss 0.216109, batch nll 0.010178, batch error rate 0.000000%\n",
      "At minibatch 28600, batch loss 0.257123, batch nll 0.051375, batch error rate 4.000000%\n",
      "At minibatch 28700, batch loss 0.239040, batch nll 0.033459, batch error rate 0.000000%\n",
      "At minibatch 28800, batch loss 0.215073, batch nll 0.009668, batch error rate 0.000000%\n",
      "At minibatch 28900, batch loss 0.209831, batch nll 0.004601, batch error rate 0.000000%\n",
      "At minibatch 29000, batch loss 0.212945, batch nll 0.007887, batch error rate 0.000000%\n",
      "At minibatch 29100, batch loss 0.211074, batch nll 0.006207, batch error rate 0.000000%\n",
      "At minibatch 29200, batch loss 0.212921, batch nll 0.008222, batch error rate 0.000000%\n",
      "At minibatch 29300, batch loss 0.213235, batch nll 0.008702, batch error rate 0.000000%\n",
      "At minibatch 29400, batch loss 0.222492, batch nll 0.018137, batch error rate 0.000000%\n",
      "At minibatch 29500, batch loss 0.209904, batch nll 0.005716, batch error rate 0.000000%\n",
      "At minibatch 29600, batch loss 0.337471, batch nll 0.133450, batch error rate 4.000000%\n",
      "At minibatch 29700, batch loss 0.227666, batch nll 0.023816, batch error rate 0.000000%\n",
      "At minibatch 29800, batch loss 0.215347, batch nll 0.011669, batch error rate 0.000000%\n",
      "At minibatch 29900, batch loss 0.230847, batch nll 0.027332, batch error rate 0.000000%\n",
      "At minibatch 30000, batch loss 0.211883, batch nll 0.008538, batch error rate 0.000000%\n",
      "After epoch 15: valid_err_rate: 0.970000% currently going to do 23 epochs\n",
      "After epoch 15: averaged train_err_rate: 0.506000% averaged train nll: 0.020701 averaged train loss: 0.225765\n",
      "At minibatch 30100, batch loss 0.204951, batch nll 0.001776, batch error rate 0.000000%\n",
      "At minibatch 30200, batch loss 0.212382, batch nll 0.009365, batch error rate 0.000000%\n",
      "At minibatch 30300, batch loss 0.209059, batch nll 0.006209, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.216493, batch nll 0.013806, batch error rate 0.000000%\n",
      "At minibatch 30500, batch loss 0.223547, batch nll 0.021026, batch error rate 0.000000%\n",
      "At minibatch 30600, batch loss 0.206501, batch nll 0.004140, batch error rate 0.000000%\n",
      "At minibatch 30700, batch loss 0.203040, batch nll 0.000843, batch error rate 0.000000%\n",
      "At minibatch 30800, batch loss 0.205853, batch nll 0.003823, batch error rate 0.000000%\n",
      "At minibatch 30900, batch loss 0.205332, batch nll 0.003462, batch error rate 0.000000%\n",
      "At minibatch 31000, batch loss 0.253462, batch nll 0.051759, batch error rate 0.000000%\n",
      "At minibatch 31100, batch loss 0.214340, batch nll 0.012804, batch error rate 0.000000%\n",
      "At minibatch 31200, batch loss 0.216064, batch nll 0.014686, batch error rate 0.000000%\n",
      "At minibatch 31300, batch loss 0.211975, batch nll 0.010754, batch error rate 0.000000%\n",
      "At minibatch 31400, batch loss 0.212280, batch nll 0.011213, batch error rate 0.000000%\n",
      "At minibatch 31500, batch loss 0.217598, batch nll 0.016684, batch error rate 0.000000%\n",
      "At minibatch 31600, batch loss 0.213232, batch nll 0.012477, batch error rate 0.000000%\n",
      "At minibatch 31700, batch loss 0.202859, batch nll 0.002250, batch error rate 0.000000%\n",
      "At minibatch 31800, batch loss 0.202467, batch nll 0.002014, batch error rate 0.000000%\n",
      "At minibatch 31900, batch loss 0.223332, batch nll 0.023025, batch error rate 0.000000%\n",
      "At minibatch 32000, batch loss 0.201162, batch nll 0.001015, batch error rate 0.000000%\n",
      "After epoch 16: valid_err_rate: 1.050000% currently going to do 23 epochs\n",
      "After epoch 16: averaged train_err_rate: 0.472000% averaged train nll: 0.020309 averaged train loss: 0.222028\n",
      "At minibatch 32100, batch loss 0.378367, batch nll 0.178374, batch error rate 4.000000%\n",
      "At minibatch 32200, batch loss 0.206403, batch nll 0.006567, batch error rate 0.000000%\n",
      "At minibatch 32300, batch loss 0.206665, batch nll 0.006981, batch error rate 0.000000%\n",
      "At minibatch 32400, batch loss 0.219872, batch nll 0.020332, batch error rate 0.000000%\n",
      "At minibatch 32500, batch loss 0.204787, batch nll 0.005394, batch error rate 0.000000%\n",
      "At minibatch 32600, batch loss 0.202500, batch nll 0.003252, batch error rate 0.000000%\n",
      "At minibatch 32700, batch loss 0.202470, batch nll 0.003370, batch error rate 0.000000%\n",
      "At minibatch 32800, batch loss 0.200920, batch nll 0.001983, batch error rate 0.000000%\n",
      "At minibatch 32900, batch loss 0.206168, batch nll 0.007379, batch error rate 0.000000%\n",
      "At minibatch 33000, batch loss 0.235510, batch nll 0.036874, batch error rate 4.000000%\n",
      "At minibatch 33100, batch loss 0.210368, batch nll 0.011886, batch error rate 0.000000%\n",
      "At minibatch 33200, batch loss 0.229838, batch nll 0.031500, batch error rate 0.000000%\n",
      "At minibatch 33300, batch loss 0.213851, batch nll 0.015657, batch error rate 0.000000%\n",
      "At minibatch 33400, batch loss 0.200452, batch nll 0.002403, batch error rate 0.000000%\n",
      "At minibatch 33500, batch loss 0.224654, batch nll 0.026750, batch error rate 0.000000%\n",
      "At minibatch 33600, batch loss 0.200455, batch nll 0.002696, batch error rate 0.000000%\n",
      "At minibatch 33700, batch loss 0.200605, batch nll 0.002988, batch error rate 0.000000%\n",
      "At minibatch 33800, batch loss 0.200068, batch nll 0.002589, batch error rate 0.000000%\n",
      "At minibatch 33900, batch loss 0.210342, batch nll 0.013008, batch error rate 0.000000%\n",
      "At minibatch 34000, batch loss 0.212665, batch nll 0.015468, batch error rate 0.000000%\n",
      "After epoch 17: valid_err_rate: 1.010000% currently going to do 23 epochs\n",
      "After epoch 17: averaged train_err_rate: 0.492000% averaged train nll: 0.020202 averaged train loss: 0.218850\n",
      "At minibatch 34100, batch loss 0.215849, batch nll 0.018792, batch error rate 0.000000%\n",
      "At minibatch 34200, batch loss 0.200277, batch nll 0.003360, batch error rate 0.000000%\n",
      "At minibatch 34300, batch loss 0.204730, batch nll 0.007958, batch error rate 0.000000%\n",
      "At minibatch 34400, batch loss 0.208367, batch nll 0.011741, batch error rate 0.000000%\n",
      "At minibatch 34500, batch loss 0.226179, batch nll 0.029688, batch error rate 0.000000%\n",
      "At minibatch 34600, batch loss 0.235873, batch nll 0.039522, batch error rate 4.000000%\n",
      "At minibatch 34700, batch loss 0.196945, batch nll 0.000729, batch error rate 0.000000%\n",
      "At minibatch 34800, batch loss 0.200922, batch nll 0.004841, batch error rate 0.000000%\n",
      "At minibatch 34900, batch loss 0.218983, batch nll 0.023045, batch error rate 0.000000%\n",
      "At minibatch 35000, batch loss 0.199964, batch nll 0.004158, batch error rate 0.000000%\n",
      "At minibatch 35100, batch loss 0.234837, batch nll 0.039168, batch error rate 0.000000%\n",
      "At minibatch 35200, batch loss 0.237496, batch nll 0.041959, batch error rate 0.000000%\n",
      "At minibatch 35300, batch loss 0.198016, batch nll 0.002616, batch error rate 0.000000%\n",
      "At minibatch 35400, batch loss 0.243874, batch nll 0.048607, batch error rate 0.000000%\n",
      "At minibatch 35500, batch loss 0.230607, batch nll 0.035477, batch error rate 0.000000%\n",
      "At minibatch 35600, batch loss 0.207225, batch nll 0.012224, batch error rate 0.000000%\n",
      "At minibatch 35700, batch loss 0.195428, batch nll 0.000554, batch error rate 0.000000%\n",
      "At minibatch 35800, batch loss 0.200015, batch nll 0.005278, batch error rate 0.000000%\n",
      "At minibatch 35900, batch loss 0.198405, batch nll 0.003805, batch error rate 0.000000%\n",
      "At minibatch 36000, batch loss 0.197833, batch nll 0.003362, batch error rate 0.000000%\n",
      "After epoch 18: valid_err_rate: 1.030000% currently going to do 23 epochs\n",
      "After epoch 18: averaged train_err_rate: 0.474000% averaged train nll: 0.019880 averaged train loss: 0.215695\n",
      "At minibatch 36100, batch loss 0.195624, batch nll 0.001282, batch error rate 0.000000%\n",
      "At minibatch 36200, batch loss 0.256979, batch nll 0.062769, batch error rate 4.000000%\n",
      "At minibatch 36300, batch loss 0.194490, batch nll 0.000417, batch error rate 0.000000%\n",
      "At minibatch 36400, batch loss 0.196073, batch nll 0.002131, batch error rate 0.000000%\n",
      "At minibatch 36500, batch loss 0.323206, batch nll 0.129392, batch error rate 4.000000%\n",
      "At minibatch 36600, batch loss 0.204163, batch nll 0.010477, batch error rate 0.000000%\n",
      "At minibatch 36700, batch loss 0.254016, batch nll 0.060456, batch error rate 4.000000%\n",
      "At minibatch 36800, batch loss 0.198892, batch nll 0.005455, batch error rate 0.000000%\n",
      "At minibatch 36900, batch loss 0.201186, batch nll 0.007874, batch error rate 0.000000%\n",
      "At minibatch 37000, batch loss 0.223746, batch nll 0.030561, batch error rate 0.000000%\n",
      "At minibatch 37100, batch loss 0.202891, batch nll 0.009827, batch error rate 0.000000%\n",
      "At minibatch 37200, batch loss 0.267413, batch nll 0.074481, batch error rate 4.000000%\n",
      "At minibatch 37300, batch loss 0.195795, batch nll 0.002984, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.197561, batch nll 0.004877, batch error rate 0.000000%\n",
      "At minibatch 37500, batch loss 0.196171, batch nll 0.003615, batch error rate 0.000000%\n",
      "At minibatch 37600, batch loss 0.195676, batch nll 0.003242, batch error rate 0.000000%\n",
      "At minibatch 37700, batch loss 0.199633, batch nll 0.007317, batch error rate 0.000000%\n",
      "At minibatch 37800, batch loss 0.224617, batch nll 0.032429, batch error rate 0.000000%\n",
      "At minibatch 37900, batch loss 0.271428, batch nll 0.079365, batch error rate 0.000000%\n",
      "At minibatch 38000, batch loss 0.205818, batch nll 0.013880, batch error rate 0.000000%\n",
      "After epoch 19: valid_err_rate: 0.990000% currently going to do 23 epochs\n",
      "After epoch 19: averaged train_err_rate: 0.474000% averaged train nll: 0.019595 averaged train loss: 0.212785\n",
      "At minibatch 38100, batch loss 0.215341, batch nll 0.023522, batch error rate 0.000000%\n",
      "At minibatch 38200, batch loss 0.196708, batch nll 0.005010, batch error rate 0.000000%\n",
      "At minibatch 38300, batch loss 0.195039, batch nll 0.003458, batch error rate 0.000000%\n",
      "At minibatch 38400, batch loss 0.201596, batch nll 0.010141, batch error rate 0.000000%\n",
      "At minibatch 38500, batch loss 0.195701, batch nll 0.004364, batch error rate 0.000000%\n",
      "At minibatch 38600, batch loss 0.220379, batch nll 0.029163, batch error rate 0.000000%\n",
      "At minibatch 38700, batch loss 0.194333, batch nll 0.003237, batch error rate 0.000000%\n",
      "At minibatch 38800, batch loss 0.193613, batch nll 0.002631, batch error rate 0.000000%\n",
      "At minibatch 38900, batch loss 0.331281, batch nll 0.140417, batch error rate 4.000000%\n",
      "At minibatch 39000, batch loss 0.200033, batch nll 0.009286, batch error rate 0.000000%\n",
      "At minibatch 39100, batch loss 0.197889, batch nll 0.007257, batch error rate 0.000000%\n",
      "At minibatch 39200, batch loss 0.204186, batch nll 0.013670, batch error rate 0.000000%\n",
      "At minibatch 39300, batch loss 0.199254, batch nll 0.008855, batch error rate 0.000000%\n",
      "At minibatch 39400, batch loss 0.217263, batch nll 0.026989, batch error rate 0.000000%\n",
      "At minibatch 39500, batch loss 0.212108, batch nll 0.021950, batch error rate 0.000000%\n",
      "At minibatch 39600, batch loss 0.196146, batch nll 0.006106, batch error rate 0.000000%\n",
      "At minibatch 39700, batch loss 0.210513, batch nll 0.020589, batch error rate 0.000000%\n",
      "At minibatch 39800, batch loss 0.210877, batch nll 0.021066, batch error rate 0.000000%\n",
      "At minibatch 39900, batch loss 0.200641, batch nll 0.010947, batch error rate 0.000000%\n",
      "At minibatch 40000, batch loss 0.190301, batch nll 0.000723, batch error rate 0.000000%\n",
      "After epoch 20: valid_err_rate: 1.080000% currently going to do 23 epochs\n",
      "After epoch 20: averaged train_err_rate: 0.454000% averaged train nll: 0.019385 averaged train loss: 0.210134\n",
      "At minibatch 40100, batch loss 0.201102, batch nll 0.011636, batch error rate 0.000000%\n",
      "At minibatch 40200, batch loss 0.297744, batch nll 0.108392, batch error rate 4.000000%\n",
      "At minibatch 40300, batch loss 0.204153, batch nll 0.014917, batch error rate 0.000000%\n",
      "At minibatch 40400, batch loss 0.190102, batch nll 0.000976, batch error rate 0.000000%\n",
      "At minibatch 40500, batch loss 0.249013, batch nll 0.060000, batch error rate 4.000000%\n",
      "At minibatch 40600, batch loss 0.224312, batch nll 0.035409, batch error rate 0.000000%\n",
      "At minibatch 40700, batch loss 0.196013, batch nll 0.007221, batch error rate 0.000000%\n",
      "At minibatch 40800, batch loss 0.198783, batch nll 0.010098, batch error rate 0.000000%\n",
      "At minibatch 40900, batch loss 0.202040, batch nll 0.013462, batch error rate 0.000000%\n",
      "At minibatch 41000, batch loss 0.196453, batch nll 0.007984, batch error rate 0.000000%\n",
      "At minibatch 41100, batch loss 0.208580, batch nll 0.020218, batch error rate 0.000000%\n",
      "At minibatch 41200, batch loss 0.199016, batch nll 0.010766, batch error rate 0.000000%\n",
      "At minibatch 41300, batch loss 0.191734, batch nll 0.003601, batch error rate 0.000000%\n",
      "At minibatch 41400, batch loss 0.208721, batch nll 0.020695, batch error rate 0.000000%\n",
      "At minibatch 41500, batch loss 0.212044, batch nll 0.024130, batch error rate 0.000000%\n",
      "At minibatch 41600, batch loss 0.189362, batch nll 0.001556, batch error rate 0.000000%\n",
      "At minibatch 41700, batch loss 0.254165, batch nll 0.066476, batch error rate 4.000000%\n",
      "At minibatch 41800, batch loss 0.209473, batch nll 0.021891, batch error rate 0.000000%\n",
      "At minibatch 41900, batch loss 0.203295, batch nll 0.015821, batch error rate 0.000000%\n",
      "At minibatch 42000, batch loss 0.208781, batch nll 0.021416, batch error rate 0.000000%\n",
      "After epoch 21: valid_err_rate: 0.990000% currently going to do 23 epochs\n",
      "After epoch 21: averaged train_err_rate: 0.450000% averaged train nll: 0.019249 averaged train loss: 0.207715\n",
      "At minibatch 42100, batch loss 0.187899, batch nll 0.000636, batch error rate 0.000000%\n",
      "At minibatch 42200, batch loss 0.187929, batch nll 0.000775, batch error rate 0.000000%\n",
      "At minibatch 42300, batch loss 0.189540, batch nll 0.002485, batch error rate 0.000000%\n",
      "At minibatch 42400, batch loss 0.312721, batch nll 0.125767, batch error rate 12.000000%\n",
      "At minibatch 42500, batch loss 0.189475, batch nll 0.002623, batch error rate 0.000000%\n",
      "At minibatch 42600, batch loss 0.193835, batch nll 0.007090, batch error rate 0.000000%\n",
      "At minibatch 42700, batch loss 0.203643, batch nll 0.017003, batch error rate 0.000000%\n",
      "At minibatch 42800, batch loss 0.205541, batch nll 0.019008, batch error rate 0.000000%\n",
      "At minibatch 42900, batch loss 0.192946, batch nll 0.006515, batch error rate 0.000000%\n",
      "At minibatch 43000, batch loss 0.194479, batch nll 0.008150, batch error rate 0.000000%\n",
      "At minibatch 43100, batch loss 0.190825, batch nll 0.004600, batch error rate 0.000000%\n",
      "At minibatch 43200, batch loss 0.192708, batch nll 0.006587, batch error rate 0.000000%\n",
      "At minibatch 43300, batch loss 0.195829, batch nll 0.009813, batch error rate 0.000000%\n",
      "At minibatch 43400, batch loss 0.190398, batch nll 0.004489, batch error rate 0.000000%\n",
      "At minibatch 43500, batch loss 0.193134, batch nll 0.007325, batch error rate 0.000000%\n",
      "At minibatch 43600, batch loss 0.192912, batch nll 0.007211, batch error rate 0.000000%\n",
      "At minibatch 43700, batch loss 0.255287, batch nll 0.069693, batch error rate 4.000000%\n",
      "At minibatch 43800, batch loss 0.208247, batch nll 0.022753, batch error rate 0.000000%\n",
      "At minibatch 43900, batch loss 0.186619, batch nll 0.001227, batch error rate 0.000000%\n",
      "At minibatch 44000, batch loss 0.203281, batch nll 0.017996, batch error rate 0.000000%\n",
      "After epoch 22: valid_err_rate: 0.980000% currently going to do 23 epochs\n",
      "After epoch 22: averaged train_err_rate: 0.442000% averaged train nll: 0.019091 averaged train loss: 0.205417\n",
      "At minibatch 44100, batch loss 0.237842, batch nll 0.052655, batch error rate 4.000000%\n",
      "At minibatch 44200, batch loss 0.187911, batch nll 0.002821, batch error rate 0.000000%\n",
      "At minibatch 44300, batch loss 0.207895, batch nll 0.022902, batch error rate 0.000000%\n",
      "At minibatch 44400, batch loss 0.188352, batch nll 0.003452, batch error rate 0.000000%\n",
      "At minibatch 44500, batch loss 0.206280, batch nll 0.021481, batch error rate 0.000000%\n",
      "At minibatch 44600, batch loss 0.211367, batch nll 0.026663, batch error rate 0.000000%\n",
      "At minibatch 44700, batch loss 0.192199, batch nll 0.007599, batch error rate 0.000000%\n",
      "At minibatch 44800, batch loss 0.200083, batch nll 0.015585, batch error rate 0.000000%\n",
      "At minibatch 44900, batch loss 0.189081, batch nll 0.004684, batch error rate 0.000000%\n",
      "At minibatch 45000, batch loss 0.189030, batch nll 0.004735, batch error rate 0.000000%\n",
      "At minibatch 45100, batch loss 0.190198, batch nll 0.005999, batch error rate 0.000000%\n",
      "At minibatch 45200, batch loss 0.186849, batch nll 0.002748, batch error rate 0.000000%\n",
      "At minibatch 45300, batch loss 0.237929, batch nll 0.053927, batch error rate 4.000000%\n",
      "At minibatch 45400, batch loss 0.212506, batch nll 0.028604, batch error rate 0.000000%\n",
      "At minibatch 45500, batch loss 0.285486, batch nll 0.101682, batch error rate 8.000000%\n",
      "At minibatch 45600, batch loss 0.184390, batch nll 0.000682, batch error rate 0.000000%\n",
      "At minibatch 45700, batch loss 0.184696, batch nll 0.001084, batch error rate 0.000000%\n",
      "At minibatch 45800, batch loss 0.185310, batch nll 0.001796, batch error rate 0.000000%\n",
      "At minibatch 45900, batch loss 0.194549, batch nll 0.011130, batch error rate 0.000000%\n",
      "At minibatch 46000, batch loss 0.206001, batch nll 0.022676, batch error rate 0.000000%\n",
      "After epoch 23: valid_err_rate: 1.060000% currently going to do 23 epochs\n",
      "After epoch 23: averaged train_err_rate: 0.428000% averaged train nll: 0.018949 averaged train loss: 0.203250\n",
      "At minibatch 46100, batch loss 0.189664, batch nll 0.006433, batch error rate 0.000000%\n",
      "At minibatch 46200, batch loss 0.223136, batch nll 0.039997, batch error rate 0.000000%\n",
      "At minibatch 46300, batch loss 0.184075, batch nll 0.001029, batch error rate 0.000000%\n",
      "At minibatch 46400, batch loss 0.184411, batch nll 0.001457, batch error rate 0.000000%\n",
      "At minibatch 46500, batch loss 0.197276, batch nll 0.014416, batch error rate 0.000000%\n",
      "At minibatch 46600, batch loss 0.240614, batch nll 0.057846, batch error rate 0.000000%\n",
      "At minibatch 46700, batch loss 0.200631, batch nll 0.017953, batch error rate 0.000000%\n",
      "At minibatch 46800, batch loss 0.289089, batch nll 0.106504, batch error rate 4.000000%\n",
      "At minibatch 46900, batch loss 0.207816, batch nll 0.025329, batch error rate 0.000000%\n",
      "At minibatch 47000, batch loss 0.186446, batch nll 0.004053, batch error rate 0.000000%\n",
      "At minibatch 47100, batch loss 0.190563, batch nll 0.008263, batch error rate 0.000000%\n",
      "At minibatch 47200, batch loss 0.186447, batch nll 0.004239, batch error rate 0.000000%\n",
      "At minibatch 47300, batch loss 0.196358, batch nll 0.014242, batch error rate 0.000000%\n",
      "At minibatch 47400, batch loss 0.186601, batch nll 0.004575, batch error rate 0.000000%\n",
      "At minibatch 47500, batch loss 0.184487, batch nll 0.002550, batch error rate 0.000000%\n",
      "At minibatch 47600, batch loss 0.284635, batch nll 0.102784, batch error rate 4.000000%\n",
      "At minibatch 47700, batch loss 0.202651, batch nll 0.020892, batch error rate 0.000000%\n",
      "At minibatch 47800, batch loss 0.186517, batch nll 0.004848, batch error rate 0.000000%\n",
      "At minibatch 47900, batch loss 0.229596, batch nll 0.048017, batch error rate 0.000000%\n",
      "At minibatch 48000, batch loss 0.196164, batch nll 0.014682, batch error rate 0.000000%\n",
      "After epoch 24: valid_err_rate: 1.030000% currently going to do 23 epochs\n",
      "After epoch 24: averaged train_err_rate: 0.430000% averaged train nll: 0.018791 averaged train loss: 0.201190\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "while e<number_of_epochs: #This loop goes over epochs\n",
    "    e += 1\n",
    "    #First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in mnist_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum=0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        #print [p.get_value().ravel()[:10] for p in model_parameters]\n",
    "        #print [p.get_value().ravel()[:10] for p in velocities]\n",
    "        \n",
    "        \n",
    "        train_loss.append((i,L))\n",
    "        train_erros.append((i,err_rate))\n",
    "        train_nll.append((i,nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate*100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(mnist_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i,val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" %(\n",
    "        e, val_error_rate*100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" %(\n",
    "        e, np.mean(np.asarray(train_erros)[epoch_start_i:,1])*100, \n",
    "        np.mean(np.asarray(train_nll)[epoch_start_i:,1]),\n",
    "        np.mean(np.asarray(train_loss)[epoch_start_i:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 15\n",
      "Test error rate is 0.820000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc0e0327e90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc106709dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" %(compute_error_rate(mnist_test_stream)*100.0,)\n",
    "\n",
    "subplot(2,1,1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:,0], train_nll_a[:,1], label='batch train nll')\n",
    "legend()\n",
    "\n",
    "subplot(2,1,2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:,0], train_erros_a[:,1], label='batch train error rate')\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:,0], validation_errors_a[:,1], label='validation error rate', color='r')\n",
    "ylim(0,0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAACWCAYAAAD64bJyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEUobIAiFDZAEQoboAiFDVCEwgYo4j/m0Ztq5CykAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc10c197a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#How do the filters in the first layer look like?\n",
    "\n",
    "plot_mat(CW1.get_value(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iii = predict.maker.inputs[0]\n",
    "X = iii.variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a function that shows how the network processes an image\n",
    "\n",
    "middle_layers_computer = theano.function([X], [\n",
    "        X,\n",
    "        after_C1,\n",
    "        after_P1,\n",
    "        after_C2,\n",
    "        after_P2\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAEKCAYAAAA2FzjXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBBE8BBE8BPk31/pndh5457sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0e1d77d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAACuCAYAAAASyDeyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0dfed1e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAACrCAYAAABCBaYBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAHiCgg0AnqBgA4An/g9+DkkMsAfVRAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0dfba7b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAADmCAYAAADxw7+zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0dfad1e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAADjCAYAAAChDi4AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nADhBYAOAEwQ2ADhBYAOAE/8GgmEE+o4zpbMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0dfa047d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_num=4\n",
    "\n",
    "middle_layers = middle_layers_computer(X_test_value[img_num:img_num+1])\n",
    "\n",
    "for ml, name in zip(middle_layers, ['X', 'C1', 'P1', 'C2', 'P2']):\n",
    "    plot_mat(ml.transpose(1,0,2,3), cmap='gray')\n",
    "    title(name)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
