{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 30 days\n"
     ]
    }
   ],
   "source": [
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a parameters' dictionary for the sake of easy usage and fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"K\" : 3000,               # was 2000\n",
    "          \"momentum\" : 0.8,         # was 0.9\n",
    "          \"lrate_const\" : 24e-3,    # was 4e-3\n",
    "          \"num_filters_1\" : 50,     # was 10\n",
    "          \"num_filters_2\" : 75,     # was 25\n",
    "          \"num_fw3_hidden\" : 500,   # was 500\n",
    "          \"gauss\" : 0.025}          # was 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will now build a convolutional network for the CIFAR-10 data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = ((ScaleAndShift, [2.0 / 255.0, -1], {\"which_sources\" : \"features\"}),\n",
    "                                (Cast, [np.float32], {\"which_sources\" : \"features\"}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset = slice(None, 40000))\n",
    "# this stream will shuffle the CIFAR-10 set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(cifar10_train,\n",
    "                                                 iteration_scheme = ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset = slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these don't do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(cifar10_validation,\n",
    "                                                      iteration_scheme = SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(cifar10_test,\n",
    "                                                iteration_scheme = SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are taken from https://github.com/mila-udem/blocks.\n",
    "class Constant():\n",
    "    '''Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    '''\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype = np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    '''Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    '''\n",
    "    def __init__(self, std = 1, mean = 0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size = shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    '''Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width / 2, mean + width / 2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    '''\n",
    "    def __init__(self, mean = 0., width = None, std = None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1 / 12 * width ^ 2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size = shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4(\"X\")\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix(\"Y\", dtype = \"uint8\")\n",
    "\n",
    "# The tag values are useful during debugging the creation of Theano graphs\n",
    "X_test_value, Y_test_value = next(cifar10_train_stream.get_epoch_iterator())\n",
    "\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values\n",
    "\n",
    "X.tag.test_value = X_test_value[: 3]\n",
    "Y.tag.test_value = Y_test_value[: 3]\n",
    "\n",
    "print \"X shape: %s\" % (X.tag.test_value.shape,)\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "# The first convolutional layer\n",
    "# The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = params[\"num_filters_1\"] # we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1, 3, 5, 5), dtype = \"float32\"),\n",
    "                    name = \"CW1\")\n",
    "# please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype = \"float32\"),\n",
    "                    name = \"CB1\")\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2, 2), ignore_border = True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "num_filters_2 = params[\"num_filters_2\"] # we will compute ten convolution filters in the first layer # was 25\n",
    "CW2 = theano.shared(np.zeros((num_filters_2, num_filters_1, 5, 5), dtype = \"float32\"),\n",
    "                   name = \"CW2\")\n",
    "CW2.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype = \"float32\"),\n",
    "                    name = \"CB2\")\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2, 2), ignore_border = True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "# Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden = params[\"num_fw3_hidden\"]\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW3\")\n",
    "FW3.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB3\")\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle(\"x\", 0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "num_fw4_hidden = 10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW4\")\n",
    "FW4.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB4\")\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle(\"x\", 0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis = 1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions, Y.ravel()).mean()\n",
    "nll = -theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1] == \"W\":\n",
    "        weight_decay += 1e-3 * (p ** 2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "# At this point stop computing test values\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "# svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar(\"lrate\", dtype = \"float32\")\n",
    "momentum = theano.tensor.scalar(\"momentum\", dtype = \"float32\")\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "# initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name = \"V_%s\" % (p.name,)) for p in model_parameters]\n",
    "\n",
    "for p, g, v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v, v_new), (p, p_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile theano functions\n",
    "\n",
    "# each call to train step will make one SGD step\n",
    "train_step = theano.function([X, Y, lrate, momentum], [cost, error_rate, nll, weight_decay], updates = updates)\n",
    "# each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X) != Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities to save values of parameters and to load them\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow = False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.443702, batch nll 1.817657, batch error rate 56.000000%\n",
      "At minibatch 200, batch loss 2.176148, batch nll 1.572055, batch error rate 60.000000%\n",
      "At minibatch 300, batch loss 2.065038, batch nll 1.482115, batch error rate 52.000000%\n",
      "At minibatch 400, batch loss 2.140155, batch nll 1.575801, batch error rate 52.000000%\n",
      "At minibatch 500, batch loss 2.083649, batch nll 1.537091, batch error rate 64.000000%\n",
      "At minibatch 600, batch loss 2.055366, batch nll 1.525197, batch error rate 64.000000%\n",
      "At minibatch 700, batch loss 1.776296, batch nll 1.262070, batch error rate 52.000000%\n",
      "At minibatch 800, batch loss 2.046129, batch nll 1.546454, batch error rate 56.000000%\n",
      "At minibatch 900, batch loss 1.969303, batch nll 1.483163, batch error rate 64.000000%\n",
      "At minibatch 1000, batch loss 1.961746, batch nll 1.487659, batch error rate 56.000000%\n",
      "At minibatch 1100, batch loss 2.039221, batch nll 1.576947, batch error rate 48.000000%\n",
      "At minibatch 1200, batch loss 1.724337, batch nll 1.271950, batch error rate 52.000000%\n",
      "At minibatch 1300, batch loss 2.101379, batch nll 1.659124, batch error rate 72.000000%\n",
      "At minibatch 1400, batch loss 1.548010, batch nll 1.116652, batch error rate 40.000000%\n",
      "At minibatch 1500, batch loss 1.649338, batch nll 1.225366, batch error rate 44.000000%\n",
      "At minibatch 1600, batch loss 1.531635, batch nll 1.114936, batch error rate 36.000000%\n",
      "After epoch 1: valid_err_rate: 43.670000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 54.592500% averaged train nll: 1.509033 averaged train loss: 2.019776\n",
      "At minibatch 1700, batch loss 1.814498, batch nll 1.403620, batch error rate 48.000000%\n",
      "At minibatch 1800, batch loss 1.365499, batch nll 0.960939, batch error rate 36.000000%\n",
      "At minibatch 1900, batch loss 1.469981, batch nll 1.070598, batch error rate 40.000000%\n",
      "At minibatch 2000, batch loss 2.067523, batch nll 1.672977, batch error rate 72.000000%\n",
      "At minibatch 2100, batch loss 1.618713, batch nll 1.228535, batch error rate 56.000000%\n",
      "At minibatch 2200, batch loss 1.700240, batch nll 1.314319, batch error rate 48.000000%\n",
      "At minibatch 2300, batch loss 2.327643, batch nll 1.947414, batch error rate 56.000000%\n",
      "At minibatch 2400, batch loss 1.445938, batch nll 1.070363, batch error rate 40.000000%\n",
      "At minibatch 2500, batch loss 2.056674, batch nll 1.685248, batch error rate 56.000000%\n",
      "At minibatch 2600, batch loss 1.627203, batch nll 1.257666, batch error rate 36.000000%\n",
      "At minibatch 2700, batch loss 1.411482, batch nll 1.044755, batch error rate 32.000000%\n",
      "At minibatch 2800, batch loss 1.499839, batch nll 1.136240, batch error rate 44.000000%\n",
      "At minibatch 2900, batch loss 1.557080, batch nll 1.197203, batch error rate 48.000000%\n",
      "At minibatch 3000, batch loss 1.222983, batch nll 0.865124, batch error rate 36.000000%\n",
      "At minibatch 3100, batch loss 1.754620, batch nll 1.398985, batch error rate 44.000000%\n",
      "At minibatch 3200, batch loss 1.207654, batch nll 0.854211, batch error rate 36.000000%\n",
      "After epoch 2: valid_err_rate: 34.430000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 39.420000% averaged train nll: 1.123612 averaged train loss: 1.503111\n",
      "At minibatch 3300, batch loss 1.119860, batch nll 0.766800, batch error rate 16.000000%\n",
      "At minibatch 3400, batch loss 0.906987, batch nll 0.553128, batch error rate 20.000000%\n",
      "At minibatch 3500, batch loss 1.515420, batch nll 1.163338, batch error rate 32.000000%\n",
      "At minibatch 3600, batch loss 1.190681, batch nll 0.840897, batch error rate 20.000000%\n",
      "At minibatch 3700, batch loss 1.345731, batch nll 0.996961, batch error rate 40.000000%\n",
      "At minibatch 3800, batch loss 1.359294, batch nll 1.013006, batch error rate 32.000000%\n",
      "At minibatch 3900, batch loss 1.900903, batch nll 1.557043, batch error rate 44.000000%\n",
      "At minibatch 4000, batch loss 1.446747, batch nll 1.105432, batch error rate 44.000000%\n",
      "At minibatch 4100, batch loss 1.198528, batch nll 0.859853, batch error rate 32.000000%\n",
      "At minibatch 4200, batch loss 0.938024, batch nll 0.601000, batch error rate 20.000000%\n",
      "At minibatch 4300, batch loss 1.212656, batch nll 0.877827, batch error rate 28.000000%\n",
      "At minibatch 4400, batch loss 0.976329, batch nll 0.644361, batch error rate 28.000000%\n",
      "At minibatch 4500, batch loss 1.264226, batch nll 0.934160, batch error rate 28.000000%\n",
      "At minibatch 4600, batch loss 1.263102, batch nll 0.936310, batch error rate 28.000000%\n",
      "At minibatch 4700, batch loss 1.140342, batch nll 0.814853, batch error rate 32.000000%\n",
      "At minibatch 4800, batch loss 1.102635, batch nll 0.780210, batch error rate 40.000000%\n",
      "After epoch 3: valid_err_rate: 30.030000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 31.810000% averaged train nll: 0.908224 averaged train loss: 1.249025\n",
      "At minibatch 4900, batch loss 1.014624, batch nll 0.691551, batch error rate 32.000000%\n",
      "At minibatch 5000, batch loss 1.355034, batch nll 1.033007, batch error rate 36.000000%\n",
      "At minibatch 5100, batch loss 0.846991, batch nll 0.524723, batch error rate 16.000000%\n",
      "At minibatch 5200, batch loss 1.083899, batch nll 0.762998, batch error rate 28.000000%\n",
      "At minibatch 5300, batch loss 1.290612, batch nll 0.970561, batch error rate 28.000000%\n",
      "At minibatch 5400, batch loss 1.099988, batch nll 0.781108, batch error rate 32.000000%\n",
      "At minibatch 5500, batch loss 1.253812, batch nll 0.936491, batch error rate 32.000000%\n",
      "At minibatch 5600, batch loss 0.876202, batch nll 0.558815, batch error rate 16.000000%\n",
      "At minibatch 5700, batch loss 0.977588, batch nll 0.661109, batch error rate 24.000000%\n",
      "At minibatch 5800, batch loss 0.721786, batch nll 0.406788, batch error rate 16.000000%\n",
      "At minibatch 5900, batch loss 1.294227, batch nll 0.980173, batch error rate 28.000000%\n",
      "At minibatch 6000, batch loss 0.926715, batch nll 0.613674, batch error rate 32.000000%\n",
      "At minibatch 6100, batch loss 0.782373, batch nll 0.470229, batch error rate 12.000000%\n",
      "At minibatch 6200, batch loss 0.892148, batch nll 0.580512, batch error rate 28.000000%\n",
      "At minibatch 6300, batch loss 1.017329, batch nll 0.707426, batch error rate 32.000000%\n",
      "At minibatch 6400, batch loss 1.240990, batch nll 0.932180, batch error rate 32.000000%\n",
      "After epoch 4: valid_err_rate: 28.730000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 26.575000% averaged train nll: 0.763550 averaged train loss: 1.080427\n",
      "At minibatch 6500, batch loss 1.174408, batch nll 0.865233, batch error rate 28.000000%\n",
      "At minibatch 6600, batch loss 1.206978, batch nll 0.898212, batch error rate 24.000000%\n",
      "At minibatch 6700, batch loss 0.720036, batch nll 0.411187, batch error rate 20.000000%\n",
      "At minibatch 6800, batch loss 1.064737, batch nll 0.756168, batch error rate 20.000000%\n",
      "At minibatch 6900, batch loss 1.035503, batch nll 0.727312, batch error rate 28.000000%\n",
      "At minibatch 7000, batch loss 1.031824, batch nll 0.724284, batch error rate 28.000000%\n",
      "At minibatch 7100, batch loss 0.969366, batch nll 0.662448, batch error rate 24.000000%\n",
      "At minibatch 7200, batch loss 1.253932, batch nll 0.947332, batch error rate 32.000000%\n",
      "At minibatch 7300, batch loss 0.818270, batch nll 0.512778, batch error rate 20.000000%\n",
      "At minibatch 7400, batch loss 0.815644, batch nll 0.510662, batch error rate 24.000000%\n",
      "At minibatch 7500, batch loss 1.110223, batch nll 0.805786, batch error rate 28.000000%\n",
      "At minibatch 7600, batch loss 0.958751, batch nll 0.654848, batch error rate 24.000000%\n",
      "At minibatch 7700, batch loss 0.879800, batch nll 0.576321, batch error rate 24.000000%\n",
      "At minibatch 7800, batch loss 0.806929, batch nll 0.503950, batch error rate 24.000000%\n",
      "At minibatch 7900, batch loss 1.087230, batch nll 0.784447, batch error rate 28.000000%\n",
      "At minibatch 8000, batch loss 0.979380, batch nll 0.677532, batch error rate 20.000000%\n",
      "After epoch 5: valid_err_rate: 25.650000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 23.207500% averaged train nll: 0.665159 averaged train loss: 0.971239\n",
      "At minibatch 8100, batch loss 0.955118, batch nll 0.652728, batch error rate 20.000000%\n",
      "At minibatch 8200, batch loss 0.705165, batch nll 0.402562, batch error rate 12.000000%\n",
      "At minibatch 8300, batch loss 0.691007, batch nll 0.388310, batch error rate 16.000000%\n",
      "At minibatch 8400, batch loss 0.983560, batch nll 0.681136, batch error rate 28.000000%\n",
      "At minibatch 8500, batch loss 0.808755, batch nll 0.506506, batch error rate 24.000000%\n",
      "At minibatch 8600, batch loss 0.791741, batch nll 0.489405, batch error rate 16.000000%\n",
      "At minibatch 8700, batch loss 0.703758, batch nll 0.401238, batch error rate 12.000000%\n",
      "At minibatch 8800, batch loss 0.914498, batch nll 0.612398, batch error rate 24.000000%\n",
      "At minibatch 8900, batch loss 0.939633, batch nll 0.637459, batch error rate 24.000000%\n",
      "At minibatch 9000, batch loss 1.016957, batch nll 0.714830, batch error rate 20.000000%\n",
      "At minibatch 9100, batch loss 0.981015, batch nll 0.679344, batch error rate 36.000000%\n",
      "At minibatch 9200, batch loss 0.799292, batch nll 0.497936, batch error rate 20.000000%\n",
      "At minibatch 9300, batch loss 0.862303, batch nll 0.560799, batch error rate 20.000000%\n",
      "At minibatch 9400, batch loss 1.021904, batch nll 0.720838, batch error rate 24.000000%\n",
      "At minibatch 9500, batch loss 0.954074, batch nll 0.653238, batch error rate 28.000000%\n",
      "At minibatch 9600, batch loss 1.341976, batch nll 1.041597, batch error rate 32.000000%\n",
      "After epoch 6: valid_err_rate: 24.030000% currently going to do 10 epochs\n",
      "After epoch 6: averaged train_err_rate: 20.432500% averaged train nll: 0.589688 averaged train loss: 0.891624\n",
      "At minibatch 9700, batch loss 1.062288, batch nll 0.761138, batch error rate 40.000000%\n",
      "At minibatch 9800, batch loss 0.651912, batch nll 0.350477, batch error rate 12.000000%\n",
      "At minibatch 9900, batch loss 0.730807, batch nll 0.429104, batch error rate 8.000000%\n",
      "At minibatch 10000, batch loss 0.966856, batch nll 0.664489, batch error rate 32.000000%\n",
      "At minibatch 10100, batch loss 1.007790, batch nll 0.705229, batch error rate 20.000000%\n",
      "At minibatch 10200, batch loss 0.814621, batch nll 0.512523, batch error rate 16.000000%\n",
      "At minibatch 10300, batch loss 1.332040, batch nll 1.029783, batch error rate 20.000000%\n",
      "At minibatch 10400, batch loss 0.786143, batch nll 0.483786, batch error rate 12.000000%\n",
      "At minibatch 10500, batch loss 1.321886, batch nll 1.019396, batch error rate 28.000000%\n",
      "At minibatch 10600, batch loss 0.968234, batch nll 0.666225, batch error rate 24.000000%\n",
      "At minibatch 10700, batch loss 0.596648, batch nll 0.294743, batch error rate 12.000000%\n",
      "At minibatch 10800, batch loss 0.626589, batch nll 0.324438, batch error rate 8.000000%\n",
      "At minibatch 10900, batch loss 0.841230, batch nll 0.539393, batch error rate 16.000000%\n",
      "At minibatch 11000, batch loss 0.796244, batch nll 0.494526, batch error rate 12.000000%\n",
      "At minibatch 11100, batch loss 0.858106, batch nll 0.556436, batch error rate 16.000000%\n",
      "At minibatch 11200, batch loss 0.649760, batch nll 0.348330, batch error rate 16.000000%\n",
      "After epoch 7: valid_err_rate: 23.810000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 18.377500% averaged train nll: 0.532741 averaged train loss: 0.834604\n",
      "At minibatch 11300, batch loss 0.778710, batch nll 0.476861, batch error rate 20.000000%\n",
      "At minibatch 11400, batch loss 0.855100, batch nll 0.552916, batch error rate 12.000000%\n",
      "At minibatch 11500, batch loss 0.724890, batch nll 0.422686, batch error rate 12.000000%\n",
      "At minibatch 11600, batch loss 0.610588, batch nll 0.308293, batch error rate 16.000000%\n",
      "At minibatch 11700, batch loss 0.892344, batch nll 0.589695, batch error rate 36.000000%\n",
      "At minibatch 11800, batch loss 0.793909, batch nll 0.490833, batch error rate 12.000000%\n",
      "At minibatch 11900, batch loss 0.550577, batch nll 0.247704, batch error rate 4.000000%\n",
      "At minibatch 12000, batch loss 1.119233, batch nll 0.815803, batch error rate 28.000000%\n",
      "At minibatch 12100, batch loss 0.709122, batch nll 0.405609, batch error rate 12.000000%\n",
      "At minibatch 12200, batch loss 0.923437, batch nll 0.619649, batch error rate 28.000000%\n",
      "At minibatch 12300, batch loss 1.094097, batch nll 0.790226, batch error rate 28.000000%\n",
      "At minibatch 12400, batch loss 1.212769, batch nll 0.909213, batch error rate 28.000000%\n",
      "At minibatch 12500, batch loss 0.498284, batch nll 0.194887, batch error rate 0.000000%\n",
      "At minibatch 12600, batch loss 0.945687, batch nll 0.642398, batch error rate 32.000000%\n",
      "At minibatch 12700, batch loss 0.670975, batch nll 0.367638, batch error rate 16.000000%\n",
      "At minibatch 12800, batch loss 0.812282, batch nll 0.509103, batch error rate 16.000000%\n",
      "After epoch 8: valid_err_rate: 22.980000% currently going to do 13 epochs\n",
      "After epoch 8: averaged train_err_rate: 16.720000% averaged train nll: 0.484973 averaged train loss: 0.787951\n",
      "At minibatch 12900, batch loss 0.667950, batch nll 0.364245, batch error rate 12.000000%\n",
      "At minibatch 13000, batch loss 0.539450, batch nll 0.235050, batch error rate 4.000000%\n",
      "At minibatch 13100, batch loss 0.731705, batch nll 0.427274, batch error rate 8.000000%\n",
      "At minibatch 13200, batch loss 0.665248, batch nll 0.360413, batch error rate 12.000000%\n",
      "At minibatch 13300, batch loss 0.641833, batch nll 0.336617, batch error rate 12.000000%\n",
      "At minibatch 13400, batch loss 0.683073, batch nll 0.377706, batch error rate 16.000000%\n",
      "At minibatch 13500, batch loss 0.620541, batch nll 0.314772, batch error rate 4.000000%\n",
      "At minibatch 13600, batch loss 0.913016, batch nll 0.607337, batch error rate 32.000000%\n",
      "At minibatch 13700, batch loss 0.687823, batch nll 0.381918, batch error rate 12.000000%\n",
      "At minibatch 13800, batch loss 0.568731, batch nll 0.262854, batch error rate 8.000000%\n",
      "At minibatch 13900, batch loss 1.041339, batch nll 0.735247, batch error rate 24.000000%\n",
      "At minibatch 14000, batch loss 0.474039, batch nll 0.168096, batch error rate 4.000000%\n",
      "At minibatch 14100, batch loss 0.694491, batch nll 0.388217, batch error rate 12.000000%\n",
      "At minibatch 14200, batch loss 0.793049, batch nll 0.486808, batch error rate 12.000000%\n",
      "At minibatch 14300, batch loss 1.243914, batch nll 0.937668, batch error rate 36.000000%\n",
      "At minibatch 14400, batch loss 1.285681, batch nll 0.979507, batch error rate 32.000000%\n",
      "After epoch 9: valid_err_rate: 21.760000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 15.007500% averaged train nll: 0.439998 averaged train loss: 0.745392\n",
      "At minibatch 14500, batch loss 0.644067, batch nll 0.337566, batch error rate 20.000000%\n",
      "At minibatch 14600, batch loss 0.581221, batch nll 0.274211, batch error rate 8.000000%\n",
      "At minibatch 14700, batch loss 0.802728, batch nll 0.495485, batch error rate 20.000000%\n",
      "At minibatch 14800, batch loss 0.690300, batch nll 0.382453, batch error rate 16.000000%\n",
      "At minibatch 14900, batch loss 0.736391, batch nll 0.428371, batch error rate 8.000000%\n",
      "At minibatch 15000, batch loss 0.766815, batch nll 0.458511, batch error rate 12.000000%\n",
      "At minibatch 15100, batch loss 0.609529, batch nll 0.300846, batch error rate 16.000000%\n",
      "At minibatch 15200, batch loss 1.095959, batch nll 0.786845, batch error rate 28.000000%\n",
      "At minibatch 15300, batch loss 0.949939, batch nll 0.640718, batch error rate 24.000000%\n",
      "At minibatch 15400, batch loss 0.654318, batch nll 0.345277, batch error rate 12.000000%\n",
      "At minibatch 15500, batch loss 0.945411, batch nll 0.636344, batch error rate 20.000000%\n",
      "At minibatch 15600, batch loss 0.781888, batch nll 0.472706, batch error rate 16.000000%\n",
      "At minibatch 15700, batch loss 0.829248, batch nll 0.520183, batch error rate 16.000000%\n",
      "At minibatch 15800, batch loss 0.885092, batch nll 0.576164, batch error rate 16.000000%\n",
      "At minibatch 15900, batch loss 0.652390, batch nll 0.343137, batch error rate 12.000000%\n",
      "At minibatch 16000, batch loss 0.718895, batch nll 0.409475, batch error rate 12.000000%\n",
      "After epoch 10: valid_err_rate: 22.250000% currently going to do 14 epochs\n",
      "After epoch 10: averaged train_err_rate: 13.535000% averaged train nll: 0.402116 averaged train loss: 0.710498\n",
      "At minibatch 16100, batch loss 0.620126, batch nll 0.310135, batch error rate 8.000000%\n",
      "At minibatch 16200, batch loss 0.565906, batch nll 0.255595, batch error rate 8.000000%\n",
      "At minibatch 16300, batch loss 0.705503, batch nll 0.394910, batch error rate 16.000000%\n",
      "At minibatch 16400, batch loss 0.833289, batch nll 0.522436, batch error rate 16.000000%\n",
      "At minibatch 16500, batch loss 0.653647, batch nll 0.342471, batch error rate 12.000000%\n",
      "At minibatch 16600, batch loss 0.589932, batch nll 0.278597, batch error rate 12.000000%\n",
      "At minibatch 16700, batch loss 0.854988, batch nll 0.543307, batch error rate 16.000000%\n",
      "At minibatch 16800, batch loss 0.756618, batch nll 0.444677, batch error rate 12.000000%\n",
      "At minibatch 16900, batch loss 0.647661, batch nll 0.335485, batch error rate 16.000000%\n",
      "At minibatch 17000, batch loss 0.820459, batch nll 0.508276, batch error rate 20.000000%\n",
      "At minibatch 17100, batch loss 0.852257, batch nll 0.539971, batch error rate 24.000000%\n",
      "At minibatch 17200, batch loss 0.576531, batch nll 0.264215, batch error rate 8.000000%\n",
      "At minibatch 17300, batch loss 0.535450, batch nll 0.223006, batch error rate 8.000000%\n",
      "At minibatch 17400, batch loss 0.695532, batch nll 0.382983, batch error rate 16.000000%\n",
      "At minibatch 17500, batch loss 0.561121, batch nll 0.248067, batch error rate 8.000000%\n",
      "At minibatch 17600, batch loss 0.601030, batch nll 0.288148, batch error rate 4.000000%\n",
      "After epoch 11: valid_err_rate: 22.370000% currently going to do 14 epochs\n",
      "After epoch 11: averaged train_err_rate: 12.282500% averaged train nll: 0.369207 averaged train loss: 0.680837\n",
      "At minibatch 17700, batch loss 0.738452, batch nll 0.425278, batch error rate 12.000000%\n",
      "At minibatch 17800, batch loss 0.602244, batch nll 0.288681, batch error rate 8.000000%\n",
      "At minibatch 17900, batch loss 0.569930, batch nll 0.256064, batch error rate 8.000000%\n",
      "At minibatch 18000, batch loss 0.624643, batch nll 0.310409, batch error rate 12.000000%\n",
      "At minibatch 18100, batch loss 0.925832, batch nll 0.611491, batch error rate 16.000000%\n",
      "At minibatch 18200, batch loss 0.770491, batch nll 0.455832, batch error rate 16.000000%\n",
      "At minibatch 18300, batch loss 0.642544, batch nll 0.327757, batch error rate 8.000000%\n",
      "At minibatch 18400, batch loss 0.640142, batch nll 0.325344, batch error rate 16.000000%\n",
      "At minibatch 18500, batch loss 0.629186, batch nll 0.314113, batch error rate 12.000000%\n",
      "At minibatch 18600, batch loss 0.658223, batch nll 0.343082, batch error rate 12.000000%\n",
      "At minibatch 18700, batch loss 0.504287, batch nll 0.189081, batch error rate 8.000000%\n",
      "At minibatch 18800, batch loss 0.736304, batch nll 0.420853, batch error rate 20.000000%\n",
      "At minibatch 18900, batch loss 0.710024, batch nll 0.394462, batch error rate 16.000000%\n",
      "At minibatch 19000, batch loss 0.605352, batch nll 0.289786, batch error rate 8.000000%\n",
      "At minibatch 19100, batch loss 0.648145, batch nll 0.332625, batch error rate 8.000000%\n",
      "At minibatch 19200, batch loss 0.457779, batch nll 0.142127, batch error rate 4.000000%\n",
      "After epoch 12: valid_err_rate: 22.220000% currently going to do 14 epochs\n",
      "After epoch 12: averaged train_err_rate: 11.132500% averaged train nll: 0.339315 averaged train loss: 0.654023\n",
      "At minibatch 19300, batch loss 0.448849, batch nll 0.132834, batch error rate 0.000000%\n",
      "At minibatch 19400, batch loss 0.814108, batch nll 0.498005, batch error rate 28.000000%\n",
      "At minibatch 19500, batch loss 0.963377, batch nll 0.646916, batch error rate 16.000000%\n",
      "At minibatch 19600, batch loss 0.563249, batch nll 0.246569, batch error rate 8.000000%\n",
      "At minibatch 19700, batch loss 0.776777, batch nll 0.459979, batch error rate 20.000000%\n",
      "At minibatch 19800, batch loss 0.562723, batch nll 0.245837, batch error rate 4.000000%\n",
      "At minibatch 19900, batch loss 0.998311, batch nll 0.681378, batch error rate 20.000000%\n",
      "At minibatch 20000, batch loss 0.752017, batch nll 0.434831, batch error rate 24.000000%\n",
      "At minibatch 20100, batch loss 0.643488, batch nll 0.326041, batch error rate 12.000000%\n",
      "At minibatch 20200, batch loss 0.666402, batch nll 0.348615, batch error rate 16.000000%\n",
      "At minibatch 20300, batch loss 0.588836, batch nll 0.270908, batch error rate 8.000000%\n",
      "At minibatch 20400, batch loss 0.648098, batch nll 0.330107, batch error rate 20.000000%\n",
      "At minibatch 20500, batch loss 0.662282, batch nll 0.344164, batch error rate 8.000000%\n",
      "At minibatch 20600, batch loss 0.495244, batch nll 0.176983, batch error rate 4.000000%\n",
      "At minibatch 20700, batch loss 0.783217, batch nll 0.464602, batch error rate 24.000000%\n",
      "At minibatch 20800, batch loss 0.680590, batch nll 0.361902, batch error rate 16.000000%\n",
      "After epoch 13: valid_err_rate: 21.650000% currently going to do 20 epochs\n",
      "After epoch 13: averaged train_err_rate: 10.150000% averaged train nll: 0.313684 averaged train loss: 0.630955\n",
      "At minibatch 20900, batch loss 0.584034, batch nll 0.264923, batch error rate 4.000000%\n",
      "At minibatch 21000, batch loss 0.414156, batch nll 0.094868, batch error rate 0.000000%\n",
      "At minibatch 21100, batch loss 0.703573, batch nll 0.383966, batch error rate 16.000000%\n",
      "At minibatch 21200, batch loss 0.731433, batch nll 0.411599, batch error rate 16.000000%\n",
      "At minibatch 21300, batch loss 0.472840, batch nll 0.152708, batch error rate 8.000000%\n",
      "At minibatch 21400, batch loss 0.453065, batch nll 0.132834, batch error rate 4.000000%\n",
      "At minibatch 21500, batch loss 0.730461, batch nll 0.410165, batch error rate 20.000000%\n",
      "At minibatch 21600, batch loss 0.473899, batch nll 0.153372, batch error rate 4.000000%\n",
      "At minibatch 21700, batch loss 0.576483, batch nll 0.255894, batch error rate 4.000000%\n",
      "At minibatch 21800, batch loss 0.603943, batch nll 0.283220, batch error rate 12.000000%\n",
      "At minibatch 21900, batch loss 0.595279, batch nll 0.274404, batch error rate 8.000000%\n",
      "At minibatch 22000, batch loss 0.599970, batch nll 0.278879, batch error rate 8.000000%\n",
      "At minibatch 22100, batch loss 0.829412, batch nll 0.508161, batch error rate 28.000000%\n",
      "At minibatch 22200, batch loss 0.844135, batch nll 0.522703, batch error rate 16.000000%\n",
      "At minibatch 22300, batch loss 0.416625, batch nll 0.095057, batch error rate 0.000000%\n",
      "At minibatch 22400, batch loss 0.448239, batch nll 0.126641, batch error rate 0.000000%\n",
      "After epoch 14: valid_err_rate: 20.980000% currently going to do 22 epochs\n",
      "After epoch 14: averaged train_err_rate: 9.002500% averaged train nll: 0.284664 averaged train loss: 0.605088\n",
      "At minibatch 22500, batch loss 0.744866, batch nll 0.423015, batch error rate 16.000000%\n",
      "At minibatch 22600, batch loss 0.611122, batch nll 0.289114, batch error rate 12.000000%\n",
      "At minibatch 22700, batch loss 0.488435, batch nll 0.166148, batch error rate 4.000000%\n",
      "At minibatch 22800, batch loss 0.523933, batch nll 0.201422, batch error rate 4.000000%\n",
      "At minibatch 22900, batch loss 0.699070, batch nll 0.376424, batch error rate 12.000000%\n",
      "At minibatch 23000, batch loss 0.645008, batch nll 0.322245, batch error rate 12.000000%\n",
      "At minibatch 23100, batch loss 0.525615, batch nll 0.202601, batch error rate 8.000000%\n",
      "At minibatch 23200, batch loss 0.476164, batch nll 0.153117, batch error rate 4.000000%\n",
      "At minibatch 23300, batch loss 0.612335, batch nll 0.289287, batch error rate 8.000000%\n",
      "At minibatch 23400, batch loss 0.771491, batch nll 0.448474, batch error rate 16.000000%\n",
      "At minibatch 23500, batch loss 0.491492, batch nll 0.168056, batch error rate 4.000000%\n",
      "At minibatch 23600, batch loss 0.682104, batch nll 0.358477, batch error rate 16.000000%\n",
      "At minibatch 23700, batch loss 0.450229, batch nll 0.126439, batch error rate 0.000000%\n",
      "At minibatch 23800, batch loss 0.676421, batch nll 0.352495, batch error rate 12.000000%\n",
      "At minibatch 23900, batch loss 0.544124, batch nll 0.220127, batch error rate 8.000000%\n",
      "At minibatch 24000, batch loss 0.518058, batch nll 0.193895, batch error rate 4.000000%\n",
      "After epoch 15: valid_err_rate: 20.550000% currently going to do 23 epochs\n",
      "After epoch 15: averaged train_err_rate: 8.252500% averaged train nll: 0.265424 averaged train loss: 0.588421\n",
      "At minibatch 24100, batch loss 0.608716, batch nll 0.284344, batch error rate 4.000000%\n",
      "At minibatch 24200, batch loss 0.525165, batch nll 0.200590, batch error rate 4.000000%\n",
      "At minibatch 24300, batch loss 0.568202, batch nll 0.243331, batch error rate 4.000000%\n",
      "At minibatch 24400, batch loss 0.491913, batch nll 0.166910, batch error rate 4.000000%\n",
      "At minibatch 24500, batch loss 0.825872, batch nll 0.500613, batch error rate 4.000000%\n",
      "At minibatch 24600, batch loss 0.590629, batch nll 0.265357, batch error rate 12.000000%\n",
      "At minibatch 24700, batch loss 0.963333, batch nll 0.637887, batch error rate 16.000000%\n",
      "At minibatch 24800, batch loss 0.520257, batch nll 0.194692, batch error rate 12.000000%\n",
      "At minibatch 24900, batch loss 0.527926, batch nll 0.202225, batch error rate 8.000000%\n",
      "At minibatch 25000, batch loss 0.482640, batch nll 0.156870, batch error rate 4.000000%\n",
      "At minibatch 25100, batch loss 0.423301, batch nll 0.097427, batch error rate 0.000000%\n",
      "At minibatch 25200, batch loss 0.472202, batch nll 0.146125, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.647371, batch nll 0.321126, batch error rate 8.000000%\n",
      "At minibatch 25400, batch loss 0.509338, batch nll 0.183084, batch error rate 4.000000%\n",
      "At minibatch 25500, batch loss 0.569137, batch nll 0.242738, batch error rate 4.000000%\n",
      "At minibatch 25600, batch loss 0.553622, batch nll 0.227182, batch error rate 8.000000%\n",
      "After epoch 16: valid_err_rate: 20.830000% currently going to do 23 epochs\n",
      "After epoch 16: averaged train_err_rate: 7.300000% averaged train nll: 0.242042 averaged train loss: 0.567535\n",
      "At minibatch 25700, batch loss 0.488764, batch nll 0.162148, batch error rate 4.000000%\n",
      "At minibatch 25800, batch loss 0.512361, batch nll 0.185625, batch error rate 8.000000%\n",
      "At minibatch 25900, batch loss 0.558671, batch nll 0.231822, batch error rate 4.000000%\n",
      "At minibatch 26000, batch loss 0.484730, batch nll 0.157718, batch error rate 4.000000%\n",
      "At minibatch 26100, batch loss 0.494651, batch nll 0.167426, batch error rate 4.000000%\n",
      "At minibatch 26200, batch loss 0.575230, batch nll 0.247876, batch error rate 8.000000%\n",
      "At minibatch 26300, batch loss 0.438685, batch nll 0.111292, batch error rate 0.000000%\n",
      "At minibatch 26400, batch loss 0.592437, batch nll 0.265042, batch error rate 8.000000%\n",
      "At minibatch 26500, batch loss 0.533838, batch nll 0.206516, batch error rate 8.000000%\n",
      "At minibatch 26600, batch loss 0.611225, batch nll 0.283763, batch error rate 8.000000%\n",
      "At minibatch 26700, batch loss 0.650765, batch nll 0.323116, batch error rate 8.000000%\n",
      "At minibatch 26800, batch loss 0.413408, batch nll 0.085522, batch error rate 4.000000%\n",
      "At minibatch 26900, batch loss 0.550008, batch nll 0.221970, batch error rate 12.000000%\n",
      "At minibatch 27000, batch loss 0.701443, batch nll 0.373331, batch error rate 8.000000%\n",
      "At minibatch 27100, batch loss 0.428873, batch nll 0.100654, batch error rate 4.000000%\n",
      "At minibatch 27200, batch loss 0.756554, batch nll 0.428217, batch error rate 16.000000%\n",
      "After epoch 17: valid_err_rate: 20.960000% currently going to do 23 epochs\n",
      "After epoch 17: averaged train_err_rate: 6.600000% averaged train nll: 0.224907 averaged train loss: 0.552330\n",
      "At minibatch 27300, batch loss 0.587689, batch nll 0.259201, batch error rate 8.000000%\n",
      "At minibatch 27400, batch loss 0.454698, batch nll 0.126018, batch error rate 4.000000%\n",
      "At minibatch 27500, batch loss 0.435228, batch nll 0.106430, batch error rate 0.000000%\n",
      "At minibatch 27600, batch loss 0.478969, batch nll 0.150074, batch error rate 4.000000%\n",
      "At minibatch 27700, batch loss 0.705526, batch nll 0.376539, batch error rate 20.000000%\n",
      "At minibatch 27800, batch loss 0.446629, batch nll 0.117505, batch error rate 4.000000%\n",
      "At minibatch 27900, batch loss 0.549187, batch nll 0.219856, batch error rate 4.000000%\n",
      "At minibatch 28000, batch loss 0.396784, batch nll 0.067280, batch error rate 0.000000%\n",
      "At minibatch 28100, batch loss 0.397205, batch nll 0.067615, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.588199, batch nll 0.258514, batch error rate 8.000000%\n",
      "At minibatch 28300, batch loss 0.581284, batch nll 0.251458, batch error rate 12.000000%\n",
      "At minibatch 28400, batch loss 0.504792, batch nll 0.174848, batch error rate 4.000000%\n",
      "At minibatch 28500, batch loss 0.763268, batch nll 0.433356, batch error rate 16.000000%\n",
      "At minibatch 28600, batch loss 0.449042, batch nll 0.119145, batch error rate 4.000000%\n",
      "At minibatch 28700, batch loss 0.513452, batch nll 0.183541, batch error rate 4.000000%\n",
      "At minibatch 28800, batch loss 0.563618, batch nll 0.233559, batch error rate 8.000000%\n",
      "After epoch 18: valid_err_rate: 20.710000% currently going to do 23 epochs\n",
      "After epoch 18: averaged train_err_rate: 6.122500% averaged train nll: 0.208313 averaged train loss: 0.537676\n",
      "At minibatch 28900, batch loss 0.517453, batch nll 0.187382, batch error rate 0.000000%\n",
      "At minibatch 29000, batch loss 0.523215, batch nll 0.193075, batch error rate 12.000000%\n",
      "At minibatch 29100, batch loss 0.453351, batch nll 0.123252, batch error rate 0.000000%\n",
      "At minibatch 29200, batch loss 0.495028, batch nll 0.164768, batch error rate 4.000000%\n",
      "At minibatch 29300, batch loss 0.441528, batch nll 0.111226, batch error rate 4.000000%\n",
      "At minibatch 29400, batch loss 0.602037, batch nll 0.271597, batch error rate 12.000000%\n",
      "At minibatch 29500, batch loss 0.440719, batch nll 0.110208, batch error rate 0.000000%\n",
      "At minibatch 29600, batch loss 0.721196, batch nll 0.390589, batch error rate 16.000000%\n",
      "At minibatch 29700, batch loss 0.706729, batch nll 0.376157, batch error rate 8.000000%\n",
      "At minibatch 29800, batch loss 0.569503, batch nll 0.238810, batch error rate 4.000000%\n",
      "At minibatch 29900, batch loss 0.580471, batch nll 0.249755, batch error rate 16.000000%\n",
      "At minibatch 30000, batch loss 0.451409, batch nll 0.120575, batch error rate 4.000000%\n",
      "At minibatch 30100, batch loss 0.564641, batch nll 0.233826, batch error rate 4.000000%\n",
      "At minibatch 30200, batch loss 0.435928, batch nll 0.104968, batch error rate 4.000000%\n",
      "At minibatch 30300, batch loss 0.434515, batch nll 0.103476, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.620181, batch nll 0.289038, batch error rate 8.000000%\n",
      "After epoch 19: valid_err_rate: 20.990000% currently going to do 23 epochs\n",
      "After epoch 19: averaged train_err_rate: 5.085000% averaged train nll: 0.187833 averaged train loss: 0.518370\n",
      "At minibatch 30500, batch loss 0.489912, batch nll 0.158638, batch error rate 8.000000%\n",
      "At minibatch 30600, batch loss 0.563084, batch nll 0.231684, batch error rate 4.000000%\n",
      "At minibatch 30700, batch loss 0.585303, batch nll 0.253862, batch error rate 8.000000%\n",
      "At minibatch 30800, batch loss 0.522563, batch nll 0.191019, batch error rate 4.000000%\n",
      "At minibatch 30900, batch loss 0.526177, batch nll 0.194588, batch error rate 8.000000%\n",
      "At minibatch 31000, batch loss 0.832326, batch nll 0.500690, batch error rate 8.000000%\n",
      "At minibatch 31100, batch loss 0.592390, batch nll 0.260747, batch error rate 12.000000%\n",
      "At minibatch 31200, batch loss 0.497137, batch nll 0.165538, batch error rate 0.000000%\n",
      "At minibatch 31300, batch loss 0.427694, batch nll 0.096087, batch error rate 0.000000%\n",
      "At minibatch 31400, batch loss 0.523159, batch nll 0.191559, batch error rate 4.000000%\n",
      "At minibatch 31500, batch loss 0.472960, batch nll 0.141373, batch error rate 4.000000%\n",
      "At minibatch 31600, batch loss 0.654298, batch nll 0.322688, batch error rate 12.000000%\n",
      "At minibatch 31700, batch loss 0.508571, batch nll 0.176955, batch error rate 4.000000%\n",
      "At minibatch 31800, batch loss 0.482847, batch nll 0.151194, batch error rate 8.000000%\n",
      "At minibatch 31900, batch loss 0.571310, batch nll 0.239565, batch error rate 8.000000%\n",
      "At minibatch 32000, batch loss 0.617839, batch nll 0.286073, batch error rate 8.000000%\n",
      "After epoch 20: valid_err_rate: 20.930000% currently going to do 23 epochs\n",
      "After epoch 20: averaged train_err_rate: 4.722500% averaged train nll: 0.176270 averaged train loss: 0.507830\n",
      "At minibatch 32100, batch loss 0.492339, batch nll 0.160485, batch error rate 4.000000%\n",
      "At minibatch 32200, batch loss 0.550356, batch nll 0.218459, batch error rate 8.000000%\n",
      "At minibatch 32300, batch loss 0.448575, batch nll 0.116594, batch error rate 4.000000%\n",
      "At minibatch 32400, batch loss 0.613743, batch nll 0.281763, batch error rate 16.000000%\n",
      "At minibatch 32500, batch loss 0.396399, batch nll 0.064372, batch error rate 0.000000%\n",
      "At minibatch 32600, batch loss 0.398221, batch nll 0.066276, batch error rate 0.000000%\n",
      "At minibatch 32700, batch loss 0.436311, batch nll 0.104280, batch error rate 0.000000%\n",
      "At minibatch 32800, batch loss 0.551881, batch nll 0.219796, batch error rate 4.000000%\n",
      "At minibatch 32900, batch loss 0.483055, batch nll 0.150973, batch error rate 4.000000%\n",
      "At minibatch 33000, batch loss 0.492941, batch nll 0.160852, batch error rate 8.000000%\n",
      "At minibatch 33100, batch loss 0.405007, batch nll 0.072953, batch error rate 0.000000%\n",
      "At minibatch 33200, batch loss 0.452271, batch nll 0.120182, batch error rate 4.000000%\n",
      "At minibatch 33300, batch loss 0.436298, batch nll 0.104169, batch error rate 4.000000%\n",
      "At minibatch 33400, batch loss 0.610014, batch nll 0.277766, batch error rate 16.000000%\n",
      "At minibatch 33500, batch loss 0.443171, batch nll 0.110816, batch error rate 4.000000%\n",
      "At minibatch 33600, batch loss 0.501349, batch nll 0.168977, batch error rate 0.000000%\n",
      "After epoch 21: valid_err_rate: 20.670000% currently going to do 23 epochs\n",
      "After epoch 21: averaged train_err_rate: 4.087500% averaged train nll: 0.164093 averaged train loss: 0.496151\n",
      "At minibatch 33700, batch loss 0.454624, batch nll 0.122153, batch error rate 4.000000%\n",
      "At minibatch 33800, batch loss 0.404493, batch nll 0.071977, batch error rate 0.000000%\n",
      "At minibatch 33900, batch loss 0.582623, batch nll 0.250094, batch error rate 4.000000%\n",
      "At minibatch 34000, batch loss 0.519960, batch nll 0.187412, batch error rate 4.000000%\n",
      "At minibatch 34100, batch loss 0.409529, batch nll 0.076891, batch error rate 0.000000%\n",
      "At minibatch 34200, batch loss 0.713551, batch nll 0.380908, batch error rate 12.000000%\n",
      "At minibatch 34300, batch loss 0.446795, batch nll 0.114111, batch error rate 4.000000%\n",
      "At minibatch 34400, batch loss 0.465771, batch nll 0.133006, batch error rate 4.000000%\n",
      "At minibatch 34500, batch loss 0.616390, batch nll 0.283537, batch error rate 12.000000%\n",
      "At minibatch 34600, batch loss 0.431176, batch nll 0.098339, batch error rate 0.000000%\n",
      "At minibatch 34700, batch loss 0.435208, batch nll 0.102392, batch error rate 0.000000%\n",
      "At minibatch 34800, batch loss 0.585098, batch nll 0.252239, batch error rate 12.000000%\n",
      "At minibatch 34900, batch loss 0.497577, batch nll 0.164731, batch error rate 12.000000%\n",
      "At minibatch 35000, batch loss 0.522475, batch nll 0.189635, batch error rate 0.000000%\n",
      "At minibatch 35100, batch loss 0.569286, batch nll 0.236375, batch error rate 12.000000%\n",
      "At minibatch 35200, batch loss 0.477678, batch nll 0.144655, batch error rate 4.000000%\n",
      "After epoch 22: valid_err_rate: 20.940000% currently going to do 23 epochs\n",
      "After epoch 22: averaged train_err_rate: 3.805000% averaged train nll: 0.153335 averaged train loss: 0.486048\n",
      "At minibatch 35300, batch loss 0.476670, batch nll 0.143646, batch error rate 4.000000%\n",
      "At minibatch 35400, batch loss 0.415621, batch nll 0.082663, batch error rate 0.000000%\n",
      "At minibatch 35500, batch loss 0.547002, batch nll 0.214119, batch error rate 8.000000%\n",
      "At minibatch 35600, batch loss 0.534237, batch nll 0.201395, batch error rate 8.000000%\n",
      "At minibatch 35700, batch loss 0.441735, batch nll 0.108899, batch error rate 0.000000%\n",
      "At minibatch 35800, batch loss 0.660136, batch nll 0.327235, batch error rate 12.000000%\n",
      "At minibatch 35900, batch loss 0.437611, batch nll 0.104723, batch error rate 4.000000%\n",
      "At minibatch 36000, batch loss 0.447783, batch nll 0.114858, batch error rate 0.000000%\n",
      "At minibatch 36100, batch loss 0.386894, batch nll 0.053980, batch error rate 0.000000%\n",
      "At minibatch 36200, batch loss 0.425014, batch nll 0.092005, batch error rate 4.000000%\n",
      "At minibatch 36300, batch loss 0.515610, batch nll 0.182627, batch error rate 8.000000%\n",
      "At minibatch 36400, batch loss 0.419745, batch nll 0.086744, batch error rate 0.000000%\n",
      "At minibatch 36500, batch loss 0.497432, batch nll 0.164414, batch error rate 4.000000%\n",
      "At minibatch 36600, batch loss 0.383895, batch nll 0.050913, batch error rate 0.000000%\n",
      "At minibatch 36700, batch loss 0.401298, batch nll 0.068202, batch error rate 0.000000%\n",
      "At minibatch 36800, batch loss 0.422745, batch nll 0.089601, batch error rate 0.000000%\n",
      "After epoch 23: valid_err_rate: 20.890000% currently going to do 23 epochs\n",
      "After epoch 23: averaged train_err_rate: 3.345000% averaged train nll: 0.143564 averaged train loss: 0.476523\n",
      "At minibatch 36900, batch loss 0.423144, batch nll 0.090004, batch error rate 0.000000%\n",
      "At minibatch 37000, batch loss 0.494334, batch nll 0.161205, batch error rate 4.000000%\n",
      "At minibatch 37100, batch loss 0.452829, batch nll 0.119730, batch error rate 8.000000%\n",
      "At minibatch 37200, batch loss 0.391061, batch nll 0.058004, batch error rate 0.000000%\n",
      "At minibatch 37300, batch loss 0.438305, batch nll 0.105292, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.487229, batch nll 0.154184, batch error rate 4.000000%\n",
      "At minibatch 37500, batch loss 0.482452, batch nll 0.149427, batch error rate 4.000000%\n",
      "At minibatch 37600, batch loss 0.589574, batch nll 0.256515, batch error rate 4.000000%\n",
      "At minibatch 37700, batch loss 0.426777, batch nll 0.093709, batch error rate 0.000000%\n",
      "At minibatch 37800, batch loss 0.377226, batch nll 0.044176, batch error rate 0.000000%\n",
      "At minibatch 37900, batch loss 0.419537, batch nll 0.086442, batch error rate 0.000000%\n",
      "At minibatch 38000, batch loss 0.363279, batch nll 0.030209, batch error rate 0.000000%\n",
      "At minibatch 38100, batch loss 0.460282, batch nll 0.127213, batch error rate 4.000000%\n",
      "At minibatch 38200, batch loss 0.500910, batch nll 0.167850, batch error rate 4.000000%\n",
      "At minibatch 38300, batch loss 0.510495, batch nll 0.177427, batch error rate 8.000000%\n",
      "At minibatch 38400, batch loss 0.456947, batch nll 0.123880, batch error rate 4.000000%\n",
      "After epoch 24: valid_err_rate: 20.640000% currently going to do 23 epochs\n",
      "After epoch 24: averaged train_err_rate: 2.875000% averaged train nll: 0.131989 averaged train loss: 0.465065\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while e < number_of_epochs: # This loop goes over epochs\n",
    "    e += 1\n",
    "    \n",
    "    # First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar10_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = params[\"K\"]\n",
    "        lrate = params[\"lrate_const\"] * K / np.maximum(K, i)    # was 4e-3 * ...\n",
    "        momentum = params[\"momentum\"]\n",
    "        \n",
    "        # \n",
    "        if np.random.randint(2) == 1:\n",
    "            X_batch = X_batch[:, :, :, : : -1]\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        # print [p.get_value().ravel()[: 10] for p in model_parameters]\n",
    "        # print [p.get_value().ravel()[: 10] for p in velocities]\n",
    "        \n",
    "        train_loss.append((i, L))\n",
    "        train_erros.append((i, err_rate))\n",
    "        train_nll.append((i, nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate * 100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar10_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion + 1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i, val_error_rate))\n",
    "    \n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" \\\n",
    "          % (e,val_error_rate * 100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" \\\n",
    "          % (e,\n",
    "             np.mean(np.asarray(train_erros)[epoch_start_i :, 1]) * 100, \n",
    "             np.mean(np.asarray(train_nll)[epoch_start_i :, 1]),\n",
    "             np.mean(np.asarray(train_loss)[epoch_start_i :, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 15\n",
      "Test error rate is 21.620000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2a74493250>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a77dfa610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" % (best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" % (compute_error_rate(cifar10_test_stream) * 100.0,)\n",
    "\n",
    "subplot(2, 1, 1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:, 0], train_nll_a[:, 1], label = \"batch train nll\")\n",
    "legend()\n",
    "\n",
    "subplot(2, 1, 2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:, 0], train_erros_a[:, 1], label = \"batch train error rate\")\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:, 0], validation_errors_a[:, 1], label = \"validation error rate\", color = \"r\")\n",
    "ylim(0, 0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results:\n",
      "\n",
      " momentum | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
      "==========+=============++=================+============+===========+================+====================+\n",
      " 0.9      |  2e-3 * ... ||      34.410000% |         27 |        42 |     34.250000% |         20.372500% |\n",
      "          |  4e-3 * ... ||      32.910000% |         15 |        24 |     33.830000% |          9.650000% |\n",
      "          |  8e-3 * ... ||      32.050000% |         14 |        22 |     30.990000% |          0.352500% |\n",
      "          | 12e-3 * ... ||      32.450000% |         10 |        16 |     32.400000% |          1.422500% |\n",
      "          | 16e-3 * ... ||      31.460000% |          9 |        15 |     31.190000% |          2.002500% |\n",
      "          | 20e-3 * ... ||      31.210000% |         31 |        48 |     30.280000% |          0.005000% |\n",
      "          | 24e-3 * ... ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
      "          | 28e-3 * ... ||      31.240000% |         33 |        51 |     30.550000% |          0.010000% |\n",
      "          | 32e-3 * ... ||      32.760000% |         11 |        18 |     32.590000% |          5.222500% |\n",
      "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
      " 0.7      | 24e-3 * ... ||      31.470000% |          9 |        14 |     31.230000% |          4.807500% |\n",
      " 0.75     |             ||      31.410000% |         16 |        25 |     31.010000% |          0.082500% |\n",
      " 0.8      |             ||      30.950000% |          9 |        14 |     30.980000% |          1.715000% |\n",
      " 0.85     |             ||      31.090000% |         26 |        40 |     31.340000% |          0.007500% |\n",
      " 0.9      |             ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
      "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ''' Results:\n",
    "\n",
    " momentum | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
    "==========+=============++=================+============+===========+================+====================+\n",
    " 0.9      |  2e-3 * ... ||      34.410000% |         27 |        42 |     34.250000% |         20.372500% |\n",
    "          |  4e-3 * ... ||      32.910000% |         15 |        24 |     33.830000% |          9.650000% |\n",
    "          |  8e-3 * ... ||      32.050000% |         14 |        22 |     30.990000% |          0.352500% |\n",
    "          | 12e-3 * ... ||      32.450000% |         10 |        16 |     32.400000% |          1.422500% |\n",
    "          | 16e-3 * ... ||      31.460000% |          9 |        15 |     31.190000% |          2.002500% |\n",
    "          | 20e-3 * ... ||      31.210000% |         31 |        48 |     30.280000% |          0.005000% |\n",
    "          | 24e-3 * ... ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "          | 28e-3 * ... ||      31.240000% |         33 |        51 |     30.550000% |          0.010000% |\n",
    "          | 32e-3 * ... ||      32.760000% |         11 |        18 |     32.590000% |          5.222500% |\n",
    "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
    " 0.7      | 24e-3 * ... ||      31.470000% |          9 |        14 |     31.230000% |          4.807500% |\n",
    " 0.75     |             ||      31.410000% |         16 |        25 |     31.010000% |          0.082500% |\n",
    " 0.8      |             ||      30.950000% |          9 |        14 |     30.980000% |          1.715000% |\n",
    " 0.85     |             ||      31.090000% |         26 |        40 |     31.340000% |          0.007500% |\n",
    " 0.9      |             ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_fw3_hidden': 500, 'K': 3000, 'gauss': 0.025, 'momentum': 0.8, 'lrate_const': 0.024, 'num_filters_1': 50, 'num_filters_2': 75}\n"
     ]
    }
   ],
   "source": [
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results:\n",
      "\n",
      " K    | lrate_const | momentum | num_filters_1 | num_filters_2 | num_fw3_hidden | gauss || test error rate\n",
      "======+=============+==========+===============+===============+================+=======++=================\n",
      " 2000 |       0.004 |      0.9 |            10 |            25 |            500 | 0.05  ||        original\n",
      "------+-------------+----------+---------------+---------------+----------------+-------++-----------------\n",
      "      |       0.024 |      0.8 |               |            35 |                |       ||      30.080000%\n",
      "      |       0.024 |      0.8 |            15 |            50 |                |       ||      28.350000%\n",
      "      |       0.024 |      0.8 |            15 |            50 |                | 0.025 ||      27.530000%\n",
      "      |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.910000%\n",
      " 4000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      26.030000%\n",
      " 3000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.690000%\n",
      " 3000 |       0.024 |      0.8 |            50 |            75 |                | 0.025 ||      25.340000%\n"
     ]
    }
   ],
   "source": [
    "print ''' Results:\n",
    "\n",
    " K    | lrate_const | momentum | num_filters_1 | num_filters_2 | num_fw3_hidden | gauss || test error rate\n",
    "======+=============+==========+===============+===============+================+=======++=================\n",
    " 2000 |       0.004 |      0.9 |            10 |            25 |            500 | 0.05  ||        original\n",
    "------+-------------+----------+---------------+---------------+----------------+-------++-----------------\n",
    "      |       0.024 |      0.8 |               |            35 |                |       ||      30.080000%\n",
    "      |       0.024 |      0.8 |            15 |            50 |                |       ||      28.350000%\n",
    "      |       0.024 |      0.8 |            15 |            50 |                | 0.025 ||      27.530000%\n",
    "      |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.910000%\n",
    " 4000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      26.030000%\n",
    " 3000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.690000%\n",
    " 3000 |       0.024 |      0.8 |            50 |            75 |                | 0.025 ||      25.340000%'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
