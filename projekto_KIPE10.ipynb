{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 22 days\n",
      "Vendor:  Continuum Analytics, Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package: mkl\n",
      "Message: trial mode expires in 22 days\n"
     ]
    }
   ],
   "source": [
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a parameters' dictionary for the sake of easy usage and fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"K\"              : 3000,   # was 2000\n",
    "          \"momentum\"       : 0.8,    # was 0.9\n",
    "          \"lrate_const\"    : 24e-3,  # was 4e-3\n",
    "          \"num_filters_1\"  : 50,     # was 10\n",
    "          \"num_filters_2\"  : 75,     # was 25\n",
    "          \"num_fw3_hidden\" : 500,    # was 500\n",
    "          \"num_fw4_hidden\" : 10,     # was 10\n",
    "          \"size_cw1\"       : (5, 5), # was (5, 5)\n",
    "          \"size_cw2\"       : (5, 5), # was (5, 5)\n",
    "          \"size_p1\"        : (2, 2), # was (2, 2)\n",
    "          \"size_p2\"        : (2, 2), # was (2, 2)\n",
    "          \"gauss\"          : 0.025,  # was 0.05\n",
    "          \"decay_const\"    : 1e-3}   # was 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will now build a convolutional network for the CIFAR-10 data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = ((ScaleAndShift, [2.0 / 255.0, -1], {\"which_sources\" : \"features\"}),\n",
    "                                (Cast, [np.float32], {\"which_sources\" : \"features\"}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset = slice(None, 40000))\n",
    "# this stream will shuffle the CIFAR-10 set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(cifar10_train,\n",
    "                                                 iteration_scheme = ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset = slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these don't do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(cifar10_validation,\n",
    "                                                      iteration_scheme = SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(cifar10_test,\n",
    "                                                iteration_scheme = SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are taken from https://github.com/mila-udem/blocks.\n",
    "class Constant():\n",
    "    '''Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    '''\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype = np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    '''Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    '''\n",
    "    def __init__(self, std = 1, mean = 0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size = shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    '''Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width / 2, mean + width / 2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    '''\n",
    "    def __init__(self, mean = 0., width = None, std = None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1 / 12 * width ^ 2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size = shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4(\"X\")\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix(\"Y\", dtype = \"uint8\")\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "# Convolutional layers\n",
    "# The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = params[\"num_filters_1\"]\n",
    "CW1 = theano.shared(np.zeros((num_filters_1, 3) + params[\"size_cw1\"], dtype = \"float32\"), name = \"CW1\")\n",
    "CW1.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype = \"float32\"), name = \"CB1\")\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(0.0, theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, params[\"size_p1\"], ignore_border = True)\n",
    "\n",
    "num_filters_2 = params[\"num_filters_2\"]\n",
    "CW2 = theano.shared(np.zeros((num_filters_2, num_filters_1) + params[\"size_cw2\"], dtype = \"float32\"), name = \"CW2\")\n",
    "CW2.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype = \"float32\"), name = \"CB2\")\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(0.0, theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, params[\"size_p2\"], ignore_border = True)\n",
    "\n",
    "# Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden = params[\"num_fw3_hidden\"]\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype = \"float32\"), name = \"FW3\")\n",
    "FW3.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype = \"float32\"), name = \"FB3\")\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle(\"x\", 0))\n",
    "\n",
    "num_fw4_hidden = params[\"num_fw4_hidden\"]\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype = \"float32\"), name = \"FW4\")\n",
    "FW4.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype = \"float32\"), name = \"FB4\")\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle(\"x\", 0)\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "predictions = theano.tensor.argmax(log_probs, axis = 1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions, Y.ravel()).mean()\n",
    "nll = -theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1] == \"W\":\n",
    "        weight_decay += params[\"decay_const\"] * (p ** 2).sum()\n",
    "\n",
    "cost = nll + weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar(\"lrate\", dtype = \"float32\")\n",
    "momentum = theano.tensor.scalar(\"momentum\", dtype = \"float32\")\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "# initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name = \"V_%s\" % (p.name,)) for p in model_parameters]\n",
    "\n",
    "for p, g, v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v, v_new), (p, p_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /tmp/i258411/theano.NOBACKUP/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.10-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /tmp/i258411/theano.NOBACKUP/compiledir_Linux-3.13--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.10-64/lock_dir/lock\n"
     ]
    }
   ],
   "source": [
    "# compile theano functions\n",
    "\n",
    "# each call to train step will make one SGD step\n",
    "train_step = theano.function([X, Y, lrate, momentum], [cost, error_rate, nll, weight_decay], updates = updates)\n",
    "# each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X) != Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities to save values of parameters and to load them\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow = False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.473640, batch nll 1.847154, batch error rate 68.000000%\n",
      "At minibatch 200, batch loss 2.336056, batch nll 1.731291, batch error rate 60.000000%\n",
      "At minibatch 300, batch loss 2.245302, batch nll 1.660432, batch error rate 68.000000%\n",
      "At minibatch 400, batch loss 1.783961, batch nll 1.218818, batch error rate 52.000000%\n",
      "At minibatch 500, batch loss 2.146228, batch nll 1.598660, batch error rate 60.000000%\n",
      "At minibatch 600, batch loss 1.924858, batch nll 1.394010, batch error rate 48.000000%\n",
      "At minibatch 700, batch loss 1.952334, batch nll 1.437484, batch error rate 48.000000%\n",
      "At minibatch 800, batch loss 2.299674, batch nll 1.800298, batch error rate 60.000000%\n",
      "At minibatch 900, batch loss 2.171168, batch nll 1.685355, batch error rate 52.000000%\n",
      "At minibatch 1000, batch loss 1.585820, batch nll 1.112429, batch error rate 44.000000%\n",
      "At minibatch 1100, batch loss 1.996298, batch nll 1.534715, batch error rate 56.000000%\n",
      "At minibatch 1200, batch loss 1.877988, batch nll 1.427863, batch error rate 48.000000%\n",
      "At minibatch 1300, batch loss 1.785663, batch nll 1.345557, batch error rate 44.000000%\n",
      "At minibatch 1400, batch loss 1.572372, batch nll 1.143265, batch error rate 40.000000%\n",
      "At minibatch 1500, batch loss 1.618888, batch nll 1.198239, batch error rate 44.000000%\n",
      "At minibatch 1600, batch loss 1.690462, batch nll 1.276674, batch error rate 52.000000%\n",
      "After epoch 1: valid_err_rate: 45.980000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 55.142500% averaged train nll: 1.518995 averaged train loss: 2.029160\n",
      "At minibatch 1700, batch loss 1.903661, batch nll 1.496460, batch error rate 64.000000%\n",
      "At minibatch 1800, batch loss 1.455827, batch nll 1.054271, batch error rate 40.000000%\n",
      "At minibatch 1900, batch loss 1.449661, batch nll 1.054277, batch error rate 32.000000%\n",
      "At minibatch 2000, batch loss 1.390267, batch nll 0.999943, batch error rate 36.000000%\n",
      "At minibatch 2100, batch loss 1.515978, batch nll 1.130316, batch error rate 48.000000%\n",
      "At minibatch 2200, batch loss 1.308432, batch nll 0.927625, batch error rate 36.000000%\n",
      "At minibatch 2300, batch loss 1.381393, batch nll 1.005576, batch error rate 32.000000%\n",
      "At minibatch 2400, batch loss 1.336584, batch nll 0.964468, batch error rate 36.000000%\n",
      "At minibatch 2500, batch loss 1.479773, batch nll 1.112036, batch error rate 40.000000%\n",
      "At minibatch 2600, batch loss 1.452917, batch nll 1.087814, batch error rate 40.000000%\n",
      "At minibatch 2700, batch loss 1.509848, batch nll 1.148104, batch error rate 52.000000%\n",
      "At minibatch 2800, batch loss 1.411104, batch nll 1.051346, batch error rate 36.000000%\n",
      "At minibatch 2900, batch loss 1.811996, batch nll 1.454894, batch error rate 48.000000%\n",
      "At minibatch 3000, batch loss 1.171424, batch nll 0.818810, batch error rate 28.000000%\n",
      "At minibatch 3100, batch loss 1.361011, batch nll 1.009685, batch error rate 40.000000%\n",
      "At minibatch 3200, batch loss 1.099938, batch nll 0.751521, batch error rate 28.000000%\n",
      "After epoch 2: valid_err_rate: 36.280000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 40.270000% averaged train nll: 1.144041 averaged train loss: 1.519436\n",
      "At minibatch 3300, batch loss 1.415857, batch nll 1.067096, batch error rate 36.000000%\n",
      "At minibatch 3400, batch loss 1.086468, batch nll 0.739870, batch error rate 32.000000%\n",
      "At minibatch 3500, batch loss 1.361106, batch nll 1.015768, batch error rate 28.000000%\n",
      "At minibatch 3600, batch loss 1.866504, batch nll 1.522503, batch error rate 64.000000%\n",
      "At minibatch 3700, batch loss 1.504580, batch nll 1.161180, batch error rate 36.000000%\n",
      "At minibatch 3800, batch loss 1.523038, batch nll 1.180743, batch error rate 44.000000%\n",
      "At minibatch 3900, batch loss 1.930250, batch nll 1.590692, batch error rate 52.000000%\n",
      "At minibatch 4000, batch loss 1.329571, batch nll 0.991808, batch error rate 36.000000%\n",
      "At minibatch 4100, batch loss 1.611505, batch nll 1.275839, batch error rate 56.000000%\n",
      "At minibatch 4200, batch loss 1.296832, batch nll 0.962025, batch error rate 32.000000%\n",
      "At minibatch 4300, batch loss 1.295826, batch nll 0.962925, batch error rate 28.000000%\n",
      "At minibatch 4400, batch loss 1.376742, batch nll 1.045962, batch error rate 32.000000%\n",
      "At minibatch 4500, batch loss 1.613318, batch nll 1.284579, batch error rate 44.000000%\n",
      "At minibatch 4600, batch loss 1.187014, batch nll 0.860422, batch error rate 32.000000%\n",
      "At minibatch 4700, batch loss 1.587378, batch nll 1.262927, batch error rate 44.000000%\n",
      "At minibatch 4800, batch loss 0.954373, batch nll 0.630919, batch error rate 28.000000%\n",
      "After epoch 3: valid_err_rate: 30.180000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 32.340000% averaged train nll: 0.928432 averaged train loss: 1.265813\n",
      "At minibatch 4900, batch loss 1.122410, batch nll 0.799125, batch error rate 32.000000%\n",
      "At minibatch 5000, batch loss 0.931995, batch nll 0.608422, batch error rate 24.000000%\n",
      "At minibatch 5100, batch loss 1.002013, batch nll 0.678775, batch error rate 24.000000%\n",
      "At minibatch 5200, batch loss 1.195583, batch nll 0.873371, batch error rate 28.000000%\n",
      "At minibatch 5300, batch loss 1.193445, batch nll 0.871812, batch error rate 32.000000%\n",
      "At minibatch 5400, batch loss 1.159579, batch nll 0.838648, batch error rate 32.000000%\n",
      "At minibatch 5500, batch loss 1.557801, batch nll 1.238421, batch error rate 32.000000%\n",
      "At minibatch 5600, batch loss 1.182185, batch nll 0.863344, batch error rate 24.000000%\n",
      "At minibatch 5700, batch loss 1.003184, batch nll 0.686460, batch error rate 20.000000%\n",
      "At minibatch 5800, batch loss 0.820400, batch nll 0.504477, batch error rate 12.000000%\n",
      "At minibatch 5900, batch loss 0.997246, batch nll 0.682484, batch error rate 28.000000%\n",
      "At minibatch 6000, batch loss 0.921970, batch nll 0.607990, batch error rate 28.000000%\n",
      "At minibatch 6100, batch loss 1.159463, batch nll 0.846609, batch error rate 24.000000%\n",
      "At minibatch 6200, batch loss 1.089564, batch nll 0.778126, batch error rate 36.000000%\n",
      "At minibatch 6300, batch loss 1.026387, batch nll 0.715215, batch error rate 28.000000%\n",
      "At minibatch 6400, batch loss 1.227377, batch nll 0.917716, batch error rate 36.000000%\n",
      "After epoch 4: valid_err_rate: 27.370000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 27.257500% averaged train nll: 0.778617 averaged train loss: 1.096575\n",
      "At minibatch 6500, batch loss 1.166588, batch nll 0.856475, batch error rate 36.000000%\n",
      "At minibatch 6600, batch loss 1.095519, batch nll 0.785458, batch error rate 36.000000%\n",
      "At minibatch 6700, batch loss 0.824800, batch nll 0.514085, batch error rate 16.000000%\n",
      "At minibatch 6800, batch loss 1.199337, batch nll 0.889062, batch error rate 28.000000%\n",
      "At minibatch 6900, batch loss 1.232895, batch nll 0.923047, batch error rate 24.000000%\n",
      "At minibatch 7000, batch loss 0.920149, batch nll 0.610184, batch error rate 16.000000%\n",
      "At minibatch 7100, batch loss 1.196687, batch nll 0.887876, batch error rate 32.000000%\n",
      "At minibatch 7200, batch loss 0.980394, batch nll 0.671777, batch error rate 16.000000%\n",
      "At minibatch 7300, batch loss 0.814546, batch nll 0.506376, batch error rate 16.000000%\n",
      "At minibatch 7400, batch loss 0.705372, batch nll 0.397143, batch error rate 12.000000%\n",
      "At minibatch 7500, batch loss 1.319664, batch nll 1.012029, batch error rate 44.000000%\n",
      "At minibatch 7600, batch loss 0.980699, batch nll 0.673624, batch error rate 28.000000%\n",
      "At minibatch 7700, batch loss 0.697869, batch nll 0.391554, batch error rate 16.000000%\n",
      "At minibatch 7800, batch loss 0.871517, batch nll 0.565254, batch error rate 16.000000%\n",
      "At minibatch 7900, batch loss 0.877254, batch nll 0.571875, batch error rate 24.000000%\n",
      "At minibatch 8000, batch loss 1.275795, batch nll 0.970939, batch error rate 36.000000%\n",
      "After epoch 5: valid_err_rate: 26.600000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 23.547500% averaged train nll: 0.678134 averaged train loss: 0.986565\n",
      "At minibatch 8100, batch loss 1.092043, batch nll 0.786548, batch error rate 20.000000%\n",
      "At minibatch 8200, batch loss 1.101898, batch nll 0.796432, batch error rate 24.000000%\n",
      "At minibatch 8300, batch loss 0.695134, batch nll 0.389352, batch error rate 16.000000%\n",
      "At minibatch 8400, batch loss 1.042323, batch nll 0.736593, batch error rate 28.000000%\n",
      "At minibatch 8500, batch loss 0.910465, batch nll 0.604137, batch error rate 24.000000%\n",
      "At minibatch 8600, batch loss 1.087720, batch nll 0.781634, batch error rate 16.000000%\n",
      "At minibatch 8700, batch loss 1.117594, batch nll 0.812023, batch error rate 24.000000%\n",
      "At minibatch 8800, batch loss 0.966163, batch nll 0.660551, batch error rate 16.000000%\n",
      "At minibatch 8900, batch loss 0.708522, batch nll 0.402983, batch error rate 16.000000%\n",
      "At minibatch 9000, batch loss 0.928655, batch nll 0.623566, batch error rate 20.000000%\n",
      "At minibatch 9100, batch loss 0.958388, batch nll 0.653180, batch error rate 24.000000%\n",
      "At minibatch 9200, batch loss 0.807555, batch nll 0.502714, batch error rate 12.000000%\n",
      "At minibatch 9300, batch loss 0.867007, batch nll 0.562464, batch error rate 20.000000%\n",
      "At minibatch 9400, batch loss 0.863833, batch nll 0.559261, batch error rate 28.000000%\n",
      "At minibatch 9500, batch loss 0.972888, batch nll 0.668517, batch error rate 24.000000%\n",
      "At minibatch 9600, batch loss 0.851002, batch nll 0.547033, batch error rate 16.000000%\n",
      "After epoch 6: valid_err_rate: 25.190000% currently going to do 10 epochs\n",
      "After epoch 6: averaged train_err_rate: 20.942500% averaged train nll: 0.601192 averaged train loss: 0.906443\n",
      "At minibatch 9700, batch loss 0.871964, batch nll 0.567102, batch error rate 20.000000%\n",
      "At minibatch 9800, batch loss 0.710641, batch nll 0.405159, batch error rate 12.000000%\n",
      "At minibatch 9900, batch loss 0.655546, batch nll 0.350244, batch error rate 12.000000%\n",
      "At minibatch 10000, batch loss 1.124805, batch nll 0.819742, batch error rate 24.000000%\n",
      "At minibatch 10100, batch loss 0.774191, batch nll 0.468888, batch error rate 16.000000%\n",
      "At minibatch 10200, batch loss 0.701828, batch nll 0.396287, batch error rate 16.000000%\n",
      "At minibatch 10300, batch loss 0.694115, batch nll 0.388437, batch error rate 12.000000%\n",
      "At minibatch 10400, batch loss 0.927553, batch nll 0.621828, batch error rate 20.000000%\n",
      "At minibatch 10500, batch loss 0.762639, batch nll 0.456721, batch error rate 20.000000%\n",
      "At minibatch 10600, batch loss 0.958301, batch nll 0.652257, batch error rate 24.000000%\n",
      "At minibatch 10700, batch loss 0.973362, batch nll 0.667323, batch error rate 20.000000%\n",
      "At minibatch 10800, batch loss 0.694723, batch nll 0.388790, batch error rate 16.000000%\n",
      "At minibatch 10900, batch loss 0.741494, batch nll 0.436144, batch error rate 16.000000%\n",
      "At minibatch 11000, batch loss 0.888551, batch nll 0.583535, batch error rate 20.000000%\n",
      "At minibatch 11100, batch loss 0.908297, batch nll 0.603411, batch error rate 20.000000%\n",
      "At minibatch 11200, batch loss 1.198215, batch nll 0.893701, batch error rate 36.000000%\n",
      "After epoch 7: valid_err_rate: 23.430000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 18.860000% averaged train nll: 0.541965 averaged train loss: 0.847395\n",
      "At minibatch 11300, batch loss 0.937913, batch nll 0.632442, batch error rate 24.000000%\n",
      "At minibatch 11400, batch loss 0.499693, batch nll 0.193830, batch error rate 4.000000%\n",
      "At minibatch 11500, batch loss 0.748385, batch nll 0.442280, batch error rate 20.000000%\n",
      "At minibatch 11600, batch loss 0.814454, batch nll 0.507970, batch error rate 20.000000%\n",
      "At minibatch 11700, batch loss 0.951556, batch nll 0.645040, batch error rate 24.000000%\n",
      "At minibatch 11800, batch loss 0.741026, batch nll 0.434729, batch error rate 16.000000%\n",
      "At minibatch 11900, batch loss 1.052816, batch nll 0.746177, batch error rate 20.000000%\n",
      "At minibatch 12000, batch loss 0.955122, batch nll 0.648304, batch error rate 24.000000%\n",
      "At minibatch 12100, batch loss 1.069816, batch nll 0.762636, batch error rate 24.000000%\n",
      "At minibatch 12200, batch loss 1.083250, batch nll 0.776166, batch error rate 20.000000%\n",
      "At minibatch 12300, batch loss 0.530202, batch nll 0.223040, batch error rate 4.000000%\n",
      "At minibatch 12400, batch loss 0.560235, batch nll 0.253136, batch error rate 8.000000%\n",
      "At minibatch 12500, batch loss 0.769937, batch nll 0.463037, batch error rate 24.000000%\n",
      "At minibatch 12600, batch loss 0.870584, batch nll 0.563531, batch error rate 20.000000%\n",
      "At minibatch 12700, batch loss 0.665363, batch nll 0.358741, batch error rate 12.000000%\n",
      "At minibatch 12800, batch loss 0.589610, batch nll 0.283051, batch error rate 12.000000%\n",
      "After epoch 8: valid_err_rate: 24.030000% currently going to do 11 epochs\n",
      "After epoch 8: averaged train_err_rate: 16.795000% averaged train nll: 0.489527 averaged train loss: 0.796074\n",
      "At minibatch 12900, batch loss 0.610456, batch nll 0.303506, batch error rate 8.000000%\n",
      "At minibatch 13000, batch loss 0.832824, batch nll 0.525476, batch error rate 12.000000%\n",
      "At minibatch 13100, batch loss 0.847744, batch nll 0.540330, batch error rate 20.000000%\n",
      "At minibatch 13200, batch loss 0.699926, batch nll 0.392474, batch error rate 24.000000%\n",
      "At minibatch 13300, batch loss 0.795923, batch nll 0.487932, batch error rate 20.000000%\n",
      "At minibatch 13400, batch loss 0.689635, batch nll 0.381168, batch error rate 16.000000%\n",
      "At minibatch 13500, batch loss 0.587007, batch nll 0.278807, batch error rate 8.000000%\n",
      "At minibatch 13600, batch loss 0.822384, batch nll 0.514121, batch error rate 28.000000%\n",
      "At minibatch 13700, batch loss 0.820890, batch nll 0.512365, batch error rate 16.000000%\n",
      "At minibatch 13800, batch loss 0.843431, batch nll 0.534679, batch error rate 12.000000%\n",
      "At minibatch 13900, batch loss 0.978228, batch nll 0.669463, batch error rate 16.000000%\n",
      "At minibatch 14000, batch loss 1.205235, batch nll 0.896356, batch error rate 32.000000%\n",
      "At minibatch 14100, batch loss 0.535798, batch nll 0.226695, batch error rate 4.000000%\n",
      "At minibatch 14200, batch loss 0.900042, batch nll 0.591031, batch error rate 24.000000%\n",
      "At minibatch 14300, batch loss 0.649343, batch nll 0.340221, batch error rate 12.000000%\n",
      "At minibatch 14400, batch loss 0.839679, batch nll 0.530546, batch error rate 16.000000%\n",
      "After epoch 9: valid_err_rate: 23.350000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 15.332500% averaged train nll: 0.448934 averaged train loss: 0.757180\n",
      "At minibatch 14500, batch loss 0.690824, batch nll 0.381406, batch error rate 8.000000%\n",
      "At minibatch 14600, batch loss 0.511668, batch nll 0.201670, batch error rate 4.000000%\n",
      "At minibatch 14700, batch loss 0.717617, batch nll 0.407471, batch error rate 12.000000%\n",
      "At minibatch 14800, batch loss 0.706812, batch nll 0.396529, batch error rate 16.000000%\n",
      "At minibatch 14900, batch loss 0.654557, batch nll 0.343947, batch error rate 16.000000%\n",
      "At minibatch 15000, batch loss 0.594183, batch nll 0.283546, batch error rate 8.000000%\n",
      "At minibatch 15100, batch loss 0.509109, batch nll 0.198126, batch error rate 4.000000%\n",
      "At minibatch 15200, batch loss 0.787425, batch nll 0.476283, batch error rate 16.000000%\n",
      "At minibatch 15300, batch loss 0.681981, batch nll 0.370770, batch error rate 8.000000%\n",
      "At minibatch 15400, batch loss 0.617631, batch nll 0.306113, batch error rate 12.000000%\n",
      "At minibatch 15500, batch loss 0.699284, batch nll 0.387515, batch error rate 16.000000%\n",
      "At minibatch 15600, batch loss 0.538060, batch nll 0.226591, batch error rate 4.000000%\n",
      "At minibatch 15700, batch loss 0.592932, batch nll 0.281245, batch error rate 4.000000%\n",
      "At minibatch 15800, batch loss 0.631043, batch nll 0.319030, batch error rate 12.000000%\n",
      "At minibatch 15900, batch loss 1.417960, batch nll 1.105755, batch error rate 28.000000%\n",
      "At minibatch 16000, batch loss 1.144543, batch nll 0.832403, batch error rate 24.000000%\n",
      "After epoch 10: valid_err_rate: 22.490000% currently going to do 16 epochs\n",
      "After epoch 10: averaged train_err_rate: 13.715000% averaged train nll: 0.408394 averaged train loss: 0.719376\n",
      "At minibatch 16100, batch loss 0.634665, batch nll 0.322293, batch error rate 12.000000%\n",
      "At minibatch 16200, batch loss 0.643742, batch nll 0.330932, batch error rate 12.000000%\n",
      "At minibatch 16300, batch loss 0.964512, batch nll 0.651356, batch error rate 24.000000%\n",
      "At minibatch 16400, batch loss 0.475767, batch nll 0.162262, batch error rate 0.000000%\n",
      "At minibatch 16500, batch loss 0.658586, batch nll 0.344930, batch error rate 12.000000%\n",
      "At minibatch 16600, batch loss 0.727321, batch nll 0.413246, batch error rate 8.000000%\n",
      "At minibatch 16700, batch loss 0.719241, batch nll 0.404858, batch error rate 16.000000%\n",
      "At minibatch 16800, batch loss 0.920993, batch nll 0.606198, batch error rate 24.000000%\n",
      "At minibatch 16900, batch loss 0.804899, batch nll 0.489791, batch error rate 12.000000%\n",
      "At minibatch 17000, batch loss 0.712225, batch nll 0.397257, batch error rate 16.000000%\n",
      "At minibatch 17100, batch loss 0.903358, batch nll 0.588487, batch error rate 16.000000%\n",
      "At minibatch 17200, batch loss 0.671723, batch nll 0.356829, batch error rate 16.000000%\n",
      "At minibatch 17300, batch loss 0.665754, batch nll 0.350809, batch error rate 4.000000%\n",
      "At minibatch 17400, batch loss 0.892692, batch nll 0.578025, batch error rate 20.000000%\n",
      "At minibatch 17500, batch loss 0.591510, batch nll 0.276584, batch error rate 8.000000%\n",
      "At minibatch 17600, batch loss 0.657091, batch nll 0.342287, batch error rate 16.000000%\n",
      "After epoch 11: valid_err_rate: 22.720000% currently going to do 16 epochs\n",
      "After epoch 11: averaged train_err_rate: 12.475000% averaged train nll: 0.376944 averaged train loss: 0.691107\n",
      "At minibatch 17700, batch loss 0.604008, batch nll 0.288791, batch error rate 8.000000%\n",
      "At minibatch 17800, batch loss 0.566244, batch nll 0.250671, batch error rate 8.000000%\n",
      "At minibatch 17900, batch loss 0.767357, batch nll 0.451452, batch error rate 16.000000%\n",
      "At minibatch 18000, batch loss 0.739830, batch nll 0.423739, batch error rate 16.000000%\n",
      "At minibatch 18100, batch loss 0.767430, batch nll 0.450962, batch error rate 20.000000%\n",
      "At minibatch 18200, batch loss 0.491089, batch nll 0.174474, batch error rate 4.000000%\n",
      "At minibatch 18300, batch loss 0.784286, batch nll 0.467632, batch error rate 16.000000%\n",
      "At minibatch 18400, batch loss 0.644290, batch nll 0.327337, batch error rate 16.000000%\n",
      "At minibatch 18500, batch loss 0.497627, batch nll 0.180433, batch error rate 8.000000%\n",
      "At minibatch 18600, batch loss 0.654417, batch nll 0.337111, batch error rate 8.000000%\n",
      "At minibatch 18700, batch loss 0.991659, batch nll 0.674242, batch error rate 28.000000%\n",
      "At minibatch 18800, batch loss 0.484357, batch nll 0.166802, batch error rate 8.000000%\n",
      "At minibatch 18900, batch loss 0.696311, batch nll 0.378489, batch error rate 20.000000%\n",
      "At minibatch 19000, batch loss 0.594362, batch nll 0.276385, batch error rate 12.000000%\n",
      "At minibatch 19100, batch loss 0.485436, batch nll 0.167064, batch error rate 4.000000%\n",
      "At minibatch 19200, batch loss 0.704461, batch nll 0.386253, batch error rate 16.000000%\n",
      "After epoch 12: valid_err_rate: 22.240000% currently going to do 19 epochs\n",
      "After epoch 12: averaged train_err_rate: 11.357500% averaged train nll: 0.343923 averaged train loss: 0.660776\n",
      "At minibatch 19300, batch loss 0.777901, batch nll 0.459411, batch error rate 24.000000%\n",
      "At minibatch 19400, batch loss 0.567596, batch nll 0.248714, batch error rate 8.000000%\n",
      "At minibatch 19500, batch loss 0.634067, batch nll 0.314736, batch error rate 16.000000%\n",
      "At minibatch 19600, batch loss 0.441856, batch nll 0.122325, batch error rate 0.000000%\n",
      "At minibatch 19700, batch loss 0.749180, batch nll 0.429358, batch error rate 16.000000%\n",
      "At minibatch 19800, batch loss 0.719338, batch nll 0.399362, batch error rate 12.000000%\n",
      "At minibatch 19900, batch loss 0.689990, batch nll 0.369788, batch error rate 12.000000%\n",
      "At minibatch 20000, batch loss 0.695738, batch nll 0.375515, batch error rate 16.000000%\n",
      "At minibatch 20100, batch loss 0.606470, batch nll 0.286055, batch error rate 4.000000%\n",
      "At minibatch 20200, batch loss 0.641037, batch nll 0.320549, batch error rate 12.000000%\n",
      "At minibatch 20300, batch loss 0.478173, batch nll 0.157388, batch error rate 8.000000%\n",
      "At minibatch 20400, batch loss 0.812979, batch nll 0.492115, batch error rate 24.000000%\n",
      "At minibatch 20500, batch loss 0.651677, batch nll 0.330660, batch error rate 12.000000%\n",
      "At minibatch 20600, batch loss 0.587797, batch nll 0.266566, batch error rate 8.000000%\n",
      "At minibatch 20700, batch loss 0.693564, batch nll 0.372367, batch error rate 12.000000%\n",
      "At minibatch 20800, batch loss 0.528770, batch nll 0.207421, batch error rate 8.000000%\n",
      "After epoch 13: valid_err_rate: 22.560000% currently going to do 19 epochs\n",
      "After epoch 13: averaged train_err_rate: 10.310000% averaged train nll: 0.316275 averaged train loss: 0.636413\n",
      "At minibatch 20900, batch loss 0.567290, batch nll 0.245578, batch error rate 4.000000%\n",
      "At minibatch 21000, batch loss 0.788244, batch nll 0.466390, batch error rate 24.000000%\n",
      "At minibatch 21100, batch loss 0.728320, batch nll 0.406110, batch error rate 12.000000%\n",
      "At minibatch 21200, batch loss 0.570165, batch nll 0.247695, batch error rate 4.000000%\n",
      "At minibatch 21300, batch loss 0.666140, batch nll 0.343429, batch error rate 8.000000%\n",
      "At minibatch 21400, batch loss 0.545334, batch nll 0.222453, batch error rate 4.000000%\n",
      "At minibatch 21500, batch loss 0.954384, batch nll 0.631460, batch error rate 24.000000%\n",
      "At minibatch 21600, batch loss 0.654516, batch nll 0.331371, batch error rate 12.000000%\n",
      "At minibatch 21700, batch loss 0.515573, batch nll 0.192340, batch error rate 8.000000%\n",
      "At minibatch 21800, batch loss 0.562029, batch nll 0.238604, batch error rate 8.000000%\n",
      "At minibatch 21900, batch loss 0.680657, batch nll 0.357087, batch error rate 12.000000%\n",
      "At minibatch 22000, batch loss 0.659922, batch nll 0.336206, batch error rate 16.000000%\n",
      "At minibatch 22100, batch loss 0.541672, batch nll 0.217870, batch error rate 4.000000%\n",
      "At minibatch 22200, batch loss 0.469950, batch nll 0.146079, batch error rate 0.000000%\n",
      "At minibatch 22300, batch loss 0.600366, batch nll 0.276155, batch error rate 8.000000%\n",
      "At minibatch 22400, batch loss 0.748359, batch nll 0.424093, batch error rate 20.000000%\n",
      "After epoch 14: valid_err_rate: 22.000000% currently going to do 22 epochs\n",
      "After epoch 14: averaged train_err_rate: 9.202500% averaged train nll: 0.290294 averaged train loss: 0.613324\n",
      "At minibatch 22500, batch loss 0.568989, batch nll 0.244392, batch error rate 4.000000%\n",
      "At minibatch 22600, batch loss 0.385803, batch nll 0.060946, batch error rate 0.000000%\n",
      "At minibatch 22700, batch loss 0.548311, batch nll 0.223280, batch error rate 8.000000%\n",
      "At minibatch 22800, batch loss 0.646060, batch nll 0.320972, batch error rate 12.000000%\n",
      "At minibatch 22900, batch loss 0.491594, batch nll 0.166228, batch error rate 8.000000%\n",
      "At minibatch 23000, batch loss 0.453442, batch nll 0.127924, batch error rate 4.000000%\n",
      "At minibatch 23100, batch loss 0.753133, batch nll 0.427607, batch error rate 16.000000%\n",
      "At minibatch 23200, batch loss 0.558888, batch nll 0.233153, batch error rate 4.000000%\n",
      "At minibatch 23300, batch loss 0.551520, batch nll 0.225711, batch error rate 12.000000%\n",
      "At minibatch 23400, batch loss 0.665015, batch nll 0.339022, batch error rate 16.000000%\n",
      "At minibatch 23500, batch loss 0.573645, batch nll 0.247447, batch error rate 12.000000%\n",
      "At minibatch 23600, batch loss 0.666239, batch nll 0.339929, batch error rate 8.000000%\n",
      "At minibatch 23700, batch loss 0.688197, batch nll 0.361750, batch error rate 12.000000%\n",
      "At minibatch 23800, batch loss 0.833499, batch nll 0.506917, batch error rate 12.000000%\n",
      "At minibatch 23900, batch loss 0.529379, batch nll 0.202699, batch error rate 4.000000%\n",
      "At minibatch 24000, batch loss 0.440683, batch nll 0.113957, batch error rate 0.000000%\n",
      "After epoch 15: valid_err_rate: 21.620000% currently going to do 23 epochs\n",
      "After epoch 15: averaged train_err_rate: 8.247500% averaged train nll: 0.266912 averaged train loss: 0.592623\n",
      "At minibatch 24100, batch loss 0.646690, batch nll 0.319797, batch error rate 8.000000%\n",
      "At minibatch 24200, batch loss 0.647254, batch nll 0.320296, batch error rate 12.000000%\n",
      "At minibatch 24300, batch loss 0.533056, batch nll 0.205878, batch error rate 4.000000%\n",
      "At minibatch 24400, batch loss 0.460265, batch nll 0.132858, batch error rate 4.000000%\n",
      "At minibatch 24500, batch loss 0.578832, batch nll 0.251237, batch error rate 8.000000%\n",
      "At minibatch 24600, batch loss 0.676546, batch nll 0.348798, batch error rate 12.000000%\n",
      "At minibatch 24700, batch loss 0.500560, batch nll 0.172604, batch error rate 4.000000%\n",
      "At minibatch 24800, batch loss 0.419949, batch nll 0.091880, batch error rate 0.000000%\n",
      "At minibatch 24900, batch loss 0.536568, batch nll 0.208465, batch error rate 4.000000%\n",
      "At minibatch 25000, batch loss 0.783949, batch nll 0.455775, batch error rate 12.000000%\n",
      "At minibatch 25100, batch loss 0.500381, batch nll 0.171861, batch error rate 4.000000%\n",
      "At minibatch 25200, batch loss 0.535014, batch nll 0.206437, batch error rate 8.000000%\n",
      "At minibatch 25300, batch loss 0.542241, batch nll 0.213434, batch error rate 8.000000%\n",
      "At minibatch 25400, batch loss 0.675510, batch nll 0.346472, batch error rate 8.000000%\n",
      "At minibatch 25500, batch loss 0.647629, batch nll 0.318568, batch error rate 8.000000%\n",
      "At minibatch 25600, batch loss 0.431363, batch nll 0.102290, batch error rate 4.000000%\n",
      "After epoch 16: valid_err_rate: 21.460000% currently going to do 25 epochs\n",
      "After epoch 16: averaged train_err_rate: 7.440000% averaged train nll: 0.245720 averaged train loss: 0.573729\n",
      "At minibatch 25700, batch loss 0.617119, batch nll 0.287834, batch error rate 8.000000%\n",
      "At minibatch 25800, batch loss 0.616710, batch nll 0.287197, batch error rate 16.000000%\n",
      "At minibatch 25900, batch loss 0.539135, batch nll 0.209614, batch error rate 8.000000%\n",
      "At minibatch 26000, batch loss 0.482819, batch nll 0.153227, batch error rate 0.000000%\n",
      "At minibatch 26100, batch loss 0.624055, batch nll 0.294235, batch error rate 12.000000%\n",
      "At minibatch 26200, batch loss 0.611390, batch nll 0.281509, batch error rate 8.000000%\n",
      "At minibatch 26300, batch loss 0.898980, batch nll 0.568940, batch error rate 4.000000%\n",
      "At minibatch 26400, batch loss 0.542091, batch nll 0.211880, batch error rate 8.000000%\n",
      "At minibatch 26500, batch loss 0.547142, batch nll 0.216833, batch error rate 8.000000%\n",
      "At minibatch 26600, batch loss 0.495242, batch nll 0.164881, batch error rate 4.000000%\n",
      "At minibatch 26700, batch loss 0.534436, batch nll 0.204058, batch error rate 4.000000%\n",
      "At minibatch 26800, batch loss 0.378125, batch nll 0.047585, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.496827, batch nll 0.166076, batch error rate 4.000000%\n",
      "At minibatch 27000, batch loss 0.492440, batch nll 0.161599, batch error rate 8.000000%\n",
      "At minibatch 27100, batch loss 0.552142, batch nll 0.221193, batch error rate 4.000000%\n",
      "At minibatch 27200, batch loss 0.736463, batch nll 0.405388, batch error rate 12.000000%\n",
      "After epoch 17: valid_err_rate: 21.230000% currently going to do 26 epochs\n",
      "After epoch 17: averaged train_err_rate: 6.760000% averaged train nll: 0.226188 averaged train loss: 0.556322\n",
      "At minibatch 27300, batch loss 0.452642, batch nll 0.121491, batch error rate 4.000000%\n",
      "At minibatch 27400, batch loss 0.525918, batch nll 0.194658, batch error rate 4.000000%\n",
      "At minibatch 27500, batch loss 0.456675, batch nll 0.125283, batch error rate 0.000000%\n",
      "At minibatch 27600, batch loss 0.534207, batch nll 0.202730, batch error rate 8.000000%\n",
      "At minibatch 27700, batch loss 0.636355, batch nll 0.304698, batch error rate 12.000000%\n",
      "At minibatch 27800, batch loss 0.536517, batch nll 0.204688, batch error rate 4.000000%\n",
      "At minibatch 27900, batch loss 0.474598, batch nll 0.142708, batch error rate 4.000000%\n",
      "At minibatch 28000, batch loss 0.699743, batch nll 0.367715, batch error rate 12.000000%\n",
      "At minibatch 28100, batch loss 0.581959, batch nll 0.249941, batch error rate 8.000000%\n",
      "At minibatch 28200, batch loss 0.495828, batch nll 0.163710, batch error rate 4.000000%\n",
      "At minibatch 28300, batch loss 0.631812, batch nll 0.299621, batch error rate 12.000000%\n",
      "At minibatch 28400, batch loss 0.378231, batch nll 0.045926, batch error rate 0.000000%\n",
      "At minibatch 28500, batch loss 0.533805, batch nll 0.201312, batch error rate 4.000000%\n",
      "At minibatch 28600, batch loss 0.513822, batch nll 0.181346, batch error rate 4.000000%\n",
      "At minibatch 28700, batch loss 0.544042, batch nll 0.211502, batch error rate 12.000000%\n",
      "At minibatch 28800, batch loss 0.538604, batch nll 0.206014, batch error rate 8.000000%\n",
      "After epoch 18: valid_err_rate: 20.970000% currently going to do 28 epochs\n",
      "After epoch 18: averaged train_err_rate: 5.857500% averaged train nll: 0.207702 averaged train loss: 0.539618\n",
      "At minibatch 28900, batch loss 0.561948, batch nll 0.229212, batch error rate 8.000000%\n",
      "At minibatch 29000, batch loss 0.386463, batch nll 0.053644, batch error rate 0.000000%\n",
      "At minibatch 29100, batch loss 0.497321, batch nll 0.164428, batch error rate 8.000000%\n",
      "At minibatch 29200, batch loss 0.483334, batch nll 0.150362, batch error rate 0.000000%\n",
      "At minibatch 29300, batch loss 0.657218, batch nll 0.324102, batch error rate 12.000000%\n",
      "At minibatch 29400, batch loss 0.438497, batch nll 0.105367, batch error rate 4.000000%\n",
      "At minibatch 29500, batch loss 0.542893, batch nll 0.209551, batch error rate 4.000000%\n",
      "At minibatch 29600, batch loss 0.458875, batch nll 0.125511, batch error rate 4.000000%\n",
      "At minibatch 29700, batch loss 0.457012, batch nll 0.123621, batch error rate 0.000000%\n",
      "At minibatch 29800, batch loss 0.536657, batch nll 0.203225, batch error rate 4.000000%\n",
      "At minibatch 29900, batch loss 0.592826, batch nll 0.259318, batch error rate 4.000000%\n",
      "At minibatch 30000, batch loss 0.448474, batch nll 0.114795, batch error rate 0.000000%\n",
      "At minibatch 30100, batch loss 0.712131, batch nll 0.378438, batch error rate 12.000000%\n",
      "At minibatch 30200, batch loss 0.392513, batch nll 0.058778, batch error rate 0.000000%\n",
      "At minibatch 30300, batch loss 0.442532, batch nll 0.108828, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.431160, batch nll 0.097320, batch error rate 4.000000%\n",
      "After epoch 19: valid_err_rate: 21.200000% currently going to do 28 epochs\n",
      "After epoch 19: averaged train_err_rate: 5.392500% averaged train nll: 0.194827 averaged train loss: 0.528135\n",
      "At minibatch 30500, batch loss 0.487983, batch nll 0.154054, batch error rate 0.000000%\n",
      "At minibatch 30600, batch loss 0.505033, batch nll 0.171017, batch error rate 8.000000%\n",
      "At minibatch 30700, batch loss 0.443369, batch nll 0.109319, batch error rate 4.000000%\n",
      "At minibatch 30800, batch loss 0.517719, batch nll 0.183561, batch error rate 4.000000%\n",
      "At minibatch 30900, batch loss 0.551798, batch nll 0.217523, batch error rate 8.000000%\n",
      "At minibatch 31000, batch loss 0.474183, batch nll 0.139835, batch error rate 4.000000%\n",
      "At minibatch 31100, batch loss 0.504889, batch nll 0.170543, batch error rate 4.000000%\n",
      "At minibatch 31200, batch loss 0.636918, batch nll 0.302504, batch error rate 8.000000%\n",
      "At minibatch 31300, batch loss 0.643122, batch nll 0.308728, batch error rate 12.000000%\n",
      "At minibatch 31400, batch loss 0.501444, batch nll 0.166945, batch error rate 4.000000%\n",
      "At minibatch 31500, batch loss 0.518314, batch nll 0.183784, batch error rate 4.000000%\n",
      "At minibatch 31600, batch loss 0.510173, batch nll 0.175597, batch error rate 4.000000%\n",
      "At minibatch 31700, batch loss 0.506258, batch nll 0.171652, batch error rate 0.000000%\n",
      "At minibatch 31800, batch loss 0.449788, batch nll 0.115157, batch error rate 4.000000%\n",
      "At minibatch 31900, batch loss 0.475080, batch nll 0.140356, batch error rate 4.000000%\n",
      "At minibatch 32000, batch loss 0.525865, batch nll 0.191052, batch error rate 0.000000%\n",
      "After epoch 20: valid_err_rate: 21.180000% currently going to do 28 epochs\n",
      "After epoch 20: averaged train_err_rate: 4.577500% averaged train nll: 0.175810 averaged train loss: 0.510168\n",
      "At minibatch 32100, batch loss 0.621910, batch nll 0.286979, batch error rate 8.000000%\n",
      "At minibatch 32200, batch loss 0.610819, batch nll 0.275832, batch error rate 4.000000%\n",
      "At minibatch 32300, batch loss 0.577101, batch nll 0.242062, batch error rate 8.000000%\n",
      "At minibatch 32400, batch loss 0.434992, batch nll 0.099930, batch error rate 0.000000%\n",
      "At minibatch 32500, batch loss 0.522305, batch nll 0.187162, batch error rate 4.000000%\n",
      "At minibatch 32600, batch loss 0.778719, batch nll 0.443548, batch error rate 4.000000%\n",
      "At minibatch 32700, batch loss 0.498586, batch nll 0.163367, batch error rate 8.000000%\n",
      "At minibatch 32800, batch loss 0.562212, batch nll 0.227086, batch error rate 8.000000%\n",
      "At minibatch 32900, batch loss 0.415589, batch nll 0.080375, batch error rate 0.000000%\n",
      "At minibatch 33000, batch loss 0.471773, batch nll 0.136561, batch error rate 0.000000%\n",
      "At minibatch 33100, batch loss 0.681470, batch nll 0.346246, batch error rate 8.000000%\n",
      "At minibatch 33200, batch loss 0.646579, batch nll 0.311395, batch error rate 16.000000%\n",
      "At minibatch 33300, batch loss 0.473414, batch nll 0.138216, batch error rate 4.000000%\n",
      "At minibatch 33400, batch loss 0.518687, batch nll 0.183413, batch error rate 4.000000%\n",
      "At minibatch 33500, batch loss 0.435246, batch nll 0.099932, batch error rate 4.000000%\n",
      "At minibatch 33600, batch loss 0.575527, batch nll 0.240130, batch error rate 8.000000%\n",
      "After epoch 21: valid_err_rate: 21.330000% currently going to do 28 epochs\n",
      "After epoch 21: averaged train_err_rate: 4.325000% averaged train nll: 0.166298 averaged train loss: 0.501450\n",
      "At minibatch 33700, batch loss 0.496805, batch nll 0.161356, batch error rate 8.000000%\n",
      "At minibatch 33800, batch loss 0.504230, batch nll 0.168783, batch error rate 4.000000%\n",
      "At minibatch 33900, batch loss 0.530582, batch nll 0.195071, batch error rate 4.000000%\n",
      "At minibatch 34000, batch loss 0.537852, batch nll 0.202296, batch error rate 8.000000%\n",
      "At minibatch 34100, batch loss 0.436581, batch nll 0.101014, batch error rate 4.000000%\n",
      "At minibatch 34200, batch loss 0.518147, batch nll 0.182622, batch error rate 4.000000%\n",
      "At minibatch 34300, batch loss 0.382914, batch nll 0.047316, batch error rate 0.000000%\n",
      "At minibatch 34400, batch loss 0.434259, batch nll 0.098562, batch error rate 0.000000%\n",
      "At minibatch 34500, batch loss 0.540181, batch nll 0.204499, batch error rate 8.000000%\n",
      "At minibatch 34600, batch loss 0.488435, batch nll 0.152775, batch error rate 4.000000%\n",
      "At minibatch 34700, batch loss 0.422615, batch nll 0.086959, batch error rate 0.000000%\n",
      "At minibatch 34800, batch loss 0.456915, batch nll 0.121245, batch error rate 4.000000%\n",
      "At minibatch 34900, batch loss 0.393906, batch nll 0.058229, batch error rate 0.000000%\n",
      "At minibatch 35000, batch loss 0.607795, batch nll 0.272092, batch error rate 4.000000%\n",
      "At minibatch 35100, batch loss 0.439769, batch nll 0.104002, batch error rate 4.000000%\n",
      "At minibatch 35200, batch loss 0.505734, batch nll 0.169931, batch error rate 4.000000%\n",
      "After epoch 22: valid_err_rate: 21.280000% currently going to do 28 epochs\n",
      "After epoch 22: averaged train_err_rate: 3.687500% averaged train nll: 0.152689 averaged train loss: 0.488294\n",
      "At minibatch 35300, batch loss 0.416311, batch nll 0.080436, batch error rate 0.000000%\n",
      "At minibatch 35400, batch loss 0.449925, batch nll 0.114069, batch error rate 4.000000%\n",
      "At minibatch 35500, batch loss 0.444305, batch nll 0.108471, batch error rate 0.000000%\n",
      "At minibatch 35600, batch loss 0.549224, batch nll 0.213384, batch error rate 12.000000%\n",
      "At minibatch 35700, batch loss 0.471297, batch nll 0.135431, batch error rate 4.000000%\n",
      "At minibatch 35800, batch loss 0.570679, batch nll 0.234840, batch error rate 8.000000%\n",
      "At minibatch 35900, batch loss 0.491993, batch nll 0.156071, batch error rate 4.000000%\n",
      "At minibatch 36000, batch loss 0.492317, batch nll 0.156388, batch error rate 8.000000%\n",
      "At minibatch 36100, batch loss 0.634553, batch nll 0.298574, batch error rate 12.000000%\n",
      "At minibatch 36200, batch loss 0.450212, batch nll 0.114275, batch error rate 0.000000%\n",
      "At minibatch 36300, batch loss 0.587319, batch nll 0.251340, batch error rate 8.000000%\n",
      "At minibatch 36400, batch loss 0.556383, batch nll 0.220352, batch error rate 8.000000%\n",
      "At minibatch 36500, batch loss 0.408023, batch nll 0.072073, batch error rate 0.000000%\n",
      "At minibatch 36600, batch loss 0.516568, batch nll 0.180672, batch error rate 8.000000%\n",
      "At minibatch 36700, batch loss 0.486714, batch nll 0.150742, batch error rate 4.000000%\n",
      "At minibatch 36800, batch loss 0.488258, batch nll 0.152251, batch error rate 4.000000%\n",
      "After epoch 23: valid_err_rate: 21.160000% currently going to do 28 epochs\n",
      "After epoch 23: averaged train_err_rate: 3.330000% averaged train nll: 0.143955 averaged train loss: 0.479868\n",
      "At minibatch 36900, batch loss 0.549181, batch nll 0.213197, batch error rate 8.000000%\n",
      "At minibatch 37000, batch loss 0.397125, batch nll 0.061201, batch error rate 0.000000%\n",
      "At minibatch 37100, batch loss 0.435642, batch nll 0.099801, batch error rate 0.000000%\n",
      "At minibatch 37200, batch loss 0.457759, batch nll 0.121937, batch error rate 0.000000%\n",
      "At minibatch 37300, batch loss 0.374693, batch nll 0.038940, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.470590, batch nll 0.134804, batch error rate 0.000000%\n",
      "At minibatch 37500, batch loss 0.445552, batch nll 0.109771, batch error rate 4.000000%\n",
      "At minibatch 37600, batch loss 0.511726, batch nll 0.175943, batch error rate 4.000000%\n",
      "At minibatch 37700, batch loss 0.432324, batch nll 0.096594, batch error rate 0.000000%\n",
      "At minibatch 37800, batch loss 0.463168, batch nll 0.127363, batch error rate 8.000000%\n",
      "At minibatch 37900, batch loss 0.447809, batch nll 0.112044, batch error rate 0.000000%\n",
      "At minibatch 38000, batch loss 0.500983, batch nll 0.165218, batch error rate 4.000000%\n",
      "At minibatch 38100, batch loss 0.469770, batch nll 0.134039, batch error rate 0.000000%\n",
      "At minibatch 38200, batch loss 0.392360, batch nll 0.056594, batch error rate 0.000000%\n",
      "At minibatch 38300, batch loss 0.421289, batch nll 0.085515, batch error rate 4.000000%\n",
      "At minibatch 38400, batch loss 0.431008, batch nll 0.095226, batch error rate 0.000000%\n",
      "After epoch 24: valid_err_rate: 21.400000% currently going to do 28 epochs\n",
      "After epoch 24: averaged train_err_rate: 3.025000% averaged train nll: 0.135560 averaged train loss: 0.471365\n",
      "At minibatch 38500, batch loss 0.546693, batch nll 0.210966, batch error rate 8.000000%\n",
      "At minibatch 38600, batch loss 0.477227, batch nll 0.141522, batch error rate 0.000000%\n",
      "At minibatch 38700, batch loss 0.431164, batch nll 0.095531, batch error rate 4.000000%\n",
      "At minibatch 38800, batch loss 0.391042, batch nll 0.055459, batch error rate 0.000000%\n",
      "At minibatch 38900, batch loss 0.478251, batch nll 0.142735, batch error rate 4.000000%\n",
      "At minibatch 39000, batch loss 0.427620, batch nll 0.092126, batch error rate 0.000000%\n",
      "At minibatch 39100, batch loss 0.467127, batch nll 0.131646, batch error rate 0.000000%\n",
      "At minibatch 39200, batch loss 0.530847, batch nll 0.195362, batch error rate 8.000000%\n",
      "At minibatch 39300, batch loss 0.394995, batch nll 0.059476, batch error rate 0.000000%\n",
      "At minibatch 39400, batch loss 0.405966, batch nll 0.070461, batch error rate 0.000000%\n",
      "At minibatch 39500, batch loss 0.431184, batch nll 0.095615, batch error rate 4.000000%\n",
      "At minibatch 39600, batch loss 0.372601, batch nll 0.037072, batch error rate 0.000000%\n",
      "At minibatch 39700, batch loss 0.499720, batch nll 0.164187, batch error rate 4.000000%\n",
      "At minibatch 39800, batch loss 0.524489, batch nll 0.188994, batch error rate 4.000000%\n",
      "At minibatch 39900, batch loss 0.503219, batch nll 0.167757, batch error rate 8.000000%\n",
      "At minibatch 40000, batch loss 0.441287, batch nll 0.105822, batch error rate 4.000000%\n",
      "After epoch 25: valid_err_rate: 20.740000% currently going to do 38 epochs\n",
      "After epoch 25: averaged train_err_rate: 2.527500% averaged train nll: 0.124594 averaged train loss: 0.460151\n",
      "At minibatch 40100, batch loss 0.444224, batch nll 0.108789, batch error rate 0.000000%\n",
      "At minibatch 40200, batch loss 0.471124, batch nll 0.135807, batch error rate 4.000000%\n",
      "At minibatch 40300, batch loss 0.423066, batch nll 0.087803, batch error rate 0.000000%\n",
      "At minibatch 40400, batch loss 0.471700, batch nll 0.136499, batch error rate 4.000000%\n",
      "At minibatch 40500, batch loss 0.616523, batch nll 0.281344, batch error rate 12.000000%\n",
      "At minibatch 40600, batch loss 0.478313, batch nll 0.143216, batch error rate 4.000000%\n",
      "At minibatch 40700, batch loss 0.457237, batch nll 0.122107, batch error rate 4.000000%\n",
      "At minibatch 40800, batch loss 0.412277, batch nll 0.077223, batch error rate 0.000000%\n",
      "At minibatch 40900, batch loss 0.391460, batch nll 0.056430, batch error rate 0.000000%\n",
      "At minibatch 41000, batch loss 0.436051, batch nll 0.101103, batch error rate 4.000000%\n",
      "At minibatch 41100, batch loss 0.425668, batch nll 0.090744, batch error rate 0.000000%\n",
      "At minibatch 41200, batch loss 0.452238, batch nll 0.117306, batch error rate 4.000000%\n",
      "At minibatch 41300, batch loss 0.456826, batch nll 0.121879, batch error rate 0.000000%\n",
      "At minibatch 41400, batch loss 0.372606, batch nll 0.037686, batch error rate 0.000000%\n",
      "At minibatch 41500, batch loss 0.430236, batch nll 0.095338, batch error rate 0.000000%\n",
      "At minibatch 41600, batch loss 0.439845, batch nll 0.104967, batch error rate 0.000000%\n",
      "After epoch 26: valid_err_rate: 21.340000% currently going to do 38 epochs\n",
      "After epoch 26: averaged train_err_rate: 2.322500% averaged train nll: 0.117463 averaged train loss: 0.452551\n",
      "At minibatch 41700, batch loss 0.447916, batch nll 0.113054, batch error rate 0.000000%\n",
      "At minibatch 41800, batch loss 0.510746, batch nll 0.176000, batch error rate 4.000000%\n",
      "At minibatch 41900, batch loss 0.439910, batch nll 0.105155, batch error rate 4.000000%\n",
      "At minibatch 42000, batch loss 0.389333, batch nll 0.054619, batch error rate 0.000000%\n",
      "At minibatch 42100, batch loss 0.442110, batch nll 0.107484, batch error rate 0.000000%\n",
      "At minibatch 42200, batch loss 0.397612, batch nll 0.063055, batch error rate 0.000000%\n",
      "At minibatch 42300, batch loss 0.564035, batch nll 0.229563, batch error rate 4.000000%\n",
      "At minibatch 42400, batch loss 0.428134, batch nll 0.093715, batch error rate 0.000000%\n",
      "At minibatch 42500, batch loss 0.404261, batch nll 0.069891, batch error rate 0.000000%\n",
      "At minibatch 42600, batch loss 0.442254, batch nll 0.107917, batch error rate 0.000000%\n",
      "At minibatch 42700, batch loss 0.467669, batch nll 0.133367, batch error rate 4.000000%\n",
      "At minibatch 42800, batch loss 0.448809, batch nll 0.114601, batch error rate 4.000000%\n",
      "At minibatch 42900, batch loss 0.393992, batch nll 0.059818, batch error rate 0.000000%\n",
      "At minibatch 43000, batch loss 0.466695, batch nll 0.132527, batch error rate 4.000000%\n",
      "At minibatch 43100, batch loss 0.443215, batch nll 0.109105, batch error rate 0.000000%\n",
      "At minibatch 43200, batch loss 0.456688, batch nll 0.122520, batch error rate 4.000000%\n",
      "After epoch 27: valid_err_rate: 20.780000% currently going to do 38 epochs\n",
      "After epoch 27: averaged train_err_rate: 2.017500% averaged train nll: 0.111144 averaged train loss: 0.445603\n",
      "At minibatch 43300, batch loss 0.433078, batch nll 0.099000, batch error rate 0.000000%\n",
      "At minibatch 43400, batch loss 0.447136, batch nll 0.113152, batch error rate 0.000000%\n",
      "At minibatch 43500, batch loss 0.431172, batch nll 0.097235, batch error rate 0.000000%\n",
      "At minibatch 43600, batch loss 0.435215, batch nll 0.101357, batch error rate 0.000000%\n",
      "At minibatch 43700, batch loss 0.432348, batch nll 0.098530, batch error rate 0.000000%\n",
      "At minibatch 43800, batch loss 0.388370, batch nll 0.054580, batch error rate 0.000000%\n",
      "At minibatch 43900, batch loss 0.362118, batch nll 0.028374, batch error rate 0.000000%\n",
      "At minibatch 44000, batch loss 0.510461, batch nll 0.176752, batch error rate 4.000000%\n",
      "At minibatch 44100, batch loss 0.433496, batch nll 0.099797, batch error rate 4.000000%\n",
      "At minibatch 44200, batch loss 0.389605, batch nll 0.055923, batch error rate 0.000000%\n",
      "At minibatch 44300, batch loss 0.401787, batch nll 0.068104, batch error rate 0.000000%\n",
      "At minibatch 44400, batch loss 0.391252, batch nll 0.057657, batch error rate 0.000000%\n",
      "At minibatch 44500, batch loss 0.550806, batch nll 0.217235, batch error rate 4.000000%\n",
      "At minibatch 44600, batch loss 0.388036, batch nll 0.054524, batch error rate 0.000000%\n",
      "At minibatch 44700, batch loss 0.423723, batch nll 0.090260, batch error rate 0.000000%\n",
      "At minibatch 44800, batch loss 0.462013, batch nll 0.128550, batch error rate 4.000000%\n",
      "After epoch 28: valid_err_rate: 20.930000% currently going to do 38 epochs\n",
      "After epoch 28: averaged train_err_rate: 1.767500% averaged train nll: 0.105256 averaged train loss: 0.439002\n",
      "At minibatch 44900, batch loss 0.465118, batch nll 0.131763, batch error rate 0.000000%\n",
      "At minibatch 45000, batch loss 0.447501, batch nll 0.114209, batch error rate 0.000000%\n",
      "At minibatch 45100, batch loss 0.414048, batch nll 0.080830, batch error rate 0.000000%\n",
      "At minibatch 45200, batch loss 0.419327, batch nll 0.086138, batch error rate 0.000000%\n",
      "At minibatch 45300, batch loss 0.396086, batch nll 0.062955, batch error rate 0.000000%\n",
      "At minibatch 45400, batch loss 0.431410, batch nll 0.098368, batch error rate 0.000000%\n",
      "At minibatch 45500, batch loss 0.372605, batch nll 0.039644, batch error rate 0.000000%\n",
      "At minibatch 45600, batch loss 0.456258, batch nll 0.123397, batch error rate 4.000000%\n",
      "At minibatch 45700, batch loss 0.472206, batch nll 0.139379, batch error rate 4.000000%\n",
      "At minibatch 45800, batch loss 0.391206, batch nll 0.058456, batch error rate 0.000000%\n",
      "At minibatch 45900, batch loss 0.442918, batch nll 0.110171, batch error rate 0.000000%\n",
      "At minibatch 46000, batch loss 0.451139, batch nll 0.118451, batch error rate 4.000000%\n",
      "At minibatch 46100, batch loss 0.673698, batch nll 0.341048, batch error rate 8.000000%\n",
      "At minibatch 46200, batch loss 0.535752, batch nll 0.203154, batch error rate 8.000000%\n",
      "At minibatch 46300, batch loss 0.484973, batch nll 0.152380, batch error rate 4.000000%\n",
      "At minibatch 46400, batch loss 0.442859, batch nll 0.110278, batch error rate 4.000000%\n",
      "After epoch 29: valid_err_rate: 21.030000% currently going to do 38 epochs\n",
      "After epoch 29: averaged train_err_rate: 1.627500% averaged train nll: 0.098839 averaged train loss: 0.431771\n",
      "At minibatch 46500, batch loss 0.394870, batch nll 0.062364, batch error rate 0.000000%\n",
      "At minibatch 46600, batch loss 0.443038, batch nll 0.110653, batch error rate 0.000000%\n",
      "At minibatch 46700, batch loss 0.402832, batch nll 0.070478, batch error rate 0.000000%\n",
      "At minibatch 46800, batch loss 0.586298, batch nll 0.254003, batch error rate 4.000000%\n",
      "At minibatch 46900, batch loss 0.578209, batch nll 0.245996, batch error rate 4.000000%\n",
      "At minibatch 47000, batch loss 0.476028, batch nll 0.143922, batch error rate 4.000000%\n",
      "At minibatch 47100, batch loss 0.457257, batch nll 0.125212, batch error rate 0.000000%\n",
      "At minibatch 47200, batch loss 0.406487, batch nll 0.074496, batch error rate 0.000000%\n",
      "At minibatch 47300, batch loss 0.391021, batch nll 0.059084, batch error rate 0.000000%\n",
      "At minibatch 47400, batch loss 0.410417, batch nll 0.078508, batch error rate 4.000000%\n",
      "At minibatch 47500, batch loss 0.447238, batch nll 0.115415, batch error rate 4.000000%\n",
      "At minibatch 47600, batch loss 0.491836, batch nll 0.160063, batch error rate 0.000000%\n",
      "At minibatch 47700, batch loss 0.438499, batch nll 0.106713, batch error rate 0.000000%\n",
      "At minibatch 47800, batch loss 0.514742, batch nll 0.183034, batch error rate 4.000000%\n",
      "At minibatch 47900, batch loss 0.443807, batch nll 0.112137, batch error rate 4.000000%\n",
      "At minibatch 48000, batch loss 0.431824, batch nll 0.100225, batch error rate 4.000000%\n",
      "After epoch 30: valid_err_rate: 21.300000% currently going to do 38 epochs\n",
      "After epoch 30: averaged train_err_rate: 1.465000% averaged train nll: 0.095173 averaged train loss: 0.427211\n",
      "At minibatch 48100, batch loss 0.408827, batch nll 0.077340, batch error rate 0.000000%\n",
      "At minibatch 48200, batch loss 0.484579, batch nll 0.153159, batch error rate 8.000000%\n",
      "At minibatch 48300, batch loss 0.431001, batch nll 0.099650, batch error rate 0.000000%\n",
      "At minibatch 48400, batch loss 0.371843, batch nll 0.040539, batch error rate 0.000000%\n",
      "At minibatch 48500, batch loss 0.412043, batch nll 0.080796, batch error rate 0.000000%\n",
      "At minibatch 48600, batch loss 0.404458, batch nll 0.073262, batch error rate 0.000000%\n",
      "At minibatch 48700, batch loss 0.418113, batch nll 0.087002, batch error rate 0.000000%\n",
      "At minibatch 48800, batch loss 0.448860, batch nll 0.117842, batch error rate 4.000000%\n",
      "At minibatch 48900, batch loss 0.394258, batch nll 0.063294, batch error rate 0.000000%\n",
      "At minibatch 49000, batch loss 0.377274, batch nll 0.046386, batch error rate 0.000000%\n",
      "At minibatch 49100, batch loss 0.418580, batch nll 0.087746, batch error rate 0.000000%\n",
      "At minibatch 49200, batch loss 0.432934, batch nll 0.102162, batch error rate 0.000000%\n",
      "At minibatch 49300, batch loss 0.636082, batch nll 0.305343, batch error rate 8.000000%\n",
      "At minibatch 49400, batch loss 0.444318, batch nll 0.113622, batch error rate 0.000000%\n",
      "At minibatch 49500, batch loss 0.407019, batch nll 0.076369, batch error rate 0.000000%\n",
      "At minibatch 49600, batch loss 0.394920, batch nll 0.064339, batch error rate 0.000000%\n",
      "After epoch 31: valid_err_rate: 20.910000% currently going to do 38 epochs\n",
      "After epoch 31: averaged train_err_rate: 1.285000% averaged train nll: 0.089977 averaged train loss: 0.421023\n",
      "At minibatch 49700, batch loss 0.454993, batch nll 0.124500, batch error rate 0.000000%\n",
      "At minibatch 49800, batch loss 0.369800, batch nll 0.039326, batch error rate 0.000000%\n",
      "At minibatch 49900, batch loss 0.535821, batch nll 0.205426, batch error rate 4.000000%\n",
      "At minibatch 50000, batch loss 0.364229, batch nll 0.033936, batch error rate 0.000000%\n",
      "At minibatch 50100, batch loss 0.467140, batch nll 0.136896, batch error rate 4.000000%\n",
      "At minibatch 50200, batch loss 0.341863, batch nll 0.011672, batch error rate 0.000000%\n",
      "At minibatch 50300, batch loss 0.435344, batch nll 0.105230, batch error rate 0.000000%\n",
      "At minibatch 50400, batch loss 0.424517, batch nll 0.094437, batch error rate 0.000000%\n",
      "At minibatch 50500, batch loss 0.421931, batch nll 0.091929, batch error rate 0.000000%\n",
      "At minibatch 50600, batch loss 0.390727, batch nll 0.060771, batch error rate 0.000000%\n",
      "At minibatch 50700, batch loss 0.351541, batch nll 0.021732, batch error rate 0.000000%\n",
      "At minibatch 50800, batch loss 0.461118, batch nll 0.131360, batch error rate 4.000000%\n",
      "At minibatch 50900, batch loss 0.393756, batch nll 0.064024, batch error rate 0.000000%\n",
      "At minibatch 51000, batch loss 0.473302, batch nll 0.143645, batch error rate 0.000000%\n",
      "At minibatch 51100, batch loss 0.507963, batch nll 0.178298, batch error rate 0.000000%\n",
      "At minibatch 51200, batch loss 0.370246, batch nll 0.040603, batch error rate 0.000000%\n",
      "After epoch 32: valid_err_rate: 21.160000% currently going to do 38 epochs\n",
      "After epoch 32: averaged train_err_rate: 1.132500% averaged train nll: 0.088734 averaged train loss: 0.418795\n",
      "At minibatch 51300, batch loss 0.422394, batch nll 0.092843, batch error rate 4.000000%\n",
      "At minibatch 51400, batch loss 0.423041, batch nll 0.093543, batch error rate 0.000000%\n",
      "At minibatch 51500, batch loss 0.351693, batch nll 0.022262, batch error rate 0.000000%\n",
      "At minibatch 51600, batch loss 0.379794, batch nll 0.050473, batch error rate 0.000000%\n",
      "At minibatch 51700, batch loss 0.456868, batch nll 0.127613, batch error rate 4.000000%\n",
      "At minibatch 51800, batch loss 0.440141, batch nll 0.111014, batch error rate 8.000000%\n",
      "At minibatch 51900, batch loss 0.447901, batch nll 0.118850, batch error rate 0.000000%\n",
      "At minibatch 52000, batch loss 0.415011, batch nll 0.086050, batch error rate 0.000000%\n",
      "At minibatch 52100, batch loss 0.384696, batch nll 0.055795, batch error rate 0.000000%\n",
      "At minibatch 52200, batch loss 0.435494, batch nll 0.106661, batch error rate 4.000000%\n",
      "At minibatch 52300, batch loss 0.468957, batch nll 0.140213, batch error rate 4.000000%\n",
      "At minibatch 52400, batch loss 0.437078, batch nll 0.108358, batch error rate 0.000000%\n",
      "At minibatch 52500, batch loss 0.424668, batch nll 0.096034, batch error rate 0.000000%\n",
      "At minibatch 52600, batch loss 0.431353, batch nll 0.102771, batch error rate 4.000000%\n",
      "At minibatch 52700, batch loss 0.430951, batch nll 0.102415, batch error rate 0.000000%\n",
      "At minibatch 52800, batch loss 0.412761, batch nll 0.084239, batch error rate 0.000000%\n",
      "After epoch 33: valid_err_rate: 20.810000% currently going to do 38 epochs\n",
      "After epoch 33: averaged train_err_rate: 1.000000% averaged train nll: 0.083284 averaged train loss: 0.412296\n",
      "At minibatch 52900, batch loss 0.398715, batch nll 0.070279, batch error rate 0.000000%\n",
      "At minibatch 53000, batch loss 0.404215, batch nll 0.075884, batch error rate 0.000000%\n",
      "At minibatch 53100, batch loss 0.370228, batch nll 0.041983, batch error rate 0.000000%\n",
      "At minibatch 53200, batch loss 0.403716, batch nll 0.075546, batch error rate 4.000000%\n",
      "At minibatch 53300, batch loss 0.464034, batch nll 0.135938, batch error rate 4.000000%\n",
      "At minibatch 53400, batch loss 0.433139, batch nll 0.105132, batch error rate 4.000000%\n",
      "At minibatch 53500, batch loss 0.381265, batch nll 0.053359, batch error rate 0.000000%\n",
      "At minibatch 53600, batch loss 0.352117, batch nll 0.024245, batch error rate 0.000000%\n",
      "At minibatch 53700, batch loss 0.424417, batch nll 0.096588, batch error rate 0.000000%\n",
      "At minibatch 53800, batch loss 0.365269, batch nll 0.037495, batch error rate 0.000000%\n",
      "At minibatch 53900, batch loss 0.460866, batch nll 0.133151, batch error rate 8.000000%\n",
      "At minibatch 54000, batch loss 0.383593, batch nll 0.055946, batch error rate 0.000000%\n",
      "At minibatch 54100, batch loss 0.388656, batch nll 0.061076, batch error rate 0.000000%\n",
      "At minibatch 54200, batch loss 0.460281, batch nll 0.132723, batch error rate 4.000000%\n",
      "At minibatch 54300, batch loss 0.405787, batch nll 0.078283, batch error rate 0.000000%\n",
      "At minibatch 54400, batch loss 0.481158, batch nll 0.153699, batch error rate 4.000000%\n",
      "After epoch 34: valid_err_rate: 20.770000% currently going to do 38 epochs\n",
      "After epoch 34: averaged train_err_rate: 0.892500% averaged train nll: 0.080215 averaged train loss: 0.408132\n",
      "At minibatch 54500, batch loss 0.400188, batch nll 0.072846, batch error rate 0.000000%\n",
      "At minibatch 54600, batch loss 0.373068, batch nll 0.045773, batch error rate 0.000000%\n",
      "At minibatch 54700, batch loss 0.433238, batch nll 0.106034, batch error rate 0.000000%\n",
      "At minibatch 54800, batch loss 0.382314, batch nll 0.055202, batch error rate 0.000000%\n",
      "At minibatch 54900, batch loss 0.433120, batch nll 0.106067, batch error rate 0.000000%\n",
      "At minibatch 55000, batch loss 0.406575, batch nll 0.079627, batch error rate 0.000000%\n",
      "At minibatch 55100, batch loss 0.398703, batch nll 0.071815, batch error rate 0.000000%\n",
      "At minibatch 55200, batch loss 0.381543, batch nll 0.054726, batch error rate 0.000000%\n",
      "At minibatch 55300, batch loss 0.423126, batch nll 0.096364, batch error rate 0.000000%\n",
      "At minibatch 55400, batch loss 0.369520, batch nll 0.042853, batch error rate 0.000000%\n",
      "At minibatch 55500, batch loss 0.401619, batch nll 0.075024, batch error rate 0.000000%\n",
      "At minibatch 55600, batch loss 0.453087, batch nll 0.126529, batch error rate 4.000000%\n",
      "At minibatch 55700, batch loss 0.419150, batch nll 0.092665, batch error rate 0.000000%\n",
      "At minibatch 55800, batch loss 0.528724, batch nll 0.202312, batch error rate 12.000000%\n",
      "At minibatch 55900, batch loss 0.395299, batch nll 0.068907, batch error rate 0.000000%\n",
      "At minibatch 56000, batch loss 0.398018, batch nll 0.071716, batch error rate 0.000000%\n",
      "After epoch 35: valid_err_rate: 21.070000% currently going to do 38 epochs\n",
      "After epoch 35: averaged train_err_rate: 0.772500% averaged train nll: 0.077343 averaged train loss: 0.404180\n",
      "At minibatch 56100, batch loss 0.393821, batch nll 0.067582, batch error rate 0.000000%\n",
      "At minibatch 56200, batch loss 0.398389, batch nll 0.072253, batch error rate 0.000000%\n",
      "At minibatch 56300, batch loss 0.387195, batch nll 0.061162, batch error rate 0.000000%\n",
      "At minibatch 56400, batch loss 0.388977, batch nll 0.063047, batch error rate 0.000000%\n",
      "At minibatch 56500, batch loss 0.455836, batch nll 0.130001, batch error rate 4.000000%\n",
      "At minibatch 56600, batch loss 0.381366, batch nll 0.055596, batch error rate 0.000000%\n",
      "At minibatch 56700, batch loss 0.396621, batch nll 0.070909, batch error rate 0.000000%\n",
      "At minibatch 56800, batch loss 0.363842, batch nll 0.038194, batch error rate 0.000000%\n",
      "At minibatch 56900, batch loss 0.371635, batch nll 0.046011, batch error rate 0.000000%\n",
      "At minibatch 57000, batch loss 0.414455, batch nll 0.088896, batch error rate 0.000000%\n",
      "At minibatch 57100, batch loss 0.411764, batch nll 0.086269, batch error rate 0.000000%\n",
      "At minibatch 57200, batch loss 0.423713, batch nll 0.098268, batch error rate 0.000000%\n",
      "At minibatch 57300, batch loss 0.385509, batch nll 0.060148, batch error rate 0.000000%\n",
      "At minibatch 57400, batch loss 0.398919, batch nll 0.073585, batch error rate 0.000000%\n",
      "At minibatch 57500, batch loss 0.351266, batch nll 0.025985, batch error rate 0.000000%\n",
      "At minibatch 57600, batch loss 0.412040, batch nll 0.086843, batch error rate 4.000000%\n",
      "After epoch 36: valid_err_rate: 21.180000% currently going to do 38 epochs\n",
      "After epoch 36: averaged train_err_rate: 0.735000% averaged train nll: 0.074882 averaged train loss: 0.400579\n",
      "At minibatch 57700, batch loss 0.429662, batch nll 0.104527, batch error rate 0.000000%\n",
      "At minibatch 57800, batch loss 0.422412, batch nll 0.097373, batch error rate 0.000000%\n",
      "At minibatch 57900, batch loss 0.449790, batch nll 0.124847, batch error rate 0.000000%\n",
      "At minibatch 58000, batch loss 0.379903, batch nll 0.055085, batch error rate 0.000000%\n",
      "At minibatch 58100, batch loss 0.418199, batch nll 0.093468, batch error rate 4.000000%\n",
      "At minibatch 58200, batch loss 0.382449, batch nll 0.057767, batch error rate 0.000000%\n",
      "At minibatch 58300, batch loss 0.405357, batch nll 0.080790, batch error rate 0.000000%\n",
      "At minibatch 58400, batch loss 0.380685, batch nll 0.056174, batch error rate 0.000000%\n",
      "At minibatch 58500, batch loss 0.402479, batch nll 0.078010, batch error rate 0.000000%\n",
      "At minibatch 58600, batch loss 0.369722, batch nll 0.045328, batch error rate 0.000000%\n",
      "At minibatch 58700, batch loss 0.386758, batch nll 0.062419, batch error rate 0.000000%\n",
      "At minibatch 58800, batch loss 0.372347, batch nll 0.048049, batch error rate 0.000000%\n",
      "At minibatch 58900, batch loss 0.428061, batch nll 0.103807, batch error rate 0.000000%\n",
      "At minibatch 59000, batch loss 0.383016, batch nll 0.058813, batch error rate 0.000000%\n",
      "At minibatch 59100, batch loss 0.391171, batch nll 0.067065, batch error rate 0.000000%\n",
      "At minibatch 59200, batch loss 0.427758, batch nll 0.103688, batch error rate 0.000000%\n",
      "After epoch 37: valid_err_rate: 20.790000% currently going to do 38 epochs\n",
      "After epoch 37: averaged train_err_rate: 0.667500% averaged train nll: 0.073057 averaged train loss: 0.397625\n",
      "At minibatch 59300, batch loss 0.367268, batch nll 0.043288, batch error rate 0.000000%\n",
      "At minibatch 59400, batch loss 0.396171, batch nll 0.072271, batch error rate 0.000000%\n",
      "At minibatch 59500, batch loss 0.405205, batch nll 0.081415, batch error rate 0.000000%\n",
      "At minibatch 59600, batch loss 0.488988, batch nll 0.165278, batch error rate 4.000000%\n",
      "At minibatch 59700, batch loss 0.382123, batch nll 0.058499, batch error rate 0.000000%\n",
      "At minibatch 59800, batch loss 0.370837, batch nll 0.047318, batch error rate 0.000000%\n",
      "At minibatch 59900, batch loss 0.392352, batch nll 0.068917, batch error rate 0.000000%\n",
      "At minibatch 60000, batch loss 0.390705, batch nll 0.067357, batch error rate 0.000000%\n",
      "At minibatch 60100, batch loss 0.420534, batch nll 0.097239, batch error rate 0.000000%\n",
      "At minibatch 60200, batch loss 0.417212, batch nll 0.093992, batch error rate 0.000000%\n",
      "At minibatch 60300, batch loss 0.357476, batch nll 0.034312, batch error rate 0.000000%\n",
      "At minibatch 60400, batch loss 0.380072, batch nll 0.056997, batch error rate 0.000000%\n",
      "At minibatch 60500, batch loss 0.414276, batch nll 0.091262, batch error rate 0.000000%\n",
      "At minibatch 60600, batch loss 0.447843, batch nll 0.124874, batch error rate 8.000000%\n",
      "At minibatch 60700, batch loss 0.374464, batch nll 0.051549, batch error rate 0.000000%\n",
      "At minibatch 60800, batch loss 0.380161, batch nll 0.057248, batch error rate 0.000000%\n",
      "After epoch 38: valid_err_rate: 20.850000% currently going to do 38 epochs\n",
      "After epoch 38: averaged train_err_rate: 0.620000% averaged train nll: 0.071545 averaged train loss: 0.394950\n",
      "At minibatch 60900, batch loss 0.374548, batch nll 0.051711, batch error rate 0.000000%\n",
      "At minibatch 61000, batch loss 0.400169, batch nll 0.077428, batch error rate 0.000000%\n",
      "At minibatch 61100, batch loss 0.469379, batch nll 0.146737, batch error rate 4.000000%\n",
      "At minibatch 61200, batch loss 0.378425, batch nll 0.055897, batch error rate 0.000000%\n",
      "At minibatch 61300, batch loss 0.452546, batch nll 0.130096, batch error rate 4.000000%\n",
      "At minibatch 61400, batch loss 0.376808, batch nll 0.054409, batch error rate 0.000000%\n",
      "At minibatch 61500, batch loss 0.399923, batch nll 0.077583, batch error rate 0.000000%\n",
      "At minibatch 61600, batch loss 0.389867, batch nll 0.067594, batch error rate 0.000000%\n",
      "At minibatch 61700, batch loss 0.392632, batch nll 0.070432, batch error rate 0.000000%\n",
      "At minibatch 61800, batch loss 0.385468, batch nll 0.063301, batch error rate 0.000000%\n",
      "At minibatch 61900, batch loss 0.424226, batch nll 0.102139, batch error rate 0.000000%\n",
      "At minibatch 62000, batch loss 0.382527, batch nll 0.060494, batch error rate 0.000000%\n",
      "At minibatch 62100, batch loss 0.367000, batch nll 0.045019, batch error rate 0.000000%\n",
      "At minibatch 62200, batch loss 0.393900, batch nll 0.071975, batch error rate 0.000000%\n",
      "At minibatch 62300, batch loss 0.353361, batch nll 0.031476, batch error rate 0.000000%\n",
      "At minibatch 62400, batch loss 0.392806, batch nll 0.070974, batch error rate 0.000000%\n",
      "After epoch 39: valid_err_rate: 20.980000% currently going to do 38 epochs\n",
      "After epoch 39: averaged train_err_rate: 0.522500% averaged train nll: 0.069441 averaged train loss: 0.391745\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while e < number_of_epochs: # This loop goes over epochs\n",
    "    e += 1\n",
    "    \n",
    "    # First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar10_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = params[\"K\"]\n",
    "        lrate = params[\"lrate_const\"] * K / np.maximum(K, i)\n",
    "        momentum = params[\"momentum\"]\n",
    "        \n",
    "        # With probability of 1/2 decide whether to flip vertically every image from a batch.\n",
    "        if np.random.randint(2) == 1:\n",
    "            X_batch = X_batch[:, :, :, : : -1]\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        train_loss.append((i, L))\n",
    "        train_erros.append((i, err_rate))\n",
    "        train_nll.append((i, nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate * 100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar10_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion + 1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i, val_error_rate))\n",
    "    \n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" \\\n",
    "          % (e,val_error_rate * 100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" \\\n",
    "          % (e,\n",
    "             np.mean(np.asarray(train_erros)[epoch_start_i :, 1]) * 100, \n",
    "             np.mean(np.asarray(train_nll)[epoch_start_i :, 1]),\n",
    "             np.mean(np.asarray(train_loss)[epoch_start_i :, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 25\n",
      "Test error rate is 22.080000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f96f41443d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXl4lNW5wH8vm2wBEgKEPSBgARXUFkFAYxVFCqLXBXHD\npW4VRatWEduEeq3LrXWrVVQEtS14tdbrAohSYrEugIoKgojsYRFlC1uA5L1/nNkzk8wkM5mZ5P09\nz3m+s5/3++ab835nF1XFMAzDMADqJVsAwzAMI3UwpWAYhmH4MKVgGIZh+DClYBiGYfgwpWAYhmH4\nMKVgGIZh+DClYBiGYfgwpWAYhmH4SKhSEJFuIvKciLySyHIMwzCM+JBQpaCqa1T1l4kswzAMw4gf\nMSsFEXleRLaKyFch/sNFZIWIfCsid8ZPRMMwDKOmqEpLYRowPNBDROoDf/b49wHGikjv6otnGIZh\n1CQxKwVVXQDsCPEeAKxS1bWqegiYCYwWkSwReRrob60HwzCM1KdBnPLpCGwIcG8ETlTV7cD1cSrD\nMAzDSDDxUgpV3n9bRGzvbsMwjCqgqhLvPOM1+6gI6Bzg7oxrLURFfn4+8+fPR1XTzuTn5yddBpM/\n+XKY/Oln0lX2+fPnk5+fH6equzzxUgqLgZ4ikisijYAxwBtxytswDMOoIaoyJXUG8CHQS0Q2iMiV\nqnoYGA+8A3wNvKyqy6PNs6CggLy8vFhFMQzDqHPk5eVRUFCQsPxjHlNQ1bER/GcDs6sihFcppKNi\nSEeZAzH5k4vJnzzSVfbCwkIKCwsTlr/tfVRN0vXF8mLyJxeTP3mks+yJRFSTO/lHRDTZMhhGOiAS\n94kmRpoQro4UETQBs4/iNSW1WqRz95Fh1CT2AVX3CP0YSHT3kbUUDCNN8HwZJlsMo4aJ9LsnqqVg\nYwqGYRiGj5RQCgUFBQltDhmGkVhyc3OZN29ewsspKCjgsssuS3g5gYwYMYKXXnop7vkWFhbSubN/\nzW+0z7CwsDChU1JTRinYeIJhpC8iUuWB8Ly8PKZOnRp1ObFQr149Vq9eXRWxfMyaNatGFFG0zzDR\n6xRSQikYhlF3iaWir8qYSkVpDh8+HHN+tZ2UUArWfWQY6c/ChQvp27cvWVlZXHXVVZSUlACwc+dO\nRo4cSdu2bcnKymLUqFEUFRUBMGnSJBYsWMD48ePJyMjg5ptvBmDZsmUMGzaM1q1bk5OTw/333w84\nBXLw4EHGjRtHixYtOProo/n000/DynPyyScD0K9fPzIyMnjllVcoLCykU6dOPPTQQ7Rv356rr766\nQvkguCUzffp0hgwZwh133EFWVhbdu3dnzpw5EZ9Jbm4uDz/8MP369aNVq1ZcdNFFvudSVRLdfZT0\nzZ0A3bFDDcOoBPd3TU26du2qxxxzjG7cuFG3b9+ugwcP1nvuuUdVVX/88Ud97bXXdP/+/VpcXKwX\nXHCBnnPOOb60eXl5OnXqVJ979+7dmpOTo3/605+0pKREi4uL9ZNPPlFV1fz8fG3cuLHOnj1by8rK\ndOLEiTpw4MCIcomIfvfddz73/PnztUGDBnrXXXfpwYMHdf/+/THJN23aNG3YsKE+99xzWlZWpk89\n9ZR26NAhYvm5ubl64okn6ubNm3X79u3au3dvffrpp32ydOrUKSjuvHnzyuUR6Xf3+Me9Tk6JlkJm\nJvzjH+HDbAaeYaQ+IsL48ePp2LEjmZmZTJo0iRkzZgCQlZXFueeeS+PGjWnevDl3330377//flB6\nDfijv/XWW3To0IFbb72VRo0a0bx5cwYMGOALHzp0KMOHD0dEuPTSS/niiy9ikrVevXpMnjyZhg0b\n0rhx46jkC6Rr165cffXViAiXX345mzdv5vvvv48Y/+abbyYnJ4fMzExGjRrFkiVLYpK3pkkJpQBw\n/vnQtSv88pfwn/+ACLz7LtSrB8uWuTiHDkEFz94w6jQi8TFVJXAmTZcuXdi0aRMA+/bt47rrriM3\nN5eWLVtyyimnsGvXriBFEDiusGHDBrp37x6xnHbt2vnsTZs25cCBA5SVlUUtZ5s2bWjUqJHPHY18\ngeTk5ASVD7Bnz56I5QXGb9KkSYVxU4EUUQoFQCHr18PUqTBkiPM94wx3ffxx+Owz6NUL2rVzL+7b\nb8OBA8G5rFxZkzIbRmqhGh9TVdavXx9k79ixIwAPP/wwK1euZOHChezatYv3338/sPu43EBzly5d\nIs4YisdWH6F5VCZfqlEnpqQ6pZAXMfSZZ+CEE2DtWr/fyJHQpAn06AEHDzq/o46CV1+F0lLn9+ij\nsHs3XH21C9+3D15/3Z/Hd98F/wn27oWf/SxOt2QYdQhV5cknn6SoqIjt27dz3333MWbMGMB9RTdp\n0oSWLVuyfft2Jk+eHJS2Xbt2fPfddz73yJEj2bx5M4899hglJSUUFxezcOFCXzmxEJp3OCqTL9VI\n6ympItJMRF4QkWdE5OJElPHdd3DEETBlinNfcAE0aADNm8Ott0LLlvD88y7sb3+Dc8/1p+3RAwIn\nPW3aBIsXJ0JKw6jdiAiXXHIJZ5xxBkceeSQ9e/bknnvuAeCWW25h//79ZGdnc9JJJ3HWWWcFfa1P\nmDCBV199laysLG655RaaN2/Ou+++y5tvvkn79u3p1auXb3ZiuLn8FbUeCgoKGDduHJmZmbz66qth\n01cmX2hZsZRfWfpU3OQwoXsfichlwHZVfVtEZqrqRWHiaDWOeI4aVXj2Wbj2WvjqK8jOhvbtXcth\n9GgX5+GH4fbbbXDbSE1s76O6ScrvfSQiz4vIVhH5KsR/uIisEJFvReROj3dHYIPHXlpNWauFiFMI\nAMcc4xQCwK5drqvpo4+cQgDXzRTICy+4VsbhwxBpUsK117pwwzCMdCbmloKIDAX2AC+q6jEev/rA\nN8DpQBGwCBgLnADs8LQUZmiYU9tqqqUQK9dcA3fcAT17+mdkXHQRzJzpxi2OOMKNa3gRcTOj2rSB\nDz904xsArVvXvOxG7cRaCnWTlG8pqOoCYEeI9wBglaquVdVDwExgNPAacJ6I/AV4I1KeH30UqxSJ\n59ln3WynwC6/mTPd9fzzYdQoKC527qefdte2bV38wYNd91R2tvP3/p79+rnwH3904x3RDmqffTas\nWOFmW/3qV37/zz6ruKuruBgyMqIrwzAMA+I30BzYTQSwEeioqvtU9SpV/ZWqzoiUeODAOElRw7Ro\n4VoUN9wQOc7WrW6txaFD8OWXzi87G/761/CD2n/9a/AsK4A333TjHd99B0895fc/4QT4+uvIZX//\nPVRnSvSsWW5GlmEYdYd4nbxWrTZtQUEBw4a5xWpuampeHESqGZ57ruJw77qVgLUyAPzwg7v+85/w\nX//l7L17w/LlcNZZMHs23Hefa114y7nxxvL5Hz7szOLFLn3LllW7j/Xr3bhI4DYuv/gFPPEEjB8f\nPs3YsbBlC8yfX7UyDcOInkSfuOYlXkqhCOgc4O6May1Ezd135/H223nlKs/ajlchgFMI4BQCwKRJ\nwXG9K/1vvNEt6APo3z84zpw5cOaZzu5dzHfgADRu7AbUDx8GzyJMH3PnuhbIO++El/Hf/3Zpc3Kc\n0vEuXJ01y60DqQ5e2QzDqBjvkcWJVg7x6j5aDPQUkVwRaQSMoYIxhEg0bOhfVVnZF3hd5NAhd/3L\nX9xajHAMH+6uN90EI0Y4e5MmrjI/4gho1iw4/r59TokELuoL5MABOOUUGDbMzdrq0iV8vO+/h82b\n/W5Vv5ILzc/Lv//tZIvEt99GDjMMI0HEuoMeMAPYBJTgxhGu9PifhZuBtAqYGEN+YXcALC2N16L9\numdefbXi8NtuUy0oUP3qq/JhJSXu+T/0UOT0Xlq08LsDw1atUn3xReceNy50Z0fVc89V/eIL1b//\nPTi/QBo1ihwWyKmnqi5cWHm82gCum9ZMHTSR3geNsf6OxsQ9w5gFAM3Pz9f58+eXu+k1a1TXrk1+\nJWsm2HjJyHDuf/zDH/bRR+HT/OY33hfZmRNPVD3tNGfv3j3cCx9cViRA9fe/rzxeNJSUqD75ZHzy\nMoxEMX/+fM3Pz6/dSqEy3nhD9eqrVZcvV508WfWSS/xfmWZq3tx0k+pjj8We7uWXI4dNmaJ6883+\n3zxUKUyY4NwXXeQ+FgLjdewY/L4sWaK6f3/4d6m0VPX55/3uw4f99t//PrhMw0hlarVSiNRSqIiy\nsuRXjmZiMxdcUHmcbdtcl1Og36xZwe477wz8Y6ivIv/hB7/7D39wFf7y5cHvzfr1/vhz5jj7pk3l\n81JVfeYZp0SqS3Fx9fMwDC/WUqiAZFdyZpJr/vUvv33HDtV58/zuSZNUe/Vy9kOH/O+MdxzlzTf9\ncVesCH6ftm1zigfc+EtxsbP/8Y/B719pqep336lu2eLKqug9/eorZ9+3T/Xjj6v8yieMkhLVt99W\n9RyWZqQBtVopVKWloOr+7Js3J79yMpPa5ssv3fvyww+qDRuWD//4Y1exe92PPx4cvmSJu/70p8Hv\n37Rpzr+w0F3373dXb8vjX//y/nldHFXVBx5w7uJi9+7u2hXdu96/v1NA0fDEExrzEbfPPee/XyO1\nsZZCFIwfn/yKx0xqm5tuqn4eP/uZu86Z41oT3bs7d9++7rppk7v+9reqW7c6u3r+Zd6jd71jI4Hm\n++9V77rLfeRs3aq6dKlTGKFjJzNnhn//v/3WdZcFxgX/TDJV191aVhb5P/T00/508cA7+cBIHLVa\nKVS1pRD8gPzmxhuTXwmZqX1mwICKw4uK/PaTTw4O+8MfXPdMuHQjR7prfn75sOJi1b17nd2rFPbu\nDR6Uv/12F75vX3DL+YMPVD//3PmPHu1mfEUiWqWwZUvFysXLf/935XkZVcNaClECblbJ8uX+P4kZ\nM+lgfvrTyGG5uf4WycyZqgcPql56qXMPG6a6YYPqr3/t3L/8ZXDaDz5w18mT/WtKsrNV33+//P8n\nWqUAbuDfy969qvXqlY93332V5+Vl+XLX2oo3hw+7Z1NbSZRSSJHjOONDjx7wk59Afr7bQdQw0oGK\nTvtbuxa8xxWrulME//pX5373Xbed+5/+5Nxr1gSnXbrUXXfs8G9H8sMPboW622fMz/XXVy7ntm3u\nGrjSfNcuKCvzu0WgqCh4d+HK+OgjWLYs+vjRsnOn/9kYMZAITROLgfh1Hy1YEOx38KB/ZbR33EFV\ntXHj5H8dmjETq4l3C/jYY1V371bdvj3Y38uKFa4lolp+jYl38P6EE4LTgOrdd6vef3+w/6JF4f+3\n//xn8Mr4yli0SPWUU1zLpjK8U5QjAf5ZYYHcd59/ksCsWdF1l9Uk1n0UJW+8EbwQKRBw4aEvrxkz\nZtwgd6C7aVPVAwec/aijVP/zn/DpAqf1Bv6v7rhD9ZprnH3kyOAZX6WlwWs/Tj01OI9mzVzZqm4K\n8IoV7n/t3coksHwvy5e7ivsvf/GvCQH/9OPFi901dEEjqL72murcuU65eeUC1dNP99tXr46+HqpJ\nTClUA/AP8nlZt869fFu3upduxw738nz0kRuUS/Yf1YyZZBrv13u05t//9tuPP77y+F5OP93v511Y\nuG2b/38LqiLuevhw+TyWLnX299931/x8f9q5c4Pj79zpZmStXu1meoFTCt5wr9IAJ5dXnrqmFGrV\nmEJFZGa608u8dOnidg1t2xbq14dWrdyhNQMHhj8/4JJL/PY2bRIvr2Ekk1i3RD/5ZL89mvE8EbdD\n7r//7ffz7sB7+LAbq/Ci6q7hdgbetMldS0rctbSCk+BbtXL/+e7d4Z57/HIEluMdH1H1yxMoY12g\nTiiFTz5xlb333OTK8L6EgUye7K5TpsAHH8RPNsOoqxw44M7pCKV9e1eBV8bBg3DGGc7uvQby6KOR\n0z74oLuee67f77HHwv/3r7jCP8heFxAN9xRqUgARzc/P9x0gkQrMnevOGGjfHh55xB27edZZ/vDt\n26F16+TJZxgGNG8e/rjZnBx3ImBVOHw48lkla9ZAbm7V8o0n3kN2Jk+ejKrGMM8rOlJCKSRbhlDm\nzHFKoCKxROBvfwvuVqqMmTPdFELDMNKPzz6D445LthR+RCQhSiGh3Uci0k1EnhORVxJZTrzp0KHy\nOIcOwcUXu77Jr78uH37VVVUr+8svq5bOMIzEkmLfrgkjoUpBVdeo6i8TWUYiOPbYyl8AbxPz3nuh\nd+/yzcrBg2HGjGC/aL4ymjePWkzDMIy4E5VSEJHnRWSriHwV4j9cRFaIyLcicmdiREwPvvkG+vVz\nsxu+/RbGjfMrlltucWMU9TxPe+9eOOec5MlqGEbsBJ4vXpuJakxBRIYCe4AXVfUYj1993JnMpwNF\nwCJgLPBT4Hjgf1R1kyfuK6p6QYS8U25Moars3Om6lbxTVouL4aWX4Fe/cu5vv4VevfzK4vzz3Uym\nrVv9eaxeDWPGwKJFNSu7YRgV07Zt8H812SR1TEFVFwA7QrwHAKtUda2qHgJmAqNV9SVVvVVVN4lI\nlog8DfSvCy2JVq2C1zBkZPgVgjc8kFdfdTOdILi7ql6EX6Wq4xSGYVSf779PtgQ1Q4TJV1HREdgQ\n4N4InBgYQVW3A5VutVVQUOCzp9LU1HjTpk35sYpjj3WbnoGbD92hg1tMF45TTnEboqnCP/7hWho9\ne8KqVXVnEMww6ireqaiJpjoDzXGthvLy8igoKKi1CqEiunZ112nT3JjEo4/Ciy86vyuucNeBA+HS\nS50CADjvPKcIPvgA1q/35xVugd4//xm+3Msvj4v4hmHUADVVR0a9TkFEcoE3A8YUBgIFqjrc454I\nlKnqgzEJUIvGFOLNOefAdde5bYUnTICGDSPH9S7X/8lP3HYeI0a4RXeffupaFcceWz7Nhx/CSSfF\nR9YRI2DWrPjkZRipSipVVYkaU6hO99FioKdHWWwCxuAGmmPGq/3qYiuhIl5/3V0DV1NHYulSOPpo\nt58MwNtvu31cVIP3fGrWzM1+ijcTJphSMIyaINHdSNFOSZ0BfAj0EpENInKlqh4GxgPvAF8DL6vq\n8oRJalRI377u2qyZ369ePTc+0acPzJ/v/IYNC06XmRnsPvLI8Pnn5IT3z8pyrZSf/zx2mQ3DSD1s\nm4taxO9+5yrnQYPc2EQoIm7Hx6FDnf3DD+EXv3Anc3k5cAAaN4amTWHfPr//I4/ArbcG53fTTfD4\n48H5R0NREXTsGP19GUaqkEpVVVpucxEtBQUFNTKqXtv5/e8hLy+8QvAS2jLo39+txi4rc8abtl07\nf5yTTnIL8ALZsgX+538il1PRhoEdOgTnbxhG9BQWFgbN2Iw7iTikIRZD4IkbRkJp3dodJqTqDg/5\n8EN30NC+fcHxQPXII/2Hj5x0kt8/9JCU0HReM3asu44Zo7pqlepPfuLcjz4aPr4ZM+lgUglP3Um8\njbUU6hA//OBfQJeRAd26uZaBd3A6kEhdQWPGxFbmzTe7cYrLLnPuCRMqjv/EE+H9H3ootnINo7Zi\nLQWjxgHVXr38X0eDBjn/BQsqPsQcVEeNcteLL3bXjz5yYXv2uCMTQ+N7zcSJ/i+xQP85c9wZvF7/\nLl2Cw7dsSdxXYc+eyf8yNZNaJpXw1J3E21hLwShHy5b+2Uz33+/OgQAYMqTiweT/+R93xkQg7du7\na7NmwUc2htK4cXj/M890ay+8RDuYXRHPPw833uj+5uFO7PLys5/BU0+FD7PXte4xenSyJXBYS8Go\ncfbudQecf/VVxS2DSIBrKVRGv36qV17p7Pn5/i8xcIe1ew9S9/Lee6qffRb85eY9gB1UmzRx1z59\nyn/h7d7txlBA9dNP/Xnefnv4r8EmTVQff9wvT6gpKUn+V6uZmjVvvRX7fyGReOpO4m1SoqVgpBZN\nm0KjRm4xXFW/zKM5D3vJEvfVDq514mXoUHeAUWjr4bTT3JkUzZvDqae6v2qDBv4dZZ95xl29Gwq2\nbOlv5WRkuC9/CL6nSGcB79vnptxGQtVd//Ofiu8RbDykthBpDU9tw9YpGHFn/343gB1pt9dwHDrk\n9nCK5o+3c6fb8iNwoZ6IO9SobVs33fXoo9025d984yr4pk3dlNv69d06Ce/pegcPlp/CG/o6epXI\ngQOugv/d75y8DRu6uJ06uTy9dOvmDoT/059c+Ndfu/Ug3o0PjfQk1aopW6dgpA1NmsSmEMBVsNF+\nibVqFawQAvn5z914SKtW/nGRpk3dtV492Lgx+LjVRo2il/GII2DiRLeFcoMG/koinBIJvP8+feC1\n18LHjQfTpzslZNQNEj2mkDJKwfY9MuJJUZG/6yiQilZSR6pYb7zRb2/QIPjMDAivFDp1CvY7+mh4\n+unIZYPbVLAy3n+/vPJs186vdLyMHFlxPn/8Y+VlxZsHY9oq04iEd7fURJESSsEw4kHgWIF3XCRa\n+vRxs6vCUdnXfbjwm26CH3/0uxs2dDveern77uD469a5TQwB7rorfDnffONmcEVzjrc3zl//Gj78\nttv84ynvvuuuvXuXj1edY2PPPDPYnZVV9byqyrJlNV9mumNKwagVPPZYdLvJRqJt26oPJA4dCscc\n43f//Oeu+yhSJagK993ndy9bBl26OPuCBXDvvfCvfzl34JbnvXpFL9P48fCb3/hbDF7FNWgQPPmk\ns997r4s3eLDbIuXrr8tX5N27u2t2tt8vUitj9uzgDRdDG/9du8KkSeXTHX+8u27ZEvl+IinKyujT\nx01aMGIgEVOaYjGA5ufn6/z58+M0UcswYqOoyL/9R0lJ+fAbbnBTEiNRVqZaWurivPNO9OV6pzqu\nXRs+fMsW1R9/dHFuvdXv36+f88vJcdc5c4LzC5V19Wp3Pe001c8/r1imM88MzufXv3bXESPc9YMP\nXLzTTy8/ZbOsTHXDBicPqN5/vz89qC5b5tK2aROcbtky1exsN204UPY77nDTiwsKnLtZs9inkaq6\n3+byy6NP07hx5LxSgfnz52t+fn7CpqSmhFIwjFTm+uujqxRefjm8UonErbe6fNevrzgeuHUcXrxK\nwVuJVqYUYuGRR1xFfOqpqvXrByuFH34IjutVIJ07u2tpqT/snXdUi4tV333XhW3YEJw2cCW6l0OH\nVI85JrJst9xScWX+y1+qnn12+Odw993B/q+/7vblCpdPhw5++2efqY4fr1pYWPVnmijSUikAo4Fn\ngJnAsAhx4v2sDCOuPPpo9Sraili4sPI4K1e6jQu9vPeeX55ApfDNN6pXXx0fWYuLnRLwKoXx48vH\n8SoFVdXvvw+fz+HDqrNmlfcPpxQqI5xS2LUruAJXVf3f/9VyrSuvUti0KbhMUL3tNne95x7VE0/0\nK4UtW6KXLRmkpVLwFQKtgOcihMX3SRlGnCkrC66UU4kjjlBds8bv3rGj4q/tWPEqhYMHy4cFKoVY\n8SqF3bujT/POO6o//anq3/7m0n77rb/bLlApqKqvNeDFu7fWvn2uBRQYb/XqYOXsVQqpTqKUQlTH\ncYrI88AvgO/Vc0azx3848ChQ31PpR5p0dg/w5xiGOgwjZRCp+IyKZHLgQLC7VSv48sv45X/bbW4V\neUXng1eHjIzo455xhn+vqgEDoEcPtyAxGtz3p1tDc/iw3795c3eqYLdu0ctR24l29tE0YHigh4jU\nx1X0w4E+wFgR6S0il4nIIyLSQRwPArNVdUlcJTcMI+F06ACXXhr/fL0LCqtKjx7uWq+e2y4lHIFT\nlL1KIZTi4vJbx7/+OsydWz350pmoWgqqukBEckO8BwCrVHUtgIjMBEar6gPASx6/m4HTgBYi0kNV\np8RJbsMwkkyLFlVPm5ERuaKOlX79Ko8Ty0l/3j2y6irVWafQEdgQ4N7o8fOhqo+r6k9V9QZTCIZR\nuwjcxDDVufnmitdBGH6iailEIE56nqAl23l5ebblhWGkAY88AnfckWwpHC+8ELyAEIK7j+rXT/9z\nwQsLC2tkj7jqKIUioHOAuzOutVAlTBkYRnrRvHlsq6wTyeWXl/fzjjvUFrx1ZKKVQ3WUwmKgp2es\nYRMwBhgbB5kMwzCqRbzGK+oiUZ2nICIzgFOA1sD3wO9UdZqInIV/SupUVb0/ZgHsPAXDMIyYSdR5\nCtHOPgrbAlDV2cDs6grh3Trbuo8MwzAqJtHdR7ZLqmEYhuHDjuM0DMNIQ+w4TsMwDCPhx3FaS8Ew\nDCMNqdUtBcMwDCM1SAmlYN1HhmEY0WHdR4ZhGEY5rPvIMAzDSDgpoRSs+8gwDCM6rPvIMAzDKId1\nHxmGYRgJx5SCYRiG4SMllIKNKRiGYUSHjSkYhmEY5bAxBcMwDCPhJFQpiMhPROQpEflfEbk6kWUZ\nhmEY1SehSkFVV6jqDcBFwJmJLCtZpPtYiMmfXEz+5JHOsieSqJSCiDwvIltF5KsQ/+EiskJEvhWR\nOyOkHQW8DcysvripR7q/WCZ/cjH5k0c6y55Iom0pTAOGB3qISH3gzx7/PsBYEektIpeJyCMi0gFA\nVd9U1bOAcXGU2zAMw0gA0Z7RvEBEckO8BwCrVHUtgIjMBEar6gPASx6/U4D/AhoD8+MjsmEYhpEo\nop6S6lEKb6rqMR73+cCZqnqNx30pcKKq3hSTACI2H9UwDKMKJGJKalQthQjEpTJPxE0ZhmEYVaM6\ns4+KgM4B7s7AxuqJYxiGYSST6iiFxUBPEckVkUbAGOCN+IhlGIZhJINop6TOAD4EeonIBhG5UlUP\nA+OBd4CvgZdVdXkshUczpbUmCDflVkSyRORdEVkpInNFpFVA2ESPzCtE5IwA/xNE5CtP2GMB/keI\nyMse/49FpGuc5e8sIvNFZJmILBWRm9PpHkSksYh8IiJLRORrEbk/neT35F9fRD4XkTfTUPa1IvKl\nR/6FaSh/KxF5VUSWe96fE9NFfhE5yvPcvWaXiNycVPlVNSkGqA+sAnKBhsASoHeSZBkKHAd8FeD3\nEPAbj/1O4AGPvY9H1oYe2VfhH7BfCAzw2GcBwz32XwF/8djHADPjLH8O0N9jbw58A/ROs3to6rk2\nAD4GhqSZ/L8G/ga8kYbvzxogK8QvneR/Abgq4P1pmU7yB9xHPWAzris+afLH/cZieACDgDkB7ruA\nu5IoTy7rb1aAAAAgAElEQVTBSmEF0M5jzwFWeOwTgTsD4s0BBgLtgeUB/hcBTwfEOTHgpd2W4Ht5\nHTg9He8BaAosAvqmi/xAJ+A94FTcDL20en9wSqF1iF9ayI9TAKvD+KeF/CEynwEsSLb8ydwQryOw\nIcC90eOXKrRT1a0e+1agncfegeABda/cof5F+O/Hd6/qut12iUhWIoQWN3X4OOAT0ugeRKSeiCzx\nyDlfVZelkfyPAHcAZQF+6SI7uJmE74nIYhG5Js3k7wZsE5FpIvKZiDwrIs3SSP5ALgJmeOxJkz+Z\nSiFt1ieoU7EpL6+INAf+AUxQ1eLAsFS/B1UtU9X+uK/uk0Xk1JDwlJRfREYC36vq50DY6dWpKnsA\ng1X1OOAs4EYRGRoYmOLyNwCOx3WPHA/sxfU6+Ehx+QEQN1lnFPBKaFhNy59MpZDqU1q3ikgOgIi0\nB773+IfK3Qknd5HHHurvTdPFk1cDoKWqbo+nsCLSEKcQXlLV19PxHgBUdRdur6wT0kT+k4CzRWQN\n7ivv5yLyUprIDoCqbvZctwH/xO1WkC7ybwQ2quoij/tVnJLYkibyezkL+NTzG0ASn38ylUKqT2l9\nA/9+TeNw/fRe/4tEpJGIdAN6AgtVdQuw2zPzQYDLgP8Lk9f5wLx4Cuopbyrwtao+mm73ICLZ3tkV\nItIEGAZ8ng7yq+rdqtpZVbvhmv//UtXL0kF2ABFpKiIZHnszXL/2V+kiv6fcDSLSy+N1OrAMeDMd\n5A9gLP6uo9Aya1b+RAyYxDCwchZupswqYGIS5ZgBbAIO4vrergSycIOHK4G5QKuA+Hd7ZF6B2+rD\n638C7g+1Cng8wP8I4H+Bb3Eza3LjLP8QXH/2Elxl+jluo8K0uAfgGOAzj/xfAnd4/NNC/oAyTsE/\n+ygtZMf1yS/xmKXe/2G6yO/Jvx9ucsIXwGu4wed0kr8Z8AOQEeCXNPmTfhynYRiGkTrYcZyGYRiG\nD1MKhmEYho9KlYJUshWFiFwiIl+IWyb/HxE5Ntq0hmEYRmpR4ZiCuNPVvsGN6BfhBnPGasAeRyIy\nCDfrZZeIDAcKVHVgNGkNwzCM1KKyloLvdDVVPYQ7Z3l0YARV/Ujd3HJwq2g7RZvWMAzDSC0qUwqx\nbkVxNW4jpqqkNQzDMJJMZSevRT1f1bMtwVXA4FjTGoZhGKlBZUohqq0oPIPLz+K2at0RY1pTHoZh\nGFVAE3CccWXdR5VuRSEiXXCrCC9V1VWxpPVSfoWfu155pXcfKGemTw92p4bJTwEZ6ob8Z5zh3ovC\nwvLy9+hRPv5f/uKuqkpZmbOvWFE+XuB7B8rIkcH+iTCgPP+8kp+fHzH8tdcSK0M8TCT508Gks+yq\nSqKosKWgqodFxHu6Wn1gqqouF5HrPOFTgN8BmcBTbssNDqnqgEhpE3YnhmEYRrWprPsIVZ0NzA7x\nmxJg/yXwy2jTGkYikLg3og2jbmIrmqtNXrIFqCZ5yRagmuQBEK41HU5RJLDVXSXy8vKSLUK1SGf5\n01n2RGJKodrkJVuAapKXbAGqSV5MsaNRCjXZ6kj3iimd5U9n2RNJpd1HhpEqVFShV60il7Bp33yz\nOnlGz1VXOROJ//qvxJZvpA+JHFgOxZSCUacI/W/V5J/NMKqC1PCAmXUfGYZhGD5MKRhpQ0UfTNF+\n8FvDwDAqxpSCYRiG4cOUglErCNeKSIcpqZHIzc1l3rxEnA8fTEFBAZdddlnCywlkxIgRvPTSSzVa\nphE9phSMWkG6VPbRIiJVHmDMy8tj6tSpUZcTC/Xq1WP16tVVEcvHrFmzalwRJZPp06czdOjQZIsR\nNaYUjLShtlX8iSKWir4qs68qSnP48OGY80sUpaWlQe5Y9wyKJn4q3W+8MKVg1Aqi7T5KJxYuXEjf\nvn3JysriqquuoqSkBICdO3cycuRI2rZtS1ZWFqNGjaKoqAiASZMmsWDBAsaPH09GRgY333wzAMuW\nLWPYsGG0bt2anJwc7r//fsApkIMHDzJu3DhatGjB0UcfzaeffhpWnpNPPhmAfv36kZGRwSuvvEJh\nYSGdOnXioYceon379lx99dUVygfBLZnp06czZMgQ7rjjDrKysujevTtz5syJ+Ew2bdrEeeedR9u2\nbenevTtPPPGEL6ygoIDzzz+fyy67jJYtWzJ9+nTy8vKYNGkSgwcPplmzZqxZs4YPP/yQn/3sZ7Rq\n1YoBAwbw0UcfBcl2zz33BMUPJTc3l4ceeohjjz2WjIwMSktLeeCBB+jRowctWrSgb9++vP766wAs\nX76cG264gY8++oiMjAyysrIAKCkp4fbbb6dr167k5ORwww03cODAgYpeh5ojBXb601C8Xlde6exe\nM316sNtM3TLDhrn3orCwfFjPnuX9nn7aXVVVy8qc/csvA+NQ7t1LFbp27arHHHOMbty4Ubdv366D\nBw/We+65R1VVf/zxR33ttdd0//79WlxcrBdccIGec845vrR5eXk6depUn3v37t2ak5Ojf/rTn7Sk\npESLi4v1k08+UVXV/Px8bdy4sc6ePVvLysp04sSJOnDgwIhyiYh+9913Pvf8+fO1QYMGetddd+nB\ngwd1//79Mck3bdo0bdiwoT733HNaVlamTz31lHbo0CFs2aWlpXr88cfrvffeq4cOHdLVq1dr9+7d\n9Z133vHdS8OGDfX//u//VFV1//79esopp2jXrl3166+/1tLSUt2yZYu2atVK//rXv2ppaanOmDFD\nMzMzdfv27aqq5eIfOnQo7G9z3HHH6caNG/XAgQOqqvrKK6/o5s2bVVX15Zdf1mbNmumWLVtUVXX6\n9Ok6ZMiQoDxuueUWHT16tO7YsUOLi4t11KhROnHixLD3Hek99fgTbxP3DGMWIMwNe71MKZgJNBUp\nhR49yvuls1LIzc3VKVOm+NyzZs3SI488Mmzczz//XDMzM33uvLw8fe6553zuv//973r88ceHTZuf\nn6/DvA9WVZctW6ZNmjSJKFc4pdCoUSMtKSmJmCacfIFKoUePHr6wvXv3qojo1q1by+Xz8ccfa5cu\nXYL8/vCHP+iVV17pu5dTTjklKDwvL0/z8/N97hdffFFPPPHEoDiDBg3S6dOnh40fjtzcXJ02bVqF\ncfr37+9TTtOmTQtSCmVlZdqsWbOg5/jhhx9qt27dwuZV00rBVjQbdQrV6OPGq/spljID6dzZf0ZV\nly5d2LRpEwD79u3j1ltv5Z133mHHDnem1Z49e1BV33hC4LjChg0b6N69e8Ry2rVr57M3bdqUAwcO\nUFZWRr160fUut2nThkaNGvnc0cgXSE5OTlD53vht27YNirdu3To2bdpEZmamz6+0tNTXrQXQqVMn\nQgl8jps2baJLly5B4V27dvU929D4kQiN8+KLL/LII4+wdu1an/w//vhj2LTbtm1j3759nHDCCT4/\nVaWsrKzScmsCG1Mw0oZYK+nqVurxauNUlfXr1wfZO3Z0R5w//PDDrFy5koULF7Jr1y7ef/9931ce\nlB9o7tKlS8QZQ/HYQiE0j8rkqypdunShW7du7Nixw2d2797NW2+95ZMj3P0E+nXs2JF169YFha9b\nt873bMPdTzgC46xbt45rr72WJ598ku3bt7Njxw6OPvroiL9HdnY2TZo04euvv/bdx86dO9m9e3cU\nTyHxmFIw0oaK6pRo67Zq1ks1hqry5JNPUlRUxPbt27nvvvsYM2YM4L5CmzRpQsuWLdm+fTuTJ08O\nStuuXTu+++47n3vkyJFs3ryZxx57jJKSEoqLi1m4cKGvnFgIzTsclclXVQYMGEBGRgYPPfQQ+/fv\np7S0lKVLl7J48WIg8r0E+o8YMYKVK1cyY8YMDh8+zMsvv8yKFSsYOXJk2PjRsHfvXkSE7OxsysrK\nmDZtGkuXLvWFt2vXjo0bN3Lo0CHATeu95ppruOWWW9i2bRsARUVFzJ07N6ZyE0VKK4Vjjgl2h2kZ\nGnWId991lX+4HY+//ba833XXuasIXHGFs/fvnyjp4ouIcMkll3DGGWdw5JFH0rNnT+655x4Abrnl\nFvbv3092djYnnXQSZ511VtDX6IQJE3j11VfJysrilltuoXnz5rz77ru8+eabtG/fnl69elFYWOgr\nJ/RLtqIv5YKCAsaNG0dmZiavvvpq2PSVyRdaVrTl16tXj7feeoslS5bQvXt32rRpw7XXXuv7wo6m\npZCVlcVbb73Fww8/THZ2Nn/84x956623fLOCKrv/cPTp04fbbruNQYMGkZOTw9KlSxkyZIgv/LTT\nTqNv377k5OT4usQefPBBevTowcCBA2nZsiXDhg1j5cqVMZWbKKQyrSgiw4FHcUdqPqeqD4aE/wSY\nBhwHTFLVhwPC1gK7gVI8x3SGyV9DZRDxf9H98IO77t8PnTvD7t3QsiU8+CDceWcst2oYoUi1uzQM\nI9GIhH9PPf5xn3hd4UCziNQH/gycDhQBi0TkDQ0+a/lH4CbgnDBZKJCnqturKmB2drC7RQt3bdy4\nqjkahmEYkais+2gAsEpV16rqIWAmMDowgqpuU9XFwKEIeaT5EiLDMIy6Q2VKoSOwIcC90eMXLQq8\nJyKLReSaWIUzDMMwapbK1ilUt8N1sKpuFpE2wLsiskJVF1QzT8MwDCNBVKYUioDAVRqdca2FqFDV\nzZ7rNhH5J647qpxSKCgo8NndYdp50RZhGIZRJygsLPTNGkskFc4+EpEGwDfAacAmYCEwNmSg2Ru3\nACj2zj4SkaZAfVUtFpFmwFxgsqrODUlX4eyj8HLBY4/BhAnR3KJhRMJmHxmpT0rNPlLVwyIyHngH\nNyV1qqouF5HrPOFTRCQHWAS0AMpEZALQB2gLvOaZ89sA+FuoQjAMwzBSi0r3PlLV2cDsEL8pAfYt\nBHcxedkDpMlSIcMwDANSfEWzYRixUVhYGLRZ29FHH82///3vqOLGyg033MB///d/Vzm9kZrYLqmG\nUYsJ3IOnOkyfPp2pU6eyYIF/nshTTz0Vl7xrC/Xq1WPVqlUV7kibDlhLwTCMtCXccZihx3BWRjTx\no82zNkxcMKVgGCnGgw8+yAUXXBDkN2HCBCZ4pttNmzaNPn360KJFC4488kieeeaZiHnl5uYyb948\nAPbv388VV1xBVlYWffv2ZdGiRUFxYz1S8oorruC3v/2tL/2zzz5Lz549ad26NaNHj2bz5s2+sHr1\n6jFlyhR69epFZmYm48ePjyizqvpkyc7OZsyYMb5zGdauXUu9evV4/vnn6dq1K6eddhovvPACgwcP\n5te//jXZ2dlMnjyZ3bt3c/nll9O2bVtyc3O57777fBX29OnTy8UPJfRozxdeeIFFixYxaNAgMjMz\n6dChAzfddJNv59NwR5UCvPXWW/Tv35/MzEwGDx7MV199FfG+U4ZEnNwTiyHMqUKVHYgFqo89Fq/d\n7s3UXUPFL1qSWLdunTZt2lSLi4tVVfXw4cPavn173xGab7/9tq5evVpVVd9//31t2rSpfvbZZ6rq\nTkLr1KmTL6/c3FydN2+eqqreeeedevLJJ+uOHTt0w4YN2rdvX+3cubMvbqxHSl5xxRX629/+VlVV\n582bp9nZ2fr5559rSUmJ3nTTTXryySf74oqIjho1Snft2qXr16/XNm3a6Jw5c8Le/6OPPqqDBg3S\noqIiPXjwoF533XU6duxYVVVds2aNioiOGzdO9+3bp/v379dp06ZpgwYN9M9//rOWlpbq/v379bLL\nLtNzzjlH9+zZo2vXrtVevXoFnfYWGj+UcEd7fvrpp/rJJ59oaWmprl27Vnv37q2PPvpo0D0Gnqb2\n2Wefadu2bXXhwoVaVlamL7zwgubm5lZ4Sl04Ir2nHn/ibeKeYcwChLnhyv6rYErBTDwMFb9oSWTI\nkCH64osvqqrq3LlzIx7Fqap6zjnn6GOPPaaqFSuFwPOMVVWfeeaZoLihVHSkpGqwUrjqqqv0zjvv\n9IXt2bNHGzZsqOvWrVNVV2H+5z//8YVfeOGF+sADD4Qtt3fv3j6ZVVU3bdqkDRs21NLSUp9SWLNm\njS982rRpQcd0Hj58WBs1aqTLly/3+U2ZMkXz8vLCxg9HuKM9Q3nkkUf03HPP9blDlcL111/vez5e\njjrqKH3//fcrzDeUmlYK1n1kGJEQiY+pAhdffDEzZswA4O9//zuXXHKJL2z27NkMHDiQ1q1bk5mZ\nyaxZsyIe/RjIpk2byh3xGciLL77IcccdR2ZmJpmZmSxdujSqfAE2b95M165dfe5mzZrRunVrioqK\nfH6hx27u2bMnbF5r167l3HPP9cnRp08fGjRowNatW31xQmdNBbp/+OEHDh06FCRPly5dgmSJZtZV\n6NGeK1euZOTIkbRv356WLVsyadKkCp/PunXrePjhh333kZmZycaNG4O61VIRUwqGEYm4NUhi5/zz\nz6ewsJCioiJef/11Lr74YgBKSko477zz+M1vfsP333/Pjh07GDFiBBpFOe3bty93xKeXWI+UDKVD\nhw6+84nBnUb2448/Bh1zGS1dunRhzpw5Qcdu7tu3j/bt2/viVHQwT3Z2Ng0bNgySZ/369UGVfGX3\nE+7AnhtuuIE+ffqwatUqdu3axX333VfhucpdunRh0qRJQfexZ88e3wl6qYopBcNIQdq0aUNeXh5X\nXHEF3bt356ijjgLg4MGDHDx4kOzsbOrVq8fs2bOjPsbxwgsv5P7772fnzp1s3LiRJ554whcW65GS\ngK+7AWDs2LFMmzaNL774gpKSEu6++24GDhxYrjUSmDYS119/PXfffbdPaW3bto033ngjqnsEqF+/\nPhdeeCGTJk1iz549rFu3jkceeYRLL7006jzCybdnzx4yMjJo2rQpK1asKDclN/So0muuuYann36a\nhQsXoqrs3buXt99+O2ILKVUwpWAYKcrFF1/MvHnzfK0EgIyMDB5//HEuvPBCsrKymDFjBqNHBx1x\nEvErOD8/n65du9KtWzeGDx/O5Zdf7otblSMlA7+mTzvtNO69917OO+88OnTowJo1a5g5c2ZEmSId\nnQluptXZZ5/NGWecQYsWLRg0aJDvTOlo83riiSdo1qwZ3bt3Z+jQoVxyySVceeWVlZZdUZ5//OMf\n+fvf/06LFi249tprueiii4LihB5VesIJJ/Dss88yfvx4srKy6NmzJy+++GKF5aYClR7HmXABqrgh\n3pNPwo03Jlg4o5ZjG+IZqU9Nb4iXki0Fz7TqiHzwAVx9tbsahmEY8SMlWwqxpY+jMEYdw1oKRupj\nLQXDMAwjaZhSMAzDMHyYUjAMwzB8VKoURGS4iKwQkW9F5M4w4T8RkY9E5ICI3BZLWsMwDCO1qOyM\n5vq4M5pPB4pwx24GndEsIm2ArsA5wA71n9FcaVpPPBtoNpKEDTQbqU9KndEMDABWqepajxAzgdGA\nr2JX1W3ANhH5RaxpDSPZVLaIyTDqGpV1H3UENgS4N3r8oqE6aQ2jBtC0N/37Rw5btCi2vIYP99uL\nigJ3Mg42Rx3lrt6wSZOCw1esUP7wB2c//fTgMG+e3jyaN1deeKF8uNeMGBE+fUWmdevo46aLqUkq\nUwrVkcba5YZRC4mlcRUpbqB/RflZQ67mqaz7qAgI3GO2M+6LPxqiTltQUOCz5+XlkZeXF2URhlG3\nSVSFms5DLekse0UUFhZSWFiY8HIqUwqLgZ4ikgtsAsYAYyPEDX0Fo04bqBQMw0htQpVNqDuwUq6s\npVBbK/BEEPrBHO4Y0XhQoVJQ1cMiMh54B6gPTFXV5SJynSd8iojk4GYWtQDKRGQC0EdV94RLm5C7\nMIw6SjJaChUpgapg3UepRWUtBVR1NjA7xG9KgH0Lwd1EFaY1DCM1iddXu2rllXm0LQVTCjWPrWg2\njDQmFVoK1a24raWQWphSMAwjJiqrqJM9pmDjFNXDlIJhGEDyKlNrKaQWphQMo5aSqAq1JmcfVeUe\nrKVQPUwpGEYaE8+KP7Ayre7so3gscIs1HyM+2MlrhlFL6dsXli2rWtrTT4fzzoP77oONEZar/vSn\nsHgxHHssfPml379fP+jTB2bMKJ9m6FCYPBl+/nO/X7dusGaNs197LVx1FezcCbt2wZgxwennzoWG\nDaF9e2jQAJ54AjIyoH9/aNsWunZ1BmD9eigogBNPhOOOc2bbNliyxIXl5kKzZk7W996DFi1cfdKl\nC2zYAN98A6NGOaWnCt27V+1ZJopEbYiX9kohMxMuvBCeecbvd9pplZ/zbBhG3eLxx+Hmm6uePtW6\npew4zgjs2AFTPKsmli51P9x77/l/wBEjkiebYRipw86dyZYgPUh7pWAYhhEN1tUcHaYUDMMwDB+m\nFAzDqBNYSyE6TCkYhmEYPkwpGIZRJ7CWQnTUeqVgL4JhGGB1QbTUeqVgGIZhRI8pBcMw6gTWUoiO\nSpWCiAwXkRUi8q2I3BkhzuOe8C9E5LgA/7Ui8qWIfC4iC+MpuGEYhhF/Kjx5TUTqA38GTgeKgEUi\n8kbgsZoiMgLooao9ReRE4ClgoCdYgTxV3Z4Q6Q3DMIy4UllLYQCwSlXXquohYCYwOiTO2cALAKr6\nCdBKRNoFhFujzTAMI02oTCl0BDYEuDd6/KKNo8B7IrJYRK6pjqCGYRjVwcYUoqPC7iNcpR4NkR73\nEFXdJCJtgHdFZIWqLohePMMwjPhgSiE6KlMKRUDnAHdnXEugojidPH6o6ibPdZuI/BPXHVVOKRQU\nFPjseXl55OXlRSW8YRhGXaGwsJDCwsLEF6SqEQ1OaXwH5AKNgCVA75A4I4BZHvtA4GOPvSmQ4bE3\nA/4DnBGmDI0HoLp0aXm/kSO9R2SYMWOmLpuHHqpe+lTDU3cSb1NhS0FVD4vIeOAdoD4wVVWXi8h1\nnvApqjpLREaIyCpgL3ClJ3kO8Jq4NlsD4G+qOre6SswwDMNIHJV1H6Gqs4HZIX5TQtzjw6RbDfSv\nroCGYRjxwMYUosNWNBuGUScwpRAdtUoptG6dbAkMw0hVbr+9eunvvTc+cqQ6tUYpqEJOTuSwXbv8\ndlWYODHYHWhmzfKHhXLuubBlS+zyHX987GkMw0gdXn452RLUDLVGKcRKuArfMAyjrlNnlIIpAcMw\njMqp9Uoh3oNLVVUuNshlGEY6UOuVQqpgLRXDSG/qyn/YlIJhGEYUmFIwDMMwfJhSMAzDMHyYUjDi\nig00G0Z6Y0rBCEtdeTEMw6ib1FmlYJW7YRixUFfqjDqrFAzDMGLBlEItwfryDcOIB6YUDMMwDB+m\nFIyw1JUXwzCMYOrKf79SpSAiw0VkhYh8KyJ3RojzuCf8CxE5Lpa0hmEYRupQoVIQkfrAn4HhQB9g\nrIj0DokzAuihqj2Ba4Gnok1bOyhMtgDVpDDZAlSTwmQLUE0Kky1ANSlMtgDVoDCm2NZScAwAVqnq\nWlU9BMwERofEORt4AUBVPwFaiUhOlGlrAYXJFqCaFCZbgGpSmGwBqklhsgWoJoXJFqAaFMYUu6ws\nMVKkGpUphY7AhgD3Ro9fNHE6RJHWMAwjLbCWgiPax5CyEz+zs921fv1g/1atIqdp2rTi/Bo0iK7s\n5s399vbto0tjGEZq0qZNsiWoGUQrUH8iMhAoUNXhHvdEoExVHwyI8zRQqKozPe4VwClAt8rSevzr\niP41DMOIL6oa9w/yyr55FwM9RSQX2ASMAcaGxHkDGA/M9CiRnaq6VUR+jCJtQm7KMAzDqBoVKgVV\nPSwi44F3gPrAVFVdLiLXecKnqOosERkhIquAvcCVFaVN5M0YhmEY1aPC7iPDMAyjbpHUFc2psrhN\nRJ4Xka0i8lWAX5aIvCsiK0Vkroi0Cgib6JF5hYicEeB/goh85Ql7LMD/CBF52eP/sYh0jbP8nUVk\nvogsE5GlInJzOt2DiDQWkU9EZImIfC0i96eT/J7864vI5yLyZhrKvlZEvvTIvzAN5W8lIq+KyHLP\n+3NiusgvIkd5nrvX7BKRm5Mqv6omxeC6lFYBuUBDYAnQO0myDAWOA74K8HsI+I3HfifwgMfexyNr\nQ4/sq/C3uBYCAzz2WcBwj/1XwF889jHAzDjLnwP099ibA98AvdPsHpp6rg2Aj4EhaSb/r4G/AW+k\n4fuzBsgK8Usn+V8Argp4f1qmk/wB91EP2Ax0Tqb8cb+xGB7AIGBOgPsu4K4kypNLsFJYAbTz2HOA\nFR77RODOgHhzgIFAe2B5gP9FwNMBcU4MeGm3JfheXgdOT8d7AJoCi4C+6SI/0Al4DzgVeDPd3h+c\nUmgd4pcW8uMUwOow/mkhf4jMZwALki1/MruPolkYl0zaqepWj30r0M5j74CT1UvgYr1A/yL89+O7\nV1U9DOwSkaxECC1uttdxwCek0T2ISD0RWeKRc76qLksj+R8B7gAC17ymi+zg1iO9JyKLReSaNJO/\nG7BNRKaJyGci8qyINEsj+QO5CJjhsSdN/mQqhbQZ4VanYlNeXhFpDvwDmKCqxYFhqX4Pqlqmqv1x\nX90ni8ipIeEpKb+IjAS+V9XPibCIM1VlD2Cwqh4HnAXcKCJDAwNTXP4GwPG47pHjcTMg7wqMkOLy\nAyAijYBRwCuhYTUtfzKVQhGu78xLZ4I1XbLZKm4PJ0SkPfC9xz9U7k44uYs89lB/b5ounrwaAC1V\ndXs8hRWRhjiF8JKqvp6O9wCgqruAt4ET0kT+k4CzRWQN7ivv5yLyUprIDoCqbvZctwH/xO1bli7y\nbwQ2quoij/tVnJLYkibyezkL+NTzG0ASn38ylYJvYZxHS47BLYRLFd4Axnns43D99F7/i0SkkYh0\nA3oCC1V1C7DbM/NBgMuA/wuT1/nAvHgK6ilvKvC1qj6abvcgItne2RUi0gQYBnyeDvKr6t2q2llV\nu+Ga//9S1cvSQXYAEWkqIhkeezNcv/ZX6SK/p9wNItLL43U6sAx4Mx3kD2As/q6j0DJrVv5EDJjE\nMLByFm6mzCpgYhLlmIFbdX0Q1/d2JZCFGzxcCcwFWgXEv9sj8wrgzAD/E3B/qFXA4wH+RwD/C3yL\nmznvP/MAAACXSURBVFmTG2f5h+D6s5fgKtPPcVuWp8U9AMcAn3nk/xK4w+OfFvIHlHEK/tlHaSE7\nrk9+iccs9f4P00V+T/79cJMTvgBeww0+p5P8zYAfgIwAv6TJb4vXDMMwDB92HKdhGIbhw5SCYRiG\n4cOUgmEYhuHDlIJhGIbhw5SCYRiG4cOUgmEYhuHDlIJhGIbhw5SCYRiG4eP/AZqxTd+d/WkgAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f96f4786490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" % (best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" % (compute_error_rate(cifar10_test_stream) * 100.0,)\n",
    "\n",
    "subplot(2, 1, 1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:, 0], train_nll_a[:, 1], label = \"batch train nll\")\n",
    "legend()\n",
    "\n",
    "subplot(2, 1, 2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:, 0], train_erros_a[:, 1], label = \"batch train error rate\")\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:, 0], validation_errors_a[:, 1], label = \"validation error rate\", color = \"r\")\n",
    "ylim(0, 0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the results of the label clasification.\n",
    "results = []\n",
    "labels  = []\n",
    "for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "    results += list(predict(X).ravel())\n",
    "    labels  += list(Y.ravel())\n",
    "    \n",
    "with open(\"listo_KIPE10.txt\", \"w\") as listing:\n",
    "    for result, label in zip(results, labels):\n",
    "        listing.write(str(result) + \" \" + str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_fw4_hidden': 10, 'decay_const': 0.001, 'K': 3000, 'gauss': 0.025, 'momentum': 0.8, 'lrate_const': 0.024, 'num_fw3_hidden': 500, 'size_cw1': (5, 5), 'size_cw2': (5, 5), 'size_p1': (2, 2), 'size_p2': (2, 2), 'num_filters_1': 50, 'num_filters_2': 75}\n"
     ]
    }
   ],
   "source": [
    "print params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here are two tables of results depending on the parameters from the `params` dictionary:\n",
    "\n",
    "| momentum | lrate_const || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
    "|:-------- | -----------:|| ---------------:|:----------:|:---------:| --------------:| ------------------:|\n",
    "| 0.9      |        2e-3 ||      34.410000% |         27 |        42 |     34.250000% |         20.372500% |\n",
    "|          |        4e-3 ||      32.910000% |         15 |        24 |     33.830000% |          9.650000% |\n",
    "|          |        8e-3 ||      32.050000% |         14 |        22 |     30.990000% |          0.352500% |\n",
    "|          |       12e-3 ||      32.450000% |         10 |        16 |     32.400000% |          1.422500% |\n",
    "|          |       16e-3 ||      31.460000% |          9 |        15 |     31.190000% |          2.002500% |\n",
    "|          |       20e-3 ||      31.210000% |         31 |        48 |     30.280000% |          0.005000% |\n",
    "|          |       24e-3 ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "|          |       28e-3 ||      31.240000% |         33 |        51 |     30.550000% |          0.010000% |\n",
    "|          |       32e-3 ||      32.760000% |         11 |        18 |     32.590000% |          5.222500% |\n",
    "|          |             ||                 |            |           |                |                    |\n",
    "| 0.7      |       24e-3 ||      31.470000% |          9 |        14 |     31.230000% |          4.807500% |\n",
    "| 0.75     |             ||      31.410000% |         16 |        25 |     31.010000% |          0.082500% |\n",
    "| 0.8      |             ||      30.950000% |          9 |        14 |     30.980000% |          1.715000% |\n",
    "| 0.85     |             ||      31.090000% |         26 |        40 |     31.340000% |          0.007500% |\n",
    "| 0.9      |             ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "\n",
    "| K    | lrate_const | momentum || num_filters_1 | num_filters_2 | gauss || test error rate |\n",
    "| ----:| -----------:| --------:|| -------------:| -------------:|:----- || ---------------:|\n",
    "| 2000 |       0.004 |      0.9 ||            10 |            25 | 0.05  ||        original |\n",
    "|      |             |          ||               |               |       ||                 |\n",
    "|      |       0.024 |      0.8 ||               |            35 |       ||      30.080000% |\n",
    "|      |       0.024 |      0.8 ||            15 |            50 |       ||      28.350000% |\n",
    "|      |       0.024 |      0.8 ||            15 |            50 | 0.025 ||      27.530000% |\n",
    "|      |       0.024 |      0.8 ||            50 |            50 | 0.025 ||      25.910000% |\n",
    "| 4000 |       0.024 |      0.8 ||            50 |            50 | 0.025 ||      26.030000% |\n",
    "| 3000 |       0.024 |      0.8 ||            50 |            50 | 0.025 ||      25.690000% |\n",
    "| 3000 |       0.024 |      0.8 ||            50 |            75 | 0.025 ||      25.340000% |\n",
    "\n",
    "Empty cells represent using the same value as in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
