{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n"
     ]
    }
   ],
   "source": [
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will now build a convolutional network for the CIFAR-10 data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = ((ScaleAndShift, [2.0 / 255.0, -1], {\"which_sources\" : \"features\"}),\n",
    "                                (Cast, [np.float32], {\"which_sources\" : \"features\"}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset = slice(None, 40000))\n",
    "# this stream will shuffle the CIFAR-10 set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(cifar10_train,\n",
    "                                                 iteration_scheme = ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset = slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these don't do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(cifar10_validation,\n",
    "                                                      iteration_scheme = SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(cifar10_test,\n",
    "                                                iteration_scheme = SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are taken from https://github.com/mila-udem/blocks.\n",
    "\n",
    "class Constant():\n",
    "    '''Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    '''\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype = np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    '''Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    '''\n",
    "    def __init__(self, std = 1, mean = 0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size = shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    '''Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width / 2, mean + width / 2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    '''\n",
    "    def __init__(self, mean = 0., width = None, std = None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1 / 12 * width ^ 2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size = shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4(\"X\")\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix(\"Y\", dtype = \"uint8\")\n",
    "\n",
    "# The tag values are useful during debugging the creation of Theano graphs\n",
    "\n",
    "X_test_value, Y_test_value = next(cifar10_train_stream.get_epoch_iterator())\n",
    "\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values\n",
    "\n",
    "\n",
    "X.tag.test_value = X_test_value[: 3]\n",
    "Y.tag.test_value = Y_test_value[: 3]\n",
    "\n",
    "print \"X shape: %s\" % (X.tag.test_value.shape,)\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "# The first convolutional layer\n",
    "# The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = 10 # we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1, 3, 5, 5), dtype = \"float32\"),\n",
    "                    name = \"CW1\")\n",
    "# please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype = \"float32\"),\n",
    "                    name = \"CB1\")\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2, 2), ignore_border = True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_filters_2 = 25 # we will compute ten convolution filters in the first layer\n",
    "CW2 = theano.shared(np.zeros((num_filters_2, num_filters_1, 5, 5), dtype = \"float32\"),\n",
    "                   name = \"CW2\")\n",
    "CW2.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype = \"float32\"),\n",
    "                    name = \"CB2\")\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2, 2), ignore_border = True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "# Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden = 500\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW3\")\n",
    "FW3.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB3\")\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle(\"x\", 0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_fw4_hidden = 10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW4\")\n",
    "FW4.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB4\")\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle(\"x\", 0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis = 1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions, Y.ravel()).mean()\n",
    "nll = -theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1] == \"W\":\n",
    "        weight_decay = weight_decay + 1e-3 * (p ** 2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "# At this point stop computing test values\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "#\n",
    "# svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar(\"lrate\", dtype = \"float32\")\n",
    "momentum = theano.tensor.scalar(\"momentum\", dtype = \"float32\")\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name = \"V_%s\" % (p.name,)) for p in model_parameters]\n",
    "\n",
    "for p, g, v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v, v_new), (p, p_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile theano functions\n",
    "\n",
    "# each call to train step will make one SGD step\n",
    "train_step = theano.function([X, Y, lrate, momentum], [cost, error_rate, nll, weight_decay], updates = updates)\n",
    "# each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X) != Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow = False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.711650, batch nll 1.959422, batch error rate 76.000000%\n",
      "At minibatch 200, batch loss 2.408616, batch nll 1.718295, batch error rate 68.000000%\n",
      "At minibatch 300, batch loss 2.374753, batch nll 1.737611, batch error rate 84.000000%\n",
      "At minibatch 400, batch loss 2.153195, batch nll 1.563283, batch error rate 56.000000%\n",
      "At minibatch 500, batch loss 2.268943, batch nll 1.718547, batch error rate 68.000000%\n",
      "At minibatch 600, batch loss 1.923370, batch nll 1.407604, batch error rate 52.000000%\n",
      "At minibatch 700, batch loss 2.286233, batch nll 1.802859, batch error rate 68.000000%\n",
      "At minibatch 800, batch loss 2.073395, batch nll 1.617808, batch error rate 60.000000%\n",
      "At minibatch 900, batch loss 2.199864, batch nll 1.764676, batch error rate 68.000000%\n",
      "At minibatch 1000, batch loss 2.029346, batch nll 1.613809, batch error rate 48.000000%\n",
      "At minibatch 1100, batch loss 1.966268, batch nll 1.571562, batch error rate 52.000000%\n",
      "At minibatch 1200, batch loss 1.957394, batch nll 1.574717, batch error rate 56.000000%\n",
      "At minibatch 1300, batch loss 2.112655, batch nll 1.743261, batch error rate 64.000000%\n",
      "At minibatch 1400, batch loss 1.869791, batch nll 1.513562, batch error rate 56.000000%\n",
      "At minibatch 1500, batch loss 1.916491, batch nll 1.567225, batch error rate 64.000000%\n",
      "At minibatch 1600, batch loss 1.429201, batch nll 1.088567, batch error rate 28.000000%\n",
      "After epoch 1: valid_err_rate: 53.640000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 61.287500% averaged train nll: 1.692010 averaged train loss: 2.188840\n",
      "At minibatch 1700, batch loss 1.883553, batch nll 1.547855, batch error rate 40.000000%\n",
      "At minibatch 1800, batch loss 1.552613, batch nll 1.221206, batch error rate 36.000000%\n",
      "At minibatch 1900, batch loss 1.801037, batch nll 1.470640, batch error rate 64.000000%\n",
      "At minibatch 2000, batch loss 2.289589, batch nll 1.963205, batch error rate 68.000000%\n",
      "At minibatch 2100, batch loss 1.834885, batch nll 1.509169, batch error rate 56.000000%\n",
      "At minibatch 2200, batch loss 1.990891, batch nll 1.669147, batch error rate 56.000000%\n",
      "At minibatch 2300, batch loss 1.799347, batch nll 1.482025, batch error rate 52.000000%\n",
      "At minibatch 2400, batch loss 1.920220, batch nll 1.609018, batch error rate 56.000000%\n",
      "At minibatch 2500, batch loss 2.614748, batch nll 2.308019, batch error rate 84.000000%\n",
      "At minibatch 2600, batch loss 1.695939, batch nll 1.391697, batch error rate 52.000000%\n",
      "At minibatch 2700, batch loss 1.833103, batch nll 1.531035, batch error rate 56.000000%\n",
      "At minibatch 2800, batch loss 1.828954, batch nll 1.531494, batch error rate 64.000000%\n",
      "At minibatch 2900, batch loss 1.876144, batch nll 1.581703, batch error rate 52.000000%\n",
      "At minibatch 3000, batch loss 1.491121, batch nll 1.199518, batch error rate 40.000000%\n",
      "At minibatch 3100, batch loss 1.930955, batch nll 1.642635, batch error rate 68.000000%\n",
      "At minibatch 3200, batch loss 1.506529, batch nll 1.219416, batch error rate 48.000000%\n",
      "After epoch 2: valid_err_rate: 46.120000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 50.972500% averaged train nll: 1.429626 averaged train loss: 1.741953\n",
      "At minibatch 3300, batch loss 1.428827, batch nll 1.139549, batch error rate 44.000000%\n",
      "At minibatch 3400, batch loss 1.394505, batch nll 1.104871, batch error rate 44.000000%\n",
      "At minibatch 3500, batch loss 1.639490, batch nll 1.349021, batch error rate 56.000000%\n",
      "At minibatch 3600, batch loss 1.570865, batch nll 1.279741, batch error rate 44.000000%\n",
      "At minibatch 3700, batch loss 1.321654, batch nll 1.031503, batch error rate 36.000000%\n",
      "At minibatch 3800, batch loss 1.659556, batch nll 1.371320, batch error rate 48.000000%\n",
      "At minibatch 3900, batch loss 1.750800, batch nll 1.462652, batch error rate 48.000000%\n",
      "At minibatch 4000, batch loss 1.601323, batch nll 1.314064, batch error rate 48.000000%\n",
      "At minibatch 4100, batch loss 1.306354, batch nll 1.020447, batch error rate 40.000000%\n",
      "At minibatch 4200, batch loss 1.392962, batch nll 1.109105, batch error rate 48.000000%\n",
      "At minibatch 4300, batch loss 1.468503, batch nll 1.185945, batch error rate 48.000000%\n",
      "At minibatch 4400, batch loss 1.233254, batch nll 0.954064, batch error rate 36.000000%\n",
      "At minibatch 4500, batch loss 1.473959, batch nll 1.195616, batch error rate 44.000000%\n",
      "At minibatch 4600, batch loss 1.469442, batch nll 1.193982, batch error rate 32.000000%\n",
      "At minibatch 4700, batch loss 1.198810, batch nll 0.924114, batch error rate 32.000000%\n",
      "At minibatch 4800, batch loss 1.624451, batch nll 1.350702, batch error rate 52.000000%\n",
      "After epoch 3: valid_err_rate: 43.030000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 41.487500% averaged train nll: 1.179407 averaged train loss: 1.464099\n",
      "At minibatch 4900, batch loss 1.114954, batch nll 0.837510, batch error rate 24.000000%\n",
      "At minibatch 5000, batch loss 1.361605, batch nll 1.081803, batch error rate 40.000000%\n",
      "At minibatch 5100, batch loss 0.974409, batch nll 0.693361, batch error rate 24.000000%\n",
      "At minibatch 5200, batch loss 1.486445, batch nll 1.206119, batch error rate 36.000000%\n",
      "At minibatch 5300, batch loss 1.557685, batch nll 1.275600, batch error rate 44.000000%\n",
      "At minibatch 5400, batch loss 1.264043, batch nll 0.981225, batch error rate 32.000000%\n",
      "At minibatch 5500, batch loss 1.337707, batch nll 1.055133, batch error rate 40.000000%\n",
      "At minibatch 5600, batch loss 1.268003, batch nll 0.984667, batch error rate 32.000000%\n",
      "At minibatch 5700, batch loss 0.975698, batch nll 0.691500, batch error rate 32.000000%\n",
      "At minibatch 5800, batch loss 1.041278, batch nll 0.758235, batch error rate 24.000000%\n",
      "At minibatch 5900, batch loss 1.661116, batch nll 1.378121, batch error rate 48.000000%\n",
      "At minibatch 6000, batch loss 1.130911, batch nll 0.848385, batch error rate 28.000000%\n",
      "At minibatch 6100, batch loss 1.097218, batch nll 0.815032, batch error rate 28.000000%\n",
      "At minibatch 6200, batch loss 1.020131, batch nll 0.739156, batch error rate 28.000000%\n",
      "At minibatch 6300, batch loss 1.182997, batch nll 0.902587, batch error rate 32.000000%\n",
      "At minibatch 6400, batch loss 1.562015, batch nll 1.282271, batch error rate 36.000000%\n",
      "After epoch 4: valid_err_rate: 35.570000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 35.535000% averaged train nll: 1.009096 averaged train loss: 1.290474\n",
      "At minibatch 6500, batch loss 1.272302, batch nll 0.990425, batch error rate 28.000000%\n",
      "At minibatch 6600, batch loss 1.486649, batch nll 1.202920, batch error rate 32.000000%\n",
      "At minibatch 6700, batch loss 0.950065, batch nll 0.664792, batch error rate 28.000000%\n",
      "At minibatch 6800, batch loss 1.349660, batch nll 1.063400, batch error rate 28.000000%\n",
      "At minibatch 6900, batch loss 1.051480, batch nll 0.764332, batch error rate 28.000000%\n",
      "At minibatch 7000, batch loss 1.330420, batch nll 1.041955, batch error rate 28.000000%\n",
      "At minibatch 7100, batch loss 1.313897, batch nll 1.024796, batch error rate 32.000000%\n",
      "At minibatch 7200, batch loss 1.347955, batch nll 1.057709, batch error rate 36.000000%\n",
      "At minibatch 7300, batch loss 1.000693, batch nll 0.709487, batch error rate 28.000000%\n",
      "At minibatch 7400, batch loss 1.000896, batch nll 0.709872, batch error rate 24.000000%\n",
      "At minibatch 7500, batch loss 1.502288, batch nll 1.210171, batch error rate 40.000000%\n",
      "At minibatch 7600, batch loss 1.253164, batch nll 0.962576, batch error rate 32.000000%\n",
      "At minibatch 7700, batch loss 1.153100, batch nll 0.863145, batch error rate 32.000000%\n",
      "At minibatch 7800, batch loss 1.033183, batch nll 0.742974, batch error rate 28.000000%\n",
      "At minibatch 7900, batch loss 1.272336, batch nll 0.981674, batch error rate 40.000000%\n",
      "At minibatch 8000, batch loss 1.153156, batch nll 0.862176, batch error rate 32.000000%\n",
      "After epoch 5: valid_err_rate: 34.910000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 30.985000% averaged train nll: 0.878583 averaged train loss: 1.166913\n",
      "At minibatch 8100, batch loss 1.057908, batch nll 0.764830, batch error rate 28.000000%\n",
      "At minibatch 8200, batch loss 0.949100, batch nll 0.653590, batch error rate 20.000000%\n",
      "At minibatch 8300, batch loss 0.877695, batch nll 0.580668, batch error rate 20.000000%\n",
      "At minibatch 8400, batch loss 1.428758, batch nll 1.130217, batch error rate 36.000000%\n",
      "At minibatch 8500, batch loss 0.836762, batch nll 0.537037, batch error rate 20.000000%\n",
      "At minibatch 8600, batch loss 0.784938, batch nll 0.484838, batch error rate 28.000000%\n",
      "At minibatch 8700, batch loss 0.773328, batch nll 0.471475, batch error rate 16.000000%\n",
      "At minibatch 8800, batch loss 1.195180, batch nll 0.892575, batch error rate 32.000000%\n",
      "At minibatch 8900, batch loss 1.130838, batch nll 0.827065, batch error rate 40.000000%\n",
      "At minibatch 9000, batch loss 1.235261, batch nll 0.930431, batch error rate 48.000000%\n",
      "At minibatch 9100, batch loss 1.105127, batch nll 0.799366, batch error rate 28.000000%\n",
      "At minibatch 9200, batch loss 1.096218, batch nll 0.790439, batch error rate 16.000000%\n",
      "At minibatch 9300, batch loss 0.921291, batch nll 0.615156, batch error rate 24.000000%\n",
      "At minibatch 9400, batch loss 1.308004, batch nll 1.001669, batch error rate 36.000000%\n",
      "At minibatch 9500, batch loss 1.141174, batch nll 0.834820, batch error rate 36.000000%\n",
      "At minibatch 9600, batch loss 1.446609, batch nll 1.140774, batch error rate 48.000000%\n",
      "After epoch 6: valid_err_rate: 34.980000% currently going to do 8 epochs\n",
      "After epoch 6: averaged train_err_rate: 27.017500% averaged train nll: 0.771392 averaged train loss: 1.073002\n",
      "At minibatch 9700, batch loss 1.186013, batch nll 0.877004, batch error rate 36.000000%\n",
      "At minibatch 9800, batch loss 0.701520, batch nll 0.389926, batch error rate 8.000000%\n",
      "At minibatch 9900, batch loss 0.816408, batch nll 0.503450, batch error rate 20.000000%\n",
      "At minibatch 10000, batch loss 1.129314, batch nll 0.814795, batch error rate 32.000000%\n",
      "At minibatch 10100, batch loss 1.166416, batch nll 0.850813, batch error rate 24.000000%\n",
      "At minibatch 10200, batch loss 1.110904, batch nll 0.794284, batch error rate 24.000000%\n",
      "At minibatch 10300, batch loss 0.919185, batch nll 0.601889, batch error rate 12.000000%\n",
      "At minibatch 10400, batch loss 1.048565, batch nll 0.730139, batch error rate 32.000000%\n",
      "At minibatch 10500, batch loss 1.163879, batch nll 0.844683, batch error rate 32.000000%\n",
      "At minibatch 10600, batch loss 1.161203, batch nll 0.841131, batch error rate 24.000000%\n",
      "At minibatch 10700, batch loss 0.732446, batch nll 0.411602, batch error rate 16.000000%\n",
      "At minibatch 10800, batch loss 0.812761, batch nll 0.491235, batch error rate 16.000000%\n",
      "At minibatch 10900, batch loss 1.361310, batch nll 1.039757, batch error rate 24.000000%\n",
      "At minibatch 11000, batch loss 1.296295, batch nll 0.974171, batch error rate 36.000000%\n",
      "At minibatch 11100, batch loss 0.801055, batch nll 0.478422, batch error rate 12.000000%\n",
      "At minibatch 11200, batch loss 0.897429, batch nll 0.574727, batch error rate 24.000000%\n",
      "After epoch 7: valid_err_rate: 32.860000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 23.772500% averaged train nll: 0.679484 averaged train loss: 0.996884\n",
      "At minibatch 11300, batch loss 0.869696, batch nll 0.545175, batch error rate 16.000000%\n",
      "At minibatch 11400, batch loss 0.852159, batch nll 0.525551, batch error rate 12.000000%\n",
      "At minibatch 11500, batch loss 0.720106, batch nll 0.391943, batch error rate 16.000000%\n",
      "At minibatch 11600, batch loss 0.914147, batch nll 0.584771, batch error rate 16.000000%\n",
      "At minibatch 11700, batch loss 0.935488, batch nll 0.604397, batch error rate 20.000000%\n",
      "At minibatch 11800, batch loss 0.937598, batch nll 0.605018, batch error rate 20.000000%\n",
      "At minibatch 11900, batch loss 0.855014, batch nll 0.521767, batch error rate 16.000000%\n",
      "At minibatch 12000, batch loss 0.941602, batch nll 0.607380, batch error rate 28.000000%\n",
      "At minibatch 12100, batch loss 1.196769, batch nll 0.860975, batch error rate 24.000000%\n",
      "At minibatch 12200, batch loss 0.931583, batch nll 0.594832, batch error rate 16.000000%\n",
      "At minibatch 12300, batch loss 1.298586, batch nll 0.960957, batch error rate 36.000000%\n",
      "At minibatch 12400, batch loss 1.215681, batch nll 0.877895, batch error rate 36.000000%\n",
      "At minibatch 12500, batch loss 0.692196, batch nll 0.353633, batch error rate 8.000000%\n",
      "At minibatch 12600, batch loss 0.987447, batch nll 0.648895, batch error rate 28.000000%\n",
      "At minibatch 12700, batch loss 0.742114, batch nll 0.403053, batch error rate 16.000000%\n",
      "At minibatch 12800, batch loss 0.885522, batch nll 0.546414, batch error rate 20.000000%\n",
      "After epoch 8: valid_err_rate: 32.170000% currently going to do 13 epochs\n",
      "After epoch 8: averaged train_err_rate: 20.267500% averaged train nll: 0.590142 averaged train loss: 0.923587\n",
      "At minibatch 12900, batch loss 0.949693, batch nll 0.608428, batch error rate 16.000000%\n",
      "At minibatch 13000, batch loss 0.711784, batch nll 0.368277, batch error rate 20.000000%\n",
      "At minibatch 13100, batch loss 0.693212, batch nll 0.348665, batch error rate 12.000000%\n",
      "At minibatch 13200, batch loss 0.769704, batch nll 0.423454, batch error rate 20.000000%\n",
      "At minibatch 13300, batch loss 0.676008, batch nll 0.328149, batch error rate 8.000000%\n",
      "At minibatch 13400, batch loss 0.802522, batch nll 0.453639, batch error rate 20.000000%\n",
      "At minibatch 13500, batch loss 0.518761, batch nll 0.168484, batch error rate 4.000000%\n",
      "At minibatch 13600, batch loss 0.916700, batch nll 0.566265, batch error rate 24.000000%\n",
      "At minibatch 13700, batch loss 0.729574, batch nll 0.378285, batch error rate 16.000000%\n",
      "At minibatch 13800, batch loss 0.782570, batch nll 0.430313, batch error rate 20.000000%\n",
      "At minibatch 13900, batch loss 0.800412, batch nll 0.447362, batch error rate 8.000000%\n",
      "At minibatch 14000, batch loss 0.483892, batch nll 0.130944, batch error rate 4.000000%\n",
      "At minibatch 14100, batch loss 0.651546, batch nll 0.297538, batch error rate 0.000000%\n",
      "At minibatch 14200, batch loss 0.850730, batch nll 0.495388, batch error rate 20.000000%\n",
      "At minibatch 14300, batch loss 1.063583, batch nll 0.707983, batch error rate 24.000000%\n",
      "At minibatch 14400, batch loss 1.143771, batch nll 0.787610, batch error rate 24.000000%\n",
      "After epoch 9: valid_err_rate: 31.660000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 17.722500% averaged train nll: 0.513086 averaged train loss: 0.862805\n",
      "At minibatch 14500, batch loss 0.688875, batch nll 0.331100, batch error rate 12.000000%\n",
      "At minibatch 14600, batch loss 0.688915, batch nll 0.329238, batch error rate 12.000000%\n",
      "At minibatch 14700, batch loss 0.836967, batch nll 0.475618, batch error rate 12.000000%\n",
      "At minibatch 14800, batch loss 0.700968, batch nll 0.338284, batch error rate 4.000000%\n",
      "At minibatch 14900, batch loss 0.567995, batch nll 0.204090, batch error rate 8.000000%\n",
      "At minibatch 15000, batch loss 1.068014, batch nll 0.703019, batch error rate 20.000000%\n",
      "At minibatch 15100, batch loss 0.811858, batch nll 0.445963, batch error rate 20.000000%\n",
      "At minibatch 15200, batch loss 1.236365, batch nll 0.869209, batch error rate 32.000000%\n",
      "At minibatch 15300, batch loss 0.870690, batch nll 0.502749, batch error rate 20.000000%\n",
      "At minibatch 15400, batch loss 0.757800, batch nll 0.389401, batch error rate 16.000000%\n",
      "At minibatch 15500, batch loss 0.910097, batch nll 0.541149, batch error rate 20.000000%\n",
      "At minibatch 15600, batch loss 0.707353, batch nll 0.337567, batch error rate 8.000000%\n",
      "At minibatch 15700, batch loss 0.900906, batch nll 0.530923, batch error rate 12.000000%\n",
      "At minibatch 15800, batch loss 1.050228, batch nll 0.679903, batch error rate 16.000000%\n",
      "At minibatch 15900, batch loss 0.900709, batch nll 0.529742, batch error rate 20.000000%\n",
      "At minibatch 16000, batch loss 0.930013, batch nll 0.558294, batch error rate 16.000000%\n",
      "After epoch 10: valid_err_rate: 31.710000% currently going to do 14 epochs\n",
      "After epoch 10: averaged train_err_rate: 15.105000% averaged train nll: 0.442040 averaged train loss: 0.807886\n",
      "At minibatch 16100, batch loss 0.714261, batch nll 0.341006, batch error rate 12.000000%\n",
      "At minibatch 16200, batch loss 0.646765, batch nll 0.272522, batch error rate 8.000000%\n",
      "At minibatch 16300, batch loss 0.730563, batch nll 0.355795, batch error rate 12.000000%\n",
      "At minibatch 16400, batch loss 0.741028, batch nll 0.365337, batch error rate 16.000000%\n",
      "At minibatch 16500, batch loss 0.753281, batch nll 0.376473, batch error rate 16.000000%\n",
      "At minibatch 16600, batch loss 0.574758, batch nll 0.196657, batch error rate 4.000000%\n",
      "At minibatch 16700, batch loss 0.779545, batch nll 0.400500, batch error rate 12.000000%\n",
      "At minibatch 16800, batch loss 0.923802, batch nll 0.543838, batch error rate 12.000000%\n",
      "At minibatch 16900, batch loss 0.592620, batch nll 0.211910, batch error rate 4.000000%\n",
      "At minibatch 17000, batch loss 0.721092, batch nll 0.339638, batch error rate 8.000000%\n",
      "At minibatch 17100, batch loss 0.758806, batch nll 0.376699, batch error rate 12.000000%\n",
      "At minibatch 17200, batch loss 0.698564, batch nll 0.315819, batch error rate 12.000000%\n",
      "At minibatch 17300, batch loss 0.612922, batch nll 0.229339, batch error rate 8.000000%\n",
      "At minibatch 17400, batch loss 0.871714, batch nll 0.487404, batch error rate 20.000000%\n",
      "At minibatch 17500, batch loss 0.651462, batch nll 0.266680, batch error rate 4.000000%\n",
      "At minibatch 17600, batch loss 0.666456, batch nll 0.280943, batch error rate 8.000000%\n",
      "After epoch 11: valid_err_rate: 31.460000% currently going to do 17 epochs\n",
      "After epoch 11: averaged train_err_rate: 12.442500% averaged train nll: 0.371629 averaged train loss: 0.750998\n",
      "At minibatch 17700, batch loss 0.725418, batch nll 0.339126, batch error rate 12.000000%\n",
      "At minibatch 17800, batch loss 0.621675, batch nll 0.234761, batch error rate 4.000000%\n",
      "At minibatch 17900, batch loss 0.629908, batch nll 0.242118, batch error rate 8.000000%\n",
      "At minibatch 18000, batch loss 0.562533, batch nll 0.173958, batch error rate 0.000000%\n",
      "At minibatch 18100, batch loss 0.779363, batch nll 0.390164, batch error rate 16.000000%\n",
      "At minibatch 18200, batch loss 0.729092, batch nll 0.339300, batch error rate 8.000000%\n",
      "At minibatch 18300, batch loss 0.595746, batch nll 0.205217, batch error rate 8.000000%\n",
      "At minibatch 18400, batch loss 0.706274, batch nll 0.315193, batch error rate 24.000000%\n",
      "At minibatch 18500, batch loss 0.607484, batch nll 0.215783, batch error rate 4.000000%\n",
      "At minibatch 18600, batch loss 0.737836, batch nll 0.345889, batch error rate 20.000000%\n",
      "At minibatch 18700, batch loss 0.633525, batch nll 0.240789, batch error rate 12.000000%\n",
      "At minibatch 18800, batch loss 0.944888, batch nll 0.551558, batch error rate 8.000000%\n",
      "At minibatch 18900, batch loss 0.921346, batch nll 0.527446, batch error rate 20.000000%\n",
      "At minibatch 19000, batch loss 0.630150, batch nll 0.236002, batch error rate 4.000000%\n",
      "At minibatch 19100, batch loss 0.621323, batch nll 0.226834, batch error rate 8.000000%\n",
      "At minibatch 19200, batch loss 0.662011, batch nll 0.266933, batch error rate 12.000000%\n",
      "After epoch 12: valid_err_rate: 31.330000% currently going to do 19 epochs\n",
      "After epoch 12: averaged train_err_rate: 9.822500% averaged train nll: 0.306852 averaged train loss: 0.697646\n",
      "At minibatch 19300, batch loss 0.537139, batch nll 0.141551, batch error rate 0.000000%\n",
      "At minibatch 19400, batch loss 0.548195, batch nll 0.152240, batch error rate 4.000000%\n",
      "At minibatch 19500, batch loss 0.750055, batch nll 0.353753, batch error rate 8.000000%\n",
      "At minibatch 19600, batch loss 0.566171, batch nll 0.169527, batch error rate 4.000000%\n",
      "At minibatch 19700, batch loss 0.881353, batch nll 0.484311, batch error rate 24.000000%\n",
      "At minibatch 19800, batch loss 0.609385, batch nll 0.212149, batch error rate 8.000000%\n",
      "At minibatch 19900, batch loss 0.802797, batch nll 0.405446, batch error rate 12.000000%\n",
      "At minibatch 20000, batch loss 0.668674, batch nll 0.270879, batch error rate 12.000000%\n",
      "At minibatch 20100, batch loss 0.637610, batch nll 0.239318, batch error rate 8.000000%\n",
      "At minibatch 20200, batch loss 0.660658, batch nll 0.261800, batch error rate 8.000000%\n",
      "At minibatch 20300, batch loss 0.697468, batch nll 0.298289, batch error rate 4.000000%\n",
      "At minibatch 20400, batch loss 0.618526, batch nll 0.218789, batch error rate 16.000000%\n",
      "At minibatch 20500, batch loss 0.836258, batch nll 0.436178, batch error rate 24.000000%\n",
      "At minibatch 20600, batch loss 0.690910, batch nll 0.290278, batch error rate 12.000000%\n",
      "At minibatch 20700, batch loss 0.703292, batch nll 0.301959, batch error rate 4.000000%\n",
      "At minibatch 20800, batch loss 0.697894, batch nll 0.296355, batch error rate 8.000000%\n",
      "After epoch 13: valid_err_rate: 31.540000% currently going to do 19 epochs\n",
      "After epoch 13: averaged train_err_rate: 8.087500% averaged train nll: 0.256743 averaged train loss: 0.654897\n",
      "At minibatch 20900, batch loss 0.647504, batch nll 0.245666, batch error rate 8.000000%\n",
      "At minibatch 21000, batch loss 0.600527, batch nll 0.198352, batch error rate 8.000000%\n",
      "At minibatch 21100, batch loss 0.696223, batch nll 0.293957, batch error rate 12.000000%\n",
      "At minibatch 21200, batch loss 0.571076, batch nll 0.168843, batch error rate 4.000000%\n",
      "At minibatch 21300, batch loss 0.494478, batch nll 0.092088, batch error rate 4.000000%\n",
      "At minibatch 21400, batch loss 0.632714, batch nll 0.230311, batch error rate 8.000000%\n",
      "At minibatch 21500, batch loss 0.787964, batch nll 0.385579, batch error rate 16.000000%\n",
      "At minibatch 21600, batch loss 0.590390, batch nll 0.187879, batch error rate 4.000000%\n",
      "At minibatch 21700, batch loss 0.783817, batch nll 0.381247, batch error rate 20.000000%\n",
      "At minibatch 21800, batch loss 0.531414, batch nll 0.128523, batch error rate 4.000000%\n",
      "At minibatch 21900, batch loss 0.711574, batch nll 0.308447, batch error rate 8.000000%\n",
      "At minibatch 22000, batch loss 0.705072, batch nll 0.301609, batch error rate 8.000000%\n",
      "At minibatch 22100, batch loss 0.662132, batch nll 0.258273, batch error rate 4.000000%\n",
      "At minibatch 22200, batch loss 0.639927, batch nll 0.236020, batch error rate 4.000000%\n",
      "At minibatch 22300, batch loss 0.517319, batch nll 0.113000, batch error rate 4.000000%\n",
      "At minibatch 22400, batch loss 0.551551, batch nll 0.146825, batch error rate 0.000000%\n",
      "After epoch 14: valid_err_rate: 30.910000% currently going to do 22 epochs\n",
      "After epoch 14: averaged train_err_rate: 6.057500% averaged train nll: 0.209208 averaged train loss: 0.612050\n",
      "At minibatch 22500, batch loss 0.531738, batch nll 0.126973, batch error rate 0.000000%\n",
      "At minibatch 22600, batch loss 0.487736, batch nll 0.083219, batch error rate 0.000000%\n",
      "At minibatch 22700, batch loss 0.507577, batch nll 0.103286, batch error rate 0.000000%\n",
      "At minibatch 22800, batch loss 0.467851, batch nll 0.063462, batch error rate 0.000000%\n",
      "At minibatch 22900, batch loss 0.563663, batch nll 0.159296, batch error rate 0.000000%\n",
      "At minibatch 23000, batch loss 0.561373, batch nll 0.156922, batch error rate 4.000000%\n",
      "At minibatch 23100, batch loss 0.597851, batch nll 0.193446, batch error rate 8.000000%\n",
      "At minibatch 23200, batch loss 0.518036, batch nll 0.113748, batch error rate 4.000000%\n",
      "At minibatch 23300, batch loss 0.529247, batch nll 0.124852, batch error rate 4.000000%\n",
      "At minibatch 23400, batch loss 0.688961, batch nll 0.284551, batch error rate 12.000000%\n",
      "At minibatch 23500, batch loss 0.658353, batch nll 0.253799, batch error rate 16.000000%\n",
      "At minibatch 23600, batch loss 0.541151, batch nll 0.136364, batch error rate 0.000000%\n",
      "At minibatch 23700, batch loss 0.521053, batch nll 0.115845, batch error rate 0.000000%\n",
      "At minibatch 23800, batch loss 0.666549, batch nll 0.261091, batch error rate 12.000000%\n",
      "At minibatch 23900, batch loss 0.504987, batch nll 0.099280, batch error rate 0.000000%\n",
      "At minibatch 24000, batch loss 0.462411, batch nll 0.056682, batch error rate 0.000000%\n",
      "After epoch 15: valid_err_rate: 31.070000% currently going to do 22 epochs\n",
      "After epoch 15: averaged train_err_rate: 4.935000% averaged train nll: 0.175349 averaged train loss: 0.580068\n",
      "At minibatch 24100, batch loss 0.708220, batch nll 0.302698, batch error rate 12.000000%\n",
      "At minibatch 24200, batch loss 0.476606, batch nll 0.071417, batch error rate 0.000000%\n",
      "At minibatch 24300, batch loss 0.478077, batch nll 0.073409, batch error rate 0.000000%\n",
      "At minibatch 24400, batch loss 0.488660, batch nll 0.084522, batch error rate 0.000000%\n",
      "At minibatch 24500, batch loss 0.525240, batch nll 0.121412, batch error rate 4.000000%\n",
      "At minibatch 24600, batch loss 0.482943, batch nll 0.079620, batch error rate 0.000000%\n",
      "At minibatch 24700, batch loss 0.670988, batch nll 0.267957, batch error rate 4.000000%\n",
      "At minibatch 24800, batch loss 0.486629, batch nll 0.083857, batch error rate 4.000000%\n",
      "At minibatch 24900, batch loss 0.461754, batch nll 0.059169, batch error rate 0.000000%\n",
      "At minibatch 25000, batch loss 0.582567, batch nll 0.180194, batch error rate 8.000000%\n",
      "At minibatch 25100, batch loss 0.556123, batch nll 0.153944, batch error rate 4.000000%\n",
      "At minibatch 25200, batch loss 0.485646, batch nll 0.083397, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.578811, batch nll 0.176545, batch error rate 8.000000%\n",
      "At minibatch 25400, batch loss 0.636109, batch nll 0.233887, batch error rate 4.000000%\n",
      "At minibatch 25500, batch loss 0.591608, batch nll 0.189286, batch error rate 4.000000%\n",
      "At minibatch 25600, batch loss 0.565518, batch nll 0.163280, batch error rate 4.000000%\n",
      "After epoch 16: valid_err_rate: 30.740000% currently going to do 25 epochs\n",
      "After epoch 16: averaged train_err_rate: 3.295000% averaged train nll: 0.139596 averaged train loss: 0.542886\n",
      "At minibatch 25700, batch loss 0.472846, batch nll 0.070989, batch error rate 0.000000%\n",
      "At minibatch 25800, batch loss 0.529359, batch nll 0.128142, batch error rate 8.000000%\n",
      "At minibatch 25900, batch loss 0.453083, batch nll 0.052506, batch error rate 0.000000%\n",
      "At minibatch 26000, batch loss 0.570539, batch nll 0.170363, batch error rate 8.000000%\n",
      "At minibatch 26100, batch loss 0.520825, batch nll 0.121065, batch error rate 4.000000%\n",
      "At minibatch 26200, batch loss 0.497783, batch nll 0.098274, batch error rate 4.000000%\n",
      "At minibatch 26300, batch loss 0.479447, batch nll 0.080455, batch error rate 0.000000%\n",
      "At minibatch 26400, batch loss 0.517042, batch nll 0.118406, batch error rate 4.000000%\n",
      "At minibatch 26500, batch loss 0.548482, batch nll 0.150279, batch error rate 4.000000%\n",
      "At minibatch 26600, batch loss 0.525101, batch nll 0.127299, batch error rate 0.000000%\n",
      "At minibatch 26700, batch loss 0.540338, batch nll 0.142485, batch error rate 4.000000%\n",
      "At minibatch 26800, batch loss 0.427918, batch nll 0.029943, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.598548, batch nll 0.200695, batch error rate 4.000000%\n",
      "At minibatch 27000, batch loss 0.575004, batch nll 0.177256, batch error rate 8.000000%\n",
      "At minibatch 27100, batch loss 0.425240, batch nll 0.027376, batch error rate 0.000000%\n",
      "At minibatch 27200, batch loss 0.500252, batch nll 0.102567, batch error rate 0.000000%\n",
      "After epoch 17: valid_err_rate: 30.900000% currently going to do 25 epochs\n",
      "After epoch 17: averaged train_err_rate: 2.432500% averaged train nll: 0.117475 averaged train loss: 0.516594\n",
      "At minibatch 27300, batch loss 0.441205, batch nll 0.044195, batch error rate 0.000000%\n",
      "At minibatch 27400, batch loss 0.426977, batch nll 0.030804, batch error rate 0.000000%\n",
      "At minibatch 27500, batch loss 0.473071, batch nll 0.077608, batch error rate 0.000000%\n",
      "At minibatch 27600, batch loss 0.447421, batch nll 0.052672, batch error rate 0.000000%\n",
      "At minibatch 27700, batch loss 0.520137, batch nll 0.125837, batch error rate 4.000000%\n",
      "At minibatch 27800, batch loss 0.437795, batch nll 0.043969, batch error rate 0.000000%\n",
      "At minibatch 27900, batch loss 0.501686, batch nll 0.108344, batch error rate 0.000000%\n",
      "At minibatch 28000, batch loss 0.439955, batch nll 0.047194, batch error rate 0.000000%\n",
      "At minibatch 28100, batch loss 0.434617, batch nll 0.042173, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.505980, batch nll 0.113875, batch error rate 0.000000%\n",
      "At minibatch 28300, batch loss 0.550068, batch nll 0.158280, batch error rate 8.000000%\n",
      "At minibatch 28400, batch loss 0.449818, batch nll 0.058349, batch error rate 0.000000%\n",
      "At minibatch 28500, batch loss 0.604941, batch nll 0.213753, batch error rate 8.000000%\n",
      "At minibatch 28600, batch loss 0.434467, batch nll 0.043386, batch error rate 0.000000%\n",
      "At minibatch 28700, batch loss 0.460658, batch nll 0.070021, batch error rate 0.000000%\n",
      "At minibatch 28800, batch loss 0.474764, batch nll 0.084264, batch error rate 0.000000%\n",
      "After epoch 18: valid_err_rate: 30.570000% currently going to do 28 epochs\n",
      "After epoch 18: averaged train_err_rate: 1.627500% averaged train nll: 0.096322 averaged train loss: 0.489602\n",
      "At minibatch 28900, batch loss 0.462188, batch nll 0.072398, batch error rate 4.000000%\n",
      "At minibatch 29000, batch loss 0.435098, batch nll 0.046036, batch error rate 0.000000%\n",
      "At minibatch 29100, batch loss 0.437725, batch nll 0.049436, batch error rate 0.000000%\n",
      "At minibatch 29200, batch loss 0.461326, batch nll 0.073534, batch error rate 0.000000%\n",
      "At minibatch 29300, batch loss 0.471592, batch nll 0.084497, batch error rate 0.000000%\n",
      "At minibatch 29400, batch loss 0.463450, batch nll 0.076972, batch error rate 0.000000%\n",
      "At minibatch 29500, batch loss 0.452205, batch nll 0.066317, batch error rate 0.000000%\n",
      "At minibatch 29600, batch loss 0.565124, batch nll 0.179879, batch error rate 4.000000%\n",
      "At minibatch 29700, batch loss 0.505753, batch nll 0.121074, batch error rate 0.000000%\n",
      "At minibatch 29800, batch loss 0.515933, batch nll 0.131676, batch error rate 0.000000%\n",
      "At minibatch 29900, batch loss 0.425584, batch nll 0.041748, batch error rate 0.000000%\n",
      "At minibatch 30000, batch loss 0.488196, batch nll 0.104842, batch error rate 4.000000%\n",
      "At minibatch 30100, batch loss 0.481333, batch nll 0.098339, batch error rate 4.000000%\n",
      "At minibatch 30200, batch loss 0.523188, batch nll 0.140504, batch error rate 4.000000%\n",
      "At minibatch 30300, batch loss 0.469572, batch nll 0.086981, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.522083, batch nll 0.139514, batch error rate 4.000000%\n",
      "After epoch 19: valid_err_rate: 31.240000% currently going to do 28 epochs\n",
      "After epoch 19: averaged train_err_rate: 1.110000% averaged train nll: 0.082129 averaged train loss: 0.467786\n",
      "At minibatch 30500, batch loss 0.446025, batch nll 0.064063, batch error rate 0.000000%\n",
      "At minibatch 30600, batch loss 0.443987, batch nll 0.062748, batch error rate 0.000000%\n",
      "At minibatch 30700, batch loss 0.433050, batch nll 0.052662, batch error rate 0.000000%\n",
      "At minibatch 30800, batch loss 0.463339, batch nll 0.083620, batch error rate 0.000000%\n",
      "At minibatch 30900, batch loss 0.474575, batch nll 0.095525, batch error rate 0.000000%\n",
      "At minibatch 31000, batch loss 0.425901, batch nll 0.047467, batch error rate 0.000000%\n",
      "At minibatch 31100, batch loss 0.519866, batch nll 0.142024, batch error rate 4.000000%\n",
      "At minibatch 31200, batch loss 0.440004, batch nll 0.062540, batch error rate 0.000000%\n",
      "At minibatch 31300, batch loss 0.437784, batch nll 0.060721, batch error rate 0.000000%\n",
      "At minibatch 31400, batch loss 0.460139, batch nll 0.083540, batch error rate 0.000000%\n",
      "At minibatch 31500, batch loss 0.472901, batch nll 0.096774, batch error rate 4.000000%\n",
      "At minibatch 31600, batch loss 0.537795, batch nll 0.161988, batch error rate 4.000000%\n",
      "At minibatch 31700, batch loss 0.413793, batch nll 0.038413, batch error rate 0.000000%\n",
      "At minibatch 31800, batch loss 0.429720, batch nll 0.054671, batch error rate 0.000000%\n",
      "At minibatch 31900, batch loss 0.449929, batch nll 0.075246, batch error rate 0.000000%\n",
      "At minibatch 32000, batch loss 0.430068, batch nll 0.055474, batch error rate 0.000000%\n",
      "After epoch 20: valid_err_rate: 30.730000% currently going to do 28 epochs\n",
      "After epoch 20: averaged train_err_rate: 0.780000% averaged train nll: 0.072877 averaged train loss: 0.450712\n",
      "At minibatch 32100, batch loss 0.429259, batch nll 0.055269, batch error rate 0.000000%\n",
      "At minibatch 32200, batch loss 0.438959, batch nll 0.065750, batch error rate 0.000000%\n",
      "At minibatch 32300, batch loss 0.475850, batch nll 0.103333, batch error rate 4.000000%\n",
      "At minibatch 32400, batch loss 0.436368, batch nll 0.064567, batch error rate 0.000000%\n",
      "At minibatch 32500, batch loss 0.404156, batch nll 0.033015, batch error rate 0.000000%\n",
      "At minibatch 32600, batch loss 0.413288, batch nll 0.042711, batch error rate 0.000000%\n",
      "At minibatch 32700, batch loss 0.443592, batch nll 0.073508, batch error rate 0.000000%\n",
      "At minibatch 32800, batch loss 0.472180, batch nll 0.102520, batch error rate 4.000000%\n",
      "At minibatch 32900, batch loss 0.425410, batch nll 0.056262, batch error rate 0.000000%\n",
      "At minibatch 33000, batch loss 0.451021, batch nll 0.082297, batch error rate 0.000000%\n",
      "At minibatch 33100, batch loss 0.467359, batch nll 0.098904, batch error rate 0.000000%\n",
      "At minibatch 33200, batch loss 0.409417, batch nll 0.041337, batch error rate 0.000000%\n",
      "At minibatch 33300, batch loss 0.437764, batch nll 0.070002, batch error rate 0.000000%\n",
      "At minibatch 33400, batch loss 0.493655, batch nll 0.126119, batch error rate 4.000000%\n",
      "At minibatch 33500, batch loss 0.427613, batch nll 0.060343, batch error rate 0.000000%\n",
      "At minibatch 33600, batch loss 0.499012, batch nll 0.131919, batch error rate 4.000000%\n",
      "After epoch 21: valid_err_rate: 30.670000% currently going to do 28 epochs\n",
      "After epoch 21: averaged train_err_rate: 0.605000% averaged train nll: 0.067769 averaged train loss: 0.437817\n",
      "At minibatch 33700, batch loss 0.443581, batch nll 0.077057, batch error rate 0.000000%\n",
      "At minibatch 33800, batch loss 0.403298, batch nll 0.037420, batch error rate 0.000000%\n",
      "At minibatch 33900, batch loss 0.446998, batch nll 0.081807, batch error rate 0.000000%\n",
      "At minibatch 34000, batch loss 0.441491, batch nll 0.076972, batch error rate 0.000000%\n",
      "At minibatch 34100, batch loss 0.404078, batch nll 0.040107, batch error rate 0.000000%\n",
      "At minibatch 34200, batch loss 0.443946, batch nll 0.080607, batch error rate 0.000000%\n",
      "At minibatch 34300, batch loss 0.414010, batch nll 0.051055, batch error rate 0.000000%\n",
      "At minibatch 34400, batch loss 0.402164, batch nll 0.039727, batch error rate 0.000000%\n",
      "At minibatch 34500, batch loss 0.447870, batch nll 0.085842, batch error rate 0.000000%\n",
      "At minibatch 34600, batch loss 0.436791, batch nll 0.075147, batch error rate 0.000000%\n",
      "At minibatch 34700, batch loss 0.402487, batch nll 0.041129, batch error rate 0.000000%\n",
      "At minibatch 34800, batch loss 0.410632, batch nll 0.049520, batch error rate 0.000000%\n",
      "At minibatch 34900, batch loss 0.409181, batch nll 0.048454, batch error rate 0.000000%\n",
      "At minibatch 35000, batch loss 0.434042, batch nll 0.073689, batch error rate 0.000000%\n",
      "At minibatch 35100, batch loss 0.425708, batch nll 0.065570, batch error rate 0.000000%\n",
      "At minibatch 35200, batch loss 0.438902, batch nll 0.078985, batch error rate 0.000000%\n",
      "After epoch 22: valid_err_rate: 30.250000% currently going to do 34 epochs\n",
      "After epoch 22: averaged train_err_rate: 0.405000% averaged train nll: 0.062108 averaged train loss: 0.424959\n",
      "At minibatch 35300, batch loss 0.410965, batch nll 0.051633, batch error rate 0.000000%\n",
      "At minibatch 35400, batch loss 0.380804, batch nll 0.022176, batch error rate 0.000000%\n",
      "At minibatch 35500, batch loss 0.405231, batch nll 0.047267, batch error rate 0.000000%\n",
      "At minibatch 35600, batch loss 0.410927, batch nll 0.053585, batch error rate 0.000000%\n",
      "At minibatch 35700, batch loss 0.389808, batch nll 0.032993, batch error rate 0.000000%\n",
      "At minibatch 35800, batch loss 0.400190, batch nll 0.043925, batch error rate 0.000000%\n",
      "At minibatch 35900, batch loss 0.378080, batch nll 0.022296, batch error rate 0.000000%\n",
      "At minibatch 36000, batch loss 0.426295, batch nll 0.070976, batch error rate 0.000000%\n",
      "At minibatch 36100, batch loss 0.417194, batch nll 0.062245, batch error rate 0.000000%\n",
      "At minibatch 36200, batch loss 0.411146, batch nll 0.056565, batch error rate 0.000000%\n",
      "At minibatch 36300, batch loss 0.407954, batch nll 0.053742, batch error rate 4.000000%\n",
      "At minibatch 36400, batch loss 0.420652, batch nll 0.066765, batch error rate 0.000000%\n",
      "At minibatch 36500, batch loss 0.408227, batch nll 0.054628, batch error rate 0.000000%\n",
      "At minibatch 36600, batch loss 0.463655, batch nll 0.110311, batch error rate 4.000000%\n",
      "At minibatch 36700, batch loss 0.392132, batch nll 0.038834, batch error rate 0.000000%\n",
      "At minibatch 36800, batch loss 0.388451, batch nll 0.035269, batch error rate 0.000000%\n",
      "After epoch 23: valid_err_rate: 30.370000% currently going to do 34 epochs\n",
      "After epoch 23: averaged train_err_rate: 0.260000% averaged train nll: 0.057895 averaged train loss: 0.413634\n",
      "At minibatch 36900, batch loss 0.386680, batch nll 0.034015, batch error rate 0.000000%\n",
      "At minibatch 37000, batch loss 0.388186, batch nll 0.036090, batch error rate 0.000000%\n",
      "At minibatch 37100, batch loss 0.377874, batch nll 0.026375, batch error rate 0.000000%\n",
      "At minibatch 37200, batch loss 0.428932, batch nll 0.077957, batch error rate 0.000000%\n",
      "At minibatch 37300, batch loss 0.394878, batch nll 0.044429, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.421586, batch nll 0.071624, batch error rate 0.000000%\n",
      "At minibatch 37500, batch loss 0.452222, batch nll 0.102605, batch error rate 0.000000%\n",
      "At minibatch 37600, batch loss 0.415820, batch nll 0.066627, batch error rate 0.000000%\n",
      "At minibatch 37700, batch loss 0.380681, batch nll 0.031843, batch error rate 0.000000%\n",
      "At minibatch 37800, batch loss 0.399925, batch nll 0.051379, batch error rate 0.000000%\n",
      "At minibatch 37900, batch loss 0.407814, batch nll 0.059460, batch error rate 0.000000%\n",
      "At minibatch 38000, batch loss 0.375513, batch nll 0.027412, batch error rate 0.000000%\n",
      "At minibatch 38100, batch loss 0.402417, batch nll 0.054578, batch error rate 0.000000%\n",
      "At minibatch 38200, batch loss 0.405696, batch nll 0.058085, batch error rate 0.000000%\n",
      "At minibatch 38300, batch loss 0.413794, batch nll 0.066343, batch error rate 0.000000%\n",
      "At minibatch 38400, batch loss 0.393381, batch nll 0.046062, batch error rate 0.000000%\n",
      "After epoch 24: valid_err_rate: 30.540000% currently going to do 34 epochs\n",
      "After epoch 24: averaged train_err_rate: 0.262500% averaged train nll: 0.056734 averaged train loss: 0.406324\n",
      "At minibatch 38500, batch loss 0.393934, batch nll 0.047057, batch error rate 0.000000%\n",
      "At minibatch 38600, batch loss 0.381214, batch nll 0.034834, batch error rate 0.000000%\n",
      "At minibatch 38700, batch loss 0.398346, batch nll 0.052372, batch error rate 0.000000%\n",
      "At minibatch 38800, batch loss 0.387832, batch nll 0.042363, batch error rate 0.000000%\n",
      "At minibatch 38900, batch loss 0.425341, batch nll 0.080353, batch error rate 0.000000%\n",
      "At minibatch 39000, batch loss 0.407469, batch nll 0.062915, batch error rate 0.000000%\n",
      "At minibatch 39100, batch loss 0.385594, batch nll 0.041393, batch error rate 0.000000%\n",
      "At minibatch 39200, batch loss 0.382658, batch nll 0.038888, batch error rate 0.000000%\n",
      "At minibatch 39300, batch loss 0.430977, batch nll 0.087551, batch error rate 4.000000%\n",
      "At minibatch 39400, batch loss 0.387503, batch nll 0.044405, batch error rate 0.000000%\n",
      "At minibatch 39500, batch loss 0.409512, batch nll 0.066653, batch error rate 0.000000%\n",
      "At minibatch 39600, batch loss 0.382967, batch nll 0.040323, batch error rate 0.000000%\n",
      "At minibatch 39700, batch loss 0.413932, batch nll 0.071506, batch error rate 0.000000%\n",
      "At minibatch 39800, batch loss 0.377333, batch nll 0.035072, batch error rate 0.000000%\n",
      "At minibatch 39900, batch loss 0.410273, batch nll 0.068262, batch error rate 0.000000%\n",
      "At minibatch 40000, batch loss 0.401339, batch nll 0.059382, batch error rate 0.000000%\n",
      "After epoch 25: valid_err_rate: 30.760000% currently going to do 34 epochs\n",
      "After epoch 25: averaged train_err_rate: 0.195000% averaged train nll: 0.054746 averaged train loss: 0.398843\n",
      "At minibatch 40100, batch loss 0.389159, batch nll 0.047568, batch error rate 0.000000%\n",
      "At minibatch 40200, batch loss 0.424512, batch nll 0.083477, batch error rate 0.000000%\n",
      "At minibatch 40300, batch loss 0.362781, batch nll 0.022143, batch error rate 0.000000%\n",
      "At minibatch 40400, batch loss 0.363568, batch nll 0.023387, batch error rate 0.000000%\n",
      "At minibatch 40500, batch loss 0.394028, batch nll 0.054281, batch error rate 0.000000%\n",
      "At minibatch 40600, batch loss 0.366319, batch nll 0.027015, batch error rate 0.000000%\n",
      "At minibatch 40700, batch loss 0.455993, batch nll 0.117095, batch error rate 4.000000%\n",
      "At minibatch 40800, batch loss 0.442573, batch nll 0.103998, batch error rate 0.000000%\n",
      "At minibatch 40900, batch loss 0.384019, batch nll 0.045791, batch error rate 0.000000%\n",
      "At minibatch 41000, batch loss 0.396524, batch nll 0.058521, batch error rate 0.000000%\n",
      "At minibatch 41100, batch loss 0.399624, batch nll 0.061849, batch error rate 0.000000%\n",
      "At minibatch 41200, batch loss 0.371968, batch nll 0.034451, batch error rate 0.000000%\n",
      "At minibatch 41300, batch loss 0.373574, batch nll 0.036272, batch error rate 0.000000%\n",
      "At minibatch 41400, batch loss 0.449500, batch nll 0.112425, batch error rate 4.000000%\n",
      "At minibatch 41500, batch loss 0.408384, batch nll 0.071442, batch error rate 0.000000%\n",
      "At minibatch 41600, batch loss 0.372686, batch nll 0.035841, batch error rate 0.000000%\n",
      "After epoch 26: valid_err_rate: 29.950000% currently going to do 40 epochs\n",
      "After epoch 26: averaged train_err_rate: 0.127500% averaged train nll: 0.052253 averaged train loss: 0.391143\n",
      "At minibatch 41700, batch loss 0.377131, batch nll 0.040720, batch error rate 0.000000%\n",
      "At minibatch 41800, batch loss 0.405322, batch nll 0.069443, batch error rate 0.000000%\n",
      "At minibatch 41900, batch loss 0.394601, batch nll 0.059102, batch error rate 0.000000%\n",
      "At minibatch 42000, batch loss 0.372606, batch nll 0.037537, batch error rate 0.000000%\n",
      "At minibatch 42100, batch loss 0.394547, batch nll 0.059934, batch error rate 0.000000%\n",
      "At minibatch 42200, batch loss 0.363978, batch nll 0.029695, batch error rate 0.000000%\n",
      "At minibatch 42300, batch loss 0.362143, batch nll 0.028138, batch error rate 0.000000%\n",
      "At minibatch 42400, batch loss 0.394868, batch nll 0.061176, batch error rate 0.000000%\n",
      "At minibatch 42500, batch loss 0.372758, batch nll 0.039319, batch error rate 0.000000%\n",
      "At minibatch 42600, batch loss 0.387419, batch nll 0.054283, batch error rate 0.000000%\n",
      "At minibatch 42700, batch loss 0.416704, batch nll 0.083702, batch error rate 0.000000%\n",
      "At minibatch 42800, batch loss 0.381854, batch nll 0.049089, batch error rate 0.000000%\n",
      "At minibatch 42900, batch loss 0.454897, batch nll 0.122316, batch error rate 4.000000%\n",
      "At minibatch 43000, batch loss 0.375746, batch nll 0.043301, batch error rate 0.000000%\n",
      "At minibatch 43100, batch loss 0.427387, batch nll 0.095043, batch error rate 0.000000%\n",
      "At minibatch 43200, batch loss 0.401101, batch nll 0.068860, batch error rate 0.000000%\n",
      "After epoch 27: valid_err_rate: 30.480000% currently going to do 40 epochs\n",
      "After epoch 27: averaged train_err_rate: 0.120000% averaged train nll: 0.051227 averaged train loss: 0.385207\n",
      "At minibatch 43300, batch loss 0.370987, batch nll 0.039146, batch error rate 0.000000%\n",
      "At minibatch 43400, batch loss 0.357520, batch nll 0.026071, batch error rate 0.000000%\n",
      "At minibatch 43500, batch loss 0.375604, batch nll 0.044570, batch error rate 0.000000%\n",
      "At minibatch 43600, batch loss 0.390696, batch nll 0.059966, batch error rate 0.000000%\n",
      "At minibatch 43700, batch loss 0.365271, batch nll 0.034879, batch error rate 0.000000%\n",
      "At minibatch 43800, batch loss 0.358550, batch nll 0.028430, batch error rate 0.000000%\n",
      "At minibatch 43900, batch loss 0.375063, batch nll 0.045241, batch error rate 0.000000%\n",
      "At minibatch 44000, batch loss 0.388395, batch nll 0.058782, batch error rate 0.000000%\n",
      "At minibatch 44100, batch loss 0.380868, batch nll 0.051504, batch error rate 0.000000%\n",
      "At minibatch 44200, batch loss 0.357333, batch nll 0.028225, batch error rate 0.000000%\n",
      "At minibatch 44300, batch loss 0.374774, batch nll 0.045855, batch error rate 0.000000%\n",
      "At minibatch 44400, batch loss 0.366609, batch nll 0.037922, batch error rate 0.000000%\n",
      "At minibatch 44500, batch loss 0.427919, batch nll 0.099375, batch error rate 0.000000%\n",
      "At minibatch 44600, batch loss 0.416015, batch nll 0.087586, batch error rate 0.000000%\n",
      "At minibatch 44700, batch loss 0.403129, batch nll 0.074738, batch error rate 0.000000%\n",
      "At minibatch 44800, batch loss 0.395898, batch nll 0.067532, batch error rate 0.000000%\n",
      "After epoch 28: valid_err_rate: 30.210000% currently going to do 40 epochs\n",
      "After epoch 28: averaged train_err_rate: 0.112500% averaged train nll: 0.051535 averaged train loss: 0.381327\n",
      "At minibatch 44900, batch loss 0.358989, batch nll 0.030944, batch error rate 0.000000%\n",
      "At minibatch 45000, batch loss 0.372727, batch nll 0.045063, batch error rate 0.000000%\n",
      "At minibatch 45100, batch loss 0.380036, batch nll 0.052687, batch error rate 0.000000%\n",
      "At minibatch 45200, batch loss 0.394462, batch nll 0.067424, batch error rate 0.000000%\n",
      "At minibatch 45300, batch loss 0.386054, batch nll 0.059360, batch error rate 0.000000%\n",
      "At minibatch 45400, batch loss 0.386453, batch nll 0.060073, batch error rate 0.000000%\n",
      "At minibatch 45500, batch loss 0.407552, batch nll 0.081444, batch error rate 0.000000%\n",
      "At minibatch 45600, batch loss 0.364588, batch nll 0.038771, batch error rate 0.000000%\n",
      "At minibatch 45700, batch loss 0.365014, batch nll 0.039436, batch error rate 0.000000%\n",
      "At minibatch 45800, batch loss 0.360919, batch nll 0.035434, batch error rate 0.000000%\n",
      "At minibatch 45900, batch loss 0.389939, batch nll 0.064650, batch error rate 0.000000%\n",
      "At minibatch 46000, batch loss 0.367427, batch nll 0.042331, batch error rate 0.000000%\n",
      "At minibatch 46100, batch loss 0.416318, batch nll 0.091352, batch error rate 4.000000%\n",
      "At minibatch 46200, batch loss 0.380128, batch nll 0.055306, batch error rate 0.000000%\n",
      "At minibatch 46300, batch loss 0.387447, batch nll 0.062738, batch error rate 0.000000%\n",
      "At minibatch 46400, batch loss 0.443535, batch nll 0.118890, batch error rate 4.000000%\n",
      "After epoch 29: valid_err_rate: 30.610000% currently going to do 40 epochs\n",
      "After epoch 29: averaged train_err_rate: 0.125000% averaged train nll: 0.050288 averaged train loss: 0.376383\n",
      "At minibatch 46500, batch loss 0.368824, batch nll 0.044451, batch error rate 0.000000%\n",
      "At minibatch 46600, batch loss 0.378812, batch nll 0.054768, batch error rate 0.000000%\n",
      "At minibatch 46700, batch loss 0.374168, batch nll 0.050467, batch error rate 0.000000%\n",
      "At minibatch 46800, batch loss 0.343149, batch nll 0.019761, batch error rate 0.000000%\n",
      "At minibatch 46900, batch loss 0.365473, batch nll 0.042412, batch error rate 0.000000%\n",
      "At minibatch 47000, batch loss 0.370631, batch nll 0.047779, batch error rate 0.000000%\n",
      "At minibatch 47100, batch loss 0.367485, batch nll 0.044861, batch error rate 0.000000%\n",
      "At minibatch 47200, batch loss 0.406764, batch nll 0.084431, batch error rate 0.000000%\n",
      "At minibatch 47300, batch loss 0.365112, batch nll 0.043001, batch error rate 0.000000%\n",
      "At minibatch 47400, batch loss 0.382985, batch nll 0.061054, batch error rate 0.000000%\n",
      "At minibatch 47500, batch loss 0.374628, batch nll 0.052865, batch error rate 0.000000%\n",
      "At minibatch 47600, batch loss 0.365005, batch nll 0.043366, batch error rate 0.000000%\n",
      "At minibatch 47700, batch loss 0.402789, batch nll 0.081215, batch error rate 4.000000%\n",
      "At minibatch 47800, batch loss 0.386697, batch nll 0.065220, batch error rate 0.000000%\n",
      "At minibatch 47900, batch loss 0.391553, batch nll 0.070139, batch error rate 0.000000%\n",
      "At minibatch 48000, batch loss 0.349660, batch nll 0.028396, batch error rate 0.000000%\n",
      "After epoch 30: valid_err_rate: 30.160000% currently going to do 40 epochs\n",
      "After epoch 30: averaged train_err_rate: 0.080000% averaged train nll: 0.049471 averaged train loss: 0.372049\n",
      "At minibatch 48100, batch loss 0.366975, batch nll 0.046021, batch error rate 0.000000%\n",
      "At minibatch 48200, batch loss 0.338333, batch nll 0.017706, batch error rate 0.000000%\n",
      "At minibatch 48300, batch loss 0.360573, batch nll 0.040287, batch error rate 0.000000%\n",
      "At minibatch 48400, batch loss 0.342620, batch nll 0.022628, batch error rate 0.000000%\n",
      "At minibatch 48500, batch loss 0.341660, batch nll 0.021932, batch error rate 0.000000%\n",
      "At minibatch 48600, batch loss 0.339996, batch nll 0.020497, batch error rate 0.000000%\n",
      "At minibatch 48700, batch loss 0.395341, batch nll 0.076065, batch error rate 0.000000%\n",
      "At minibatch 48800, batch loss 0.365141, batch nll 0.046095, batch error rate 0.000000%\n",
      "At minibatch 48900, batch loss 0.369145, batch nll 0.050324, batch error rate 0.000000%\n",
      "At minibatch 49000, batch loss 0.382208, batch nll 0.063562, batch error rate 0.000000%\n",
      "At minibatch 49100, batch loss 0.344782, batch nll 0.026325, batch error rate 0.000000%\n",
      "At minibatch 49200, batch loss 0.366012, batch nll 0.047663, batch error rate 0.000000%\n",
      "At minibatch 49300, batch loss 0.372533, batch nll 0.054273, batch error rate 0.000000%\n",
      "At minibatch 49400, batch loss 0.356369, batch nll 0.038273, batch error rate 0.000000%\n",
      "At minibatch 49500, batch loss 0.412299, batch nll 0.094264, batch error rate 0.000000%\n",
      "At minibatch 49600, batch loss 0.370769, batch nll 0.052791, batch error rate 0.000000%\n",
      "After epoch 31: valid_err_rate: 30.020000% currently going to do 40 epochs\n",
      "After epoch 31: averaged train_err_rate: 0.075000% averaged train nll: 0.048095 averaged train loss: 0.367324\n",
      "At minibatch 49700, batch loss 0.348325, batch nll 0.030655, batch error rate 0.000000%\n",
      "At minibatch 49800, batch loss 0.381670, batch nll 0.064313, batch error rate 0.000000%\n",
      "At minibatch 49900, batch loss 0.350197, batch nll 0.033147, batch error rate 0.000000%\n",
      "At minibatch 50000, batch loss 0.351031, batch nll 0.034296, batch error rate 0.000000%\n",
      "At minibatch 50100, batch loss 0.362478, batch nll 0.045976, batch error rate 0.000000%\n",
      "At minibatch 50200, batch loss 0.365588, batch nll 0.049394, batch error rate 0.000000%\n",
      "At minibatch 50300, batch loss 0.376764, batch nll 0.060781, batch error rate 0.000000%\n",
      "At minibatch 50400, batch loss 0.374993, batch nll 0.059215, batch error rate 0.000000%\n",
      "At minibatch 50500, batch loss 0.353037, batch nll 0.037425, batch error rate 0.000000%\n",
      "At minibatch 50600, batch loss 0.359161, batch nll 0.043670, batch error rate 0.000000%\n",
      "At minibatch 50700, batch loss 0.396517, batch nll 0.081174, batch error rate 0.000000%\n",
      "At minibatch 50800, batch loss 0.401409, batch nll 0.086151, batch error rate 0.000000%\n",
      "At minibatch 50900, batch loss 0.382188, batch nll 0.067052, batch error rate 0.000000%\n",
      "At minibatch 51000, batch loss 0.357869, batch nll 0.042790, batch error rate 0.000000%\n",
      "At minibatch 51100, batch loss 0.362204, batch nll 0.047214, batch error rate 0.000000%\n",
      "At minibatch 51200, batch loss 0.364356, batch nll 0.049410, batch error rate 0.000000%\n",
      "After epoch 32: valid_err_rate: 29.950000% currently going to do 40 epochs\n",
      "After epoch 32: averaged train_err_rate: 0.090000% averaged train nll: 0.047548 averaged train loss: 0.363589\n",
      "At minibatch 51300, batch loss 0.363084, batch nll 0.048397, batch error rate 0.000000%\n",
      "At minibatch 51400, batch loss 0.372233, batch nll 0.057854, batch error rate 0.000000%\n",
      "At minibatch 51500, batch loss 0.346947, batch nll 0.032883, batch error rate 0.000000%\n",
      "At minibatch 51600, batch loss 0.340028, batch nll 0.026229, batch error rate 0.000000%\n",
      "At minibatch 51700, batch loss 0.372617, batch nll 0.059040, batch error rate 0.000000%\n",
      "At minibatch 51800, batch loss 0.357110, batch nll 0.043784, batch error rate 0.000000%\n",
      "At minibatch 51900, batch loss 0.356829, batch nll 0.043743, batch error rate 0.000000%\n",
      "At minibatch 52000, batch loss 0.346745, batch nll 0.033865, batch error rate 0.000000%\n",
      "At minibatch 52100, batch loss 0.350342, batch nll 0.037604, batch error rate 0.000000%\n",
      "At minibatch 52200, batch loss 0.346644, batch nll 0.034046, batch error rate 0.000000%\n",
      "At minibatch 52300, batch loss 0.380463, batch nll 0.068016, batch error rate 0.000000%\n",
      "At minibatch 52400, batch loss 0.349083, batch nll 0.036742, batch error rate 0.000000%\n",
      "At minibatch 52500, batch loss 0.365709, batch nll 0.053469, batch error rate 0.000000%\n",
      "At minibatch 52600, batch loss 0.377506, batch nll 0.065333, batch error rate 0.000000%\n",
      "At minibatch 52700, batch loss 0.360034, batch nll 0.047903, batch error rate 0.000000%\n",
      "At minibatch 52800, batch loss 0.386551, batch nll 0.074512, batch error rate 0.000000%\n",
      "After epoch 33: valid_err_rate: 29.670000% currently going to do 50 epochs\n",
      "After epoch 33: averaged train_err_rate: 0.027500% averaged train nll: 0.046389 averaged train loss: 0.359510\n",
      "At minibatch 52900, batch loss 0.353551, batch nll 0.041780, batch error rate 0.000000%\n",
      "At minibatch 53000, batch loss 0.363962, batch nll 0.052452, batch error rate 0.000000%\n",
      "At minibatch 53100, batch loss 0.333411, batch nll 0.022171, batch error rate 0.000000%\n",
      "At minibatch 53200, batch loss 0.346897, batch nll 0.035935, batch error rate 0.000000%\n",
      "At minibatch 53300, batch loss 0.355294, batch nll 0.044561, batch error rate 0.000000%\n",
      "At minibatch 53400, batch loss 0.352306, batch nll 0.041818, batch error rate 0.000000%\n",
      "At minibatch 53500, batch loss 0.352388, batch nll 0.042089, batch error rate 0.000000%\n",
      "At minibatch 53600, batch loss 0.347550, batch nll 0.037405, batch error rate 0.000000%\n",
      "At minibatch 53700, batch loss 0.359571, batch nll 0.049591, batch error rate 0.000000%\n",
      "At minibatch 53800, batch loss 0.347150, batch nll 0.037373, batch error rate 0.000000%\n",
      "At minibatch 53900, batch loss 0.346413, batch nll 0.036746, batch error rate 0.000000%\n",
      "At minibatch 54000, batch loss 0.371227, batch nll 0.061627, batch error rate 0.000000%\n",
      "At minibatch 54100, batch loss 0.358823, batch nll 0.049305, batch error rate 0.000000%\n",
      "At minibatch 54200, batch loss 0.369690, batch nll 0.060297, batch error rate 0.000000%\n",
      "At minibatch 54300, batch loss 0.374937, batch nll 0.065592, batch error rate 0.000000%\n",
      "At minibatch 54400, batch loss 0.351152, batch nll 0.041842, batch error rate 0.000000%\n",
      "After epoch 34: valid_err_rate: 30.500000% currently going to do 50 epochs\n",
      "After epoch 34: averaged train_err_rate: 0.057500% averaged train nll: 0.045947 averaged train loss: 0.356266\n",
      "At minibatch 54500, batch loss 0.348998, batch nll 0.039892, batch error rate 0.000000%\n",
      "At minibatch 54600, batch loss 0.350005, batch nll 0.041157, batch error rate 0.000000%\n",
      "At minibatch 54700, batch loss 0.362547, batch nll 0.053922, batch error rate 0.000000%\n",
      "At minibatch 54800, batch loss 0.348535, batch nll 0.040172, batch error rate 0.000000%\n",
      "At minibatch 54900, batch loss 0.354039, batch nll 0.045894, batch error rate 0.000000%\n",
      "At minibatch 55000, batch loss 0.345330, batch nll 0.037359, batch error rate 0.000000%\n",
      "At minibatch 55100, batch loss 0.356210, batch nll 0.048400, batch error rate 0.000000%\n",
      "At minibatch 55200, batch loss 0.348295, batch nll 0.040632, batch error rate 0.000000%\n",
      "At minibatch 55300, batch loss 0.325936, batch nll 0.018409, batch error rate 0.000000%\n",
      "At minibatch 55400, batch loss 0.376837, batch nll 0.069425, batch error rate 0.000000%\n",
      "At minibatch 55500, batch loss 0.385283, batch nll 0.077946, batch error rate 0.000000%\n",
      "At minibatch 55600, batch loss 0.357408, batch nll 0.050167, batch error rate 0.000000%\n",
      "At minibatch 55700, batch loss 0.373757, batch nll 0.066650, batch error rate 0.000000%\n",
      "At minibatch 55800, batch loss 0.348096, batch nll 0.041059, batch error rate 0.000000%\n",
      "At minibatch 55900, batch loss 0.342415, batch nll 0.035476, batch error rate 0.000000%\n",
      "At minibatch 56000, batch loss 0.344391, batch nll 0.037520, batch error rate 0.000000%\n",
      "After epoch 35: valid_err_rate: 30.270000% currently going to do 50 epochs\n",
      "After epoch 35: averaged train_err_rate: 0.055000% averaged train nll: 0.045527 averaged train loss: 0.353351\n",
      "At minibatch 56100, batch loss 0.359103, batch nll 0.052401, batch error rate 0.000000%\n",
      "At minibatch 56200, batch loss 0.352736, batch nll 0.046309, batch error rate 0.000000%\n",
      "At minibatch 56300, batch loss 0.342712, batch nll 0.036544, batch error rate 0.000000%\n",
      "At minibatch 56400, batch loss 0.329644, batch nll 0.023738, batch error rate 0.000000%\n",
      "At minibatch 56500, batch loss 0.344425, batch nll 0.038738, batch error rate 0.000000%\n",
      "At minibatch 56600, batch loss 0.351659, batch nll 0.046149, batch error rate 0.000000%\n",
      "At minibatch 56700, batch loss 0.350796, batch nll 0.045505, batch error rate 0.000000%\n",
      "At minibatch 56800, batch loss 0.360305, batch nll 0.055123, batch error rate 0.000000%\n",
      "At minibatch 56900, batch loss 0.370143, batch nll 0.065063, batch error rate 0.000000%\n",
      "At minibatch 57000, batch loss 0.356951, batch nll 0.051951, batch error rate 0.000000%\n",
      "At minibatch 57100, batch loss 0.356146, batch nll 0.051220, batch error rate 0.000000%\n",
      "At minibatch 57200, batch loss 0.332526, batch nll 0.027631, batch error rate 0.000000%\n",
      "At minibatch 57300, batch loss 0.377736, batch nll 0.072918, batch error rate 0.000000%\n",
      "At minibatch 57400, batch loss 0.353912, batch nll 0.049146, batch error rate 0.000000%\n",
      "At minibatch 57500, batch loss 0.358784, batch nll 0.054067, batch error rate 0.000000%\n",
      "At minibatch 57600, batch loss 0.388666, batch nll 0.083960, batch error rate 0.000000%\n",
      "After epoch 36: valid_err_rate: 30.400000% currently going to do 50 epochs\n",
      "After epoch 36: averaged train_err_rate: 0.060000% averaged train nll: 0.045891 averaged train loss: 0.351316\n",
      "At minibatch 57700, batch loss 0.331245, batch nll 0.026725, batch error rate 0.000000%\n",
      "At minibatch 57800, batch loss 0.345326, batch nll 0.041018, batch error rate 0.000000%\n",
      "At minibatch 57900, batch loss 0.353132, batch nll 0.049045, batch error rate 0.000000%\n",
      "At minibatch 58000, batch loss 0.361945, batch nll 0.058065, batch error rate 0.000000%\n",
      "At minibatch 58100, batch loss 0.344484, batch nll 0.040789, batch error rate 0.000000%\n",
      "At minibatch 58200, batch loss 0.353957, batch nll 0.050470, batch error rate 0.000000%\n",
      "At minibatch 58300, batch loss 0.347406, batch nll 0.044098, batch error rate 0.000000%\n",
      "At minibatch 58400, batch loss 0.345650, batch nll 0.042500, batch error rate 0.000000%\n",
      "At minibatch 58500, batch loss 0.350059, batch nll 0.046976, batch error rate 0.000000%\n",
      "At minibatch 58600, batch loss 0.361146, batch nll 0.058201, batch error rate 0.000000%\n",
      "At minibatch 58700, batch loss 0.330274, batch nll 0.027413, batch error rate 0.000000%\n",
      "At minibatch 58800, batch loss 0.343894, batch nll 0.041181, batch error rate 0.000000%\n",
      "At minibatch 58900, batch loss 0.358488, batch nll 0.055851, batch error rate 0.000000%\n",
      "At minibatch 59000, batch loss 0.340056, batch nll 0.037459, batch error rate 0.000000%\n",
      "At minibatch 59100, batch loss 0.362874, batch nll 0.060279, batch error rate 0.000000%\n",
      "At minibatch 59200, batch loss 0.337405, batch nll 0.034836, batch error rate 0.000000%\n",
      "After epoch 37: valid_err_rate: 30.170000% currently going to do 50 epochs\n",
      "After epoch 37: averaged train_err_rate: 0.022500% averaged train nll: 0.044686 averaged train loss: 0.348030\n",
      "At minibatch 59300, batch loss 0.342900, batch nll 0.040529, batch error rate 0.000000%\n",
      "At minibatch 59400, batch loss 0.358229, batch nll 0.056101, batch error rate 0.000000%\n",
      "At minibatch 59500, batch loss 0.350566, batch nll 0.048630, batch error rate 0.000000%\n",
      "At minibatch 59600, batch loss 0.339900, batch nll 0.038203, batch error rate 0.000000%\n",
      "At minibatch 59700, batch loss 0.346457, batch nll 0.044950, batch error rate 0.000000%\n",
      "At minibatch 59800, batch loss 0.344157, batch nll 0.042769, batch error rate 0.000000%\n",
      "At minibatch 59900, batch loss 0.332268, batch nll 0.031050, batch error rate 0.000000%\n",
      "At minibatch 60000, batch loss 0.338822, batch nll 0.037729, batch error rate 0.000000%\n",
      "At minibatch 60100, batch loss 0.353187, batch nll 0.052242, batch error rate 0.000000%\n",
      "At minibatch 60200, batch loss 0.331775, batch nll 0.030935, batch error rate 0.000000%\n",
      "At minibatch 60300, batch loss 0.381799, batch nll 0.081106, batch error rate 0.000000%\n",
      "At minibatch 60400, batch loss 0.336679, batch nll 0.036087, batch error rate 0.000000%\n",
      "At minibatch 60500, batch loss 0.363384, batch nll 0.062834, batch error rate 0.000000%\n",
      "At minibatch 60600, batch loss 0.342169, batch nll 0.041674, batch error rate 0.000000%\n",
      "At minibatch 60700, batch loss 0.331143, batch nll 0.030642, batch error rate 0.000000%\n",
      "At minibatch 60800, batch loss 0.338963, batch nll 0.038461, batch error rate 0.000000%\n",
      "After epoch 38: valid_err_rate: 30.150000% currently going to do 50 epochs\n",
      "After epoch 38: averaged train_err_rate: 0.027500% averaged train nll: 0.044211 averaged train loss: 0.345428\n",
      "At minibatch 60900, batch loss 0.317138, batch nll 0.016817, batch error rate 0.000000%\n",
      "At minibatch 61000, batch loss 0.331567, batch nll 0.031490, batch error rate 0.000000%\n",
      "At minibatch 61100, batch loss 0.338385, batch nll 0.038497, batch error rate 0.000000%\n",
      "At minibatch 61200, batch loss 0.330241, batch nll 0.030572, batch error rate 0.000000%\n",
      "At minibatch 61300, batch loss 0.326846, batch nll 0.027361, batch error rate 0.000000%\n",
      "At minibatch 61400, batch loss 0.325281, batch nll 0.025972, batch error rate 0.000000%\n",
      "At minibatch 61500, batch loss 0.347137, batch nll 0.047935, batch error rate 0.000000%\n",
      "At minibatch 61600, batch loss 0.351729, batch nll 0.052621, batch error rate 0.000000%\n",
      "At minibatch 61700, batch loss 0.354465, batch nll 0.055446, batch error rate 0.000000%\n",
      "At minibatch 61800, batch loss 0.345129, batch nll 0.046230, batch error rate 0.000000%\n",
      "At minibatch 61900, batch loss 0.347341, batch nll 0.048532, batch error rate 0.000000%\n",
      "At minibatch 62000, batch loss 0.378950, batch nll 0.080238, batch error rate 0.000000%\n",
      "At minibatch 62100, batch loss 0.341040, batch nll 0.042396, batch error rate 0.000000%\n",
      "At minibatch 62200, batch loss 0.326784, batch nll 0.028178, batch error rate 0.000000%\n",
      "At minibatch 62300, batch loss 0.338059, batch nll 0.039470, batch error rate 0.000000%\n",
      "At minibatch 62400, batch loss 0.348216, batch nll 0.049643, batch error rate 0.000000%\n",
      "After epoch 39: valid_err_rate: 30.290000% currently going to do 50 epochs\n",
      "After epoch 39: averaged train_err_rate: 0.017500% averaged train nll: 0.043450 averaged train loss: 0.342691\n",
      "At minibatch 62500, batch loss 0.347773, batch nll 0.049366, batch error rate 0.000000%\n",
      "At minibatch 62600, batch loss 0.327468, batch nll 0.029275, batch error rate 0.000000%\n",
      "At minibatch 62700, batch loss 0.331176, batch nll 0.033202, batch error rate 0.000000%\n",
      "At minibatch 62800, batch loss 0.331090, batch nll 0.033276, batch error rate 0.000000%\n",
      "At minibatch 62900, batch loss 0.331973, batch nll 0.034351, batch error rate 0.000000%\n",
      "At minibatch 63000, batch loss 0.340650, batch nll 0.043192, batch error rate 0.000000%\n",
      "At minibatch 63100, batch loss 0.325549, batch nll 0.028231, batch error rate 0.000000%\n",
      "At minibatch 63200, batch loss 0.335406, batch nll 0.038230, batch error rate 0.000000%\n",
      "At minibatch 63300, batch loss 0.327614, batch nll 0.030560, batch error rate 0.000000%\n",
      "At minibatch 63400, batch loss 0.331130, batch nll 0.034132, batch error rate 0.000000%\n",
      "At minibatch 63500, batch loss 0.337402, batch nll 0.040463, batch error rate 0.000000%\n",
      "At minibatch 63600, batch loss 0.341323, batch nll 0.044450, batch error rate 0.000000%\n",
      "At minibatch 63700, batch loss 0.345735, batch nll 0.048907, batch error rate 0.000000%\n",
      "At minibatch 63800, batch loss 0.354477, batch nll 0.057694, batch error rate 0.000000%\n",
      "At minibatch 63900, batch loss 0.350358, batch nll 0.053578, batch error rate 0.000000%\n",
      "At minibatch 64000, batch loss 0.346467, batch nll 0.049742, batch error rate 0.000000%\n",
      "After epoch 40: valid_err_rate: 30.390000% currently going to do 50 epochs\n",
      "After epoch 40: averaged train_err_rate: 0.022500% averaged train nll: 0.043371 averaged train loss: 0.340735\n",
      "At minibatch 64100, batch loss 0.352596, batch nll 0.056022, batch error rate 0.000000%\n",
      "At minibatch 64200, batch loss 0.332815, batch nll 0.036440, batch error rate 0.000000%\n",
      "At minibatch 64300, batch loss 0.323384, batch nll 0.027244, batch error rate 0.000000%\n",
      "At minibatch 64400, batch loss 0.325910, batch nll 0.029924, batch error rate 0.000000%\n",
      "At minibatch 64500, batch loss 0.328059, batch nll 0.032235, batch error rate 0.000000%\n",
      "At minibatch 64600, batch loss 0.333600, batch nll 0.037951, batch error rate 0.000000%\n",
      "At minibatch 64700, batch loss 0.316552, batch nll 0.021000, batch error rate 0.000000%\n",
      "At minibatch 64800, batch loss 0.345022, batch nll 0.049568, batch error rate 0.000000%\n",
      "At minibatch 64900, batch loss 0.344702, batch nll 0.049302, batch error rate 0.000000%\n",
      "At minibatch 65000, batch loss 0.337501, batch nll 0.042181, batch error rate 0.000000%\n",
      "At minibatch 65100, batch loss 0.322138, batch nll 0.026862, batch error rate 0.000000%\n",
      "At minibatch 65200, batch loss 0.341885, batch nll 0.046665, batch error rate 0.000000%\n",
      "At minibatch 65300, batch loss 0.357948, batch nll 0.062785, batch error rate 0.000000%\n",
      "At minibatch 65400, batch loss 0.343269, batch nll 0.048173, batch error rate 0.000000%\n",
      "At minibatch 65500, batch loss 0.340909, batch nll 0.045818, batch error rate 0.000000%\n",
      "At minibatch 65600, batch loss 0.319352, batch nll 0.024270, batch error rate 0.000000%\n",
      "After epoch 41: valid_err_rate: 30.210000% currently going to do 50 epochs\n",
      "After epoch 41: averaged train_err_rate: 0.020000% averaged train nll: 0.043527 averaged train loss: 0.339152\n",
      "At minibatch 65700, batch loss 0.328967, batch nll 0.034023, batch error rate 0.000000%\n",
      "At minibatch 65800, batch loss 0.348223, batch nll 0.053470, batch error rate 0.000000%\n",
      "At minibatch 65900, batch loss 0.320563, batch nll 0.025976, batch error rate 0.000000%\n",
      "At minibatch 66000, batch loss 0.350583, batch nll 0.056144, batch error rate 0.000000%\n",
      "At minibatch 66100, batch loss 0.340195, batch nll 0.045948, batch error rate 0.000000%\n",
      "At minibatch 66200, batch loss 0.339307, batch nll 0.045237, batch error rate 0.000000%\n",
      "At minibatch 66300, batch loss 0.354609, batch nll 0.060652, batch error rate 0.000000%\n",
      "At minibatch 66400, batch loss 0.335184, batch nll 0.041357, batch error rate 0.000000%\n",
      "At minibatch 66500, batch loss 0.321029, batch nll 0.027249, batch error rate 0.000000%\n",
      "At minibatch 66600, batch loss 0.334221, batch nll 0.040508, batch error rate 0.000000%\n",
      "At minibatch 66700, batch loss 0.318437, batch nll 0.024782, batch error rate 0.000000%\n",
      "At minibatch 66800, batch loss 0.324448, batch nll 0.030858, batch error rate 0.000000%\n",
      "At minibatch 66900, batch loss 0.347016, batch nll 0.053483, batch error rate 0.000000%\n",
      "At minibatch 67000, batch loss 0.349768, batch nll 0.056273, batch error rate 0.000000%\n",
      "At minibatch 67100, batch loss 0.349208, batch nll 0.055752, batch error rate 0.000000%\n",
      "At minibatch 67200, batch loss 0.323013, batch nll 0.029551, batch error rate 0.000000%\n",
      "After epoch 42: valid_err_rate: 30.500000% currently going to do 50 epochs\n",
      "After epoch 42: averaged train_err_rate: 0.022500% averaged train nll: 0.042978 averaged train loss: 0.336997\n",
      "At minibatch 67300, batch loss 0.315873, batch nll 0.022539, batch error rate 0.000000%\n",
      "At minibatch 67400, batch loss 0.317808, batch nll 0.024620, batch error rate 0.000000%\n",
      "At minibatch 67500, batch loss 0.321709, batch nll 0.028697, batch error rate 0.000000%\n",
      "At minibatch 67600, batch loss 0.328042, batch nll 0.035231, batch error rate 0.000000%\n",
      "At minibatch 67700, batch loss 0.322010, batch nll 0.029335, batch error rate 0.000000%\n",
      "At minibatch 67800, batch loss 0.335062, batch nll 0.042494, batch error rate 0.000000%\n",
      "At minibatch 67900, batch loss 0.323757, batch nll 0.031301, batch error rate 0.000000%\n",
      "At minibatch 68000, batch loss 0.331423, batch nll 0.039097, batch error rate 0.000000%\n",
      "At minibatch 68100, batch loss 0.327296, batch nll 0.035037, batch error rate 0.000000%\n",
      "At minibatch 68200, batch loss 0.329787, batch nll 0.037619, batch error rate 0.000000%\n",
      "At minibatch 68300, batch loss 0.348918, batch nll 0.056785, batch error rate 0.000000%\n",
      "At minibatch 68400, batch loss 0.351634, batch nll 0.059531, batch error rate 0.000000%\n",
      "At minibatch 68500, batch loss 0.355860, batch nll 0.063804, batch error rate 0.000000%\n",
      "At minibatch 68600, batch loss 0.322710, batch nll 0.030709, batch error rate 0.000000%\n",
      "At minibatch 68700, batch loss 0.327566, batch nll 0.035580, batch error rate 0.000000%\n",
      "At minibatch 68800, batch loss 0.353938, batch nll 0.061979, batch error rate 0.000000%\n",
      "After epoch 43: valid_err_rate: 30.250000% currently going to do 50 epochs\n",
      "After epoch 43: averaged train_err_rate: 0.010000% averaged train nll: 0.042681 averaged train loss: 0.335166\n",
      "At minibatch 68900, batch loss 0.319370, batch nll 0.027546, batch error rate 0.000000%\n",
      "At minibatch 69000, batch loss 0.340964, batch nll 0.049319, batch error rate 0.000000%\n",
      "At minibatch 69100, batch loss 0.328613, batch nll 0.037110, batch error rate 0.000000%\n",
      "At minibatch 69200, batch loss 0.329371, batch nll 0.038011, batch error rate 0.000000%\n",
      "At minibatch 69300, batch loss 0.327889, batch nll 0.036630, batch error rate 0.000000%\n",
      "At minibatch 69400, batch loss 0.320665, batch nll 0.029556, batch error rate 0.000000%\n",
      "At minibatch 69500, batch loss 0.320395, batch nll 0.029418, batch error rate 0.000000%\n",
      "At minibatch 69600, batch loss 0.324230, batch nll 0.033373, batch error rate 0.000000%\n",
      "At minibatch 69700, batch loss 0.341383, batch nll 0.050631, batch error rate 0.000000%\n",
      "At minibatch 69800, batch loss 0.336522, batch nll 0.045859, batch error rate 0.000000%\n",
      "At minibatch 69900, batch loss 0.343932, batch nll 0.053326, batch error rate 0.000000%\n",
      "At minibatch 70000, batch loss 0.343971, batch nll 0.053410, batch error rate 0.000000%\n",
      "At minibatch 70100, batch loss 0.329579, batch nll 0.039062, batch error rate 0.000000%\n",
      "At minibatch 70200, batch loss 0.343874, batch nll 0.053376, batch error rate 0.000000%\n",
      "At minibatch 70300, batch loss 0.325527, batch nll 0.035066, batch error rate 0.000000%\n",
      "At minibatch 70400, batch loss 0.341433, batch nll 0.050966, batch error rate 0.000000%\n",
      "After epoch 44: valid_err_rate: 30.390000% currently going to do 50 epochs\n",
      "After epoch 44: averaged train_err_rate: 0.010000% averaged train nll: 0.042089 averaged train loss: 0.333078\n",
      "At minibatch 70500, batch loss 0.345795, batch nll 0.055493, batch error rate 0.000000%\n",
      "At minibatch 70600, batch loss 0.374293, batch nll 0.084151, batch error rate 0.000000%\n",
      "At minibatch 70700, batch loss 0.331312, batch nll 0.041322, batch error rate 0.000000%\n",
      "At minibatch 70800, batch loss 0.342533, batch nll 0.052687, batch error rate 0.000000%\n",
      "At minibatch 70900, batch loss 0.310681, batch nll 0.020946, batch error rate 0.000000%\n",
      "At minibatch 71000, batch loss 0.319154, batch nll 0.029552, batch error rate 0.000000%\n",
      "At minibatch 71100, batch loss 0.315360, batch nll 0.025870, batch error rate 0.000000%\n",
      "At minibatch 71200, batch loss 0.324222, batch nll 0.034855, batch error rate 0.000000%\n",
      "At minibatch 71300, batch loss 0.326511, batch nll 0.037209, batch error rate 0.000000%\n",
      "At minibatch 71400, batch loss 0.343448, batch nll 0.054209, batch error rate 0.000000%\n",
      "At minibatch 71500, batch loss 0.319956, batch nll 0.030762, batch error rate 0.000000%\n",
      "At minibatch 71600, batch loss 0.323384, batch nll 0.034249, batch error rate 0.000000%\n",
      "At minibatch 71700, batch loss 0.347975, batch nll 0.058878, batch error rate 0.000000%\n",
      "At minibatch 71800, batch loss 0.327823, batch nll 0.038781, batch error rate 0.000000%\n",
      "At minibatch 71900, batch loss 0.341584, batch nll 0.052587, batch error rate 0.000000%\n",
      "At minibatch 72000, batch loss 0.327452, batch nll 0.038431, batch error rate 0.000000%\n",
      "After epoch 45: valid_err_rate: 30.550000% currently going to do 50 epochs\n",
      "After epoch 45: averaged train_err_rate: 0.027500% averaged train nll: 0.041949 averaged train loss: 0.331461\n",
      "At minibatch 72100, batch loss 0.326419, batch nll 0.037496, batch error rate 0.000000%\n",
      "At minibatch 72200, batch loss 0.319397, batch nll 0.030616, batch error rate 0.000000%\n",
      "At minibatch 72300, batch loss 0.326252, batch nll 0.037604, batch error rate 0.000000%\n",
      "At minibatch 72400, batch loss 0.335185, batch nll 0.046665, batch error rate 0.000000%\n",
      "At minibatch 72500, batch loss 0.311669, batch nll 0.023288, batch error rate 0.000000%\n",
      "At minibatch 72600, batch loss 0.313585, batch nll 0.025302, batch error rate 0.000000%\n",
      "At minibatch 72700, batch loss 0.323792, batch nll 0.035573, batch error rate 0.000000%\n",
      "At minibatch 72800, batch loss 0.311046, batch nll 0.022959, batch error rate 0.000000%\n",
      "At minibatch 72900, batch loss 0.327934, batch nll 0.039919, batch error rate 0.000000%\n",
      "At minibatch 73000, batch loss 0.346292, batch nll 0.058393, batch error rate 0.000000%\n",
      "At minibatch 73100, batch loss 0.319606, batch nll 0.031766, batch error rate 0.000000%\n",
      "At minibatch 73200, batch loss 0.334433, batch nll 0.046611, batch error rate 0.000000%\n",
      "At minibatch 73300, batch loss 0.336590, batch nll 0.048781, batch error rate 0.000000%\n",
      "At minibatch 73400, batch loss 0.307591, batch nll 0.019790, batch error rate 0.000000%\n",
      "At minibatch 73500, batch loss 0.344931, batch nll 0.057130, batch error rate 0.000000%\n",
      "At minibatch 73600, batch loss 0.331868, batch nll 0.044122, batch error rate 0.000000%\n",
      "After epoch 46: valid_err_rate: 30.490000% currently going to do 50 epochs\n",
      "After epoch 46: averaged train_err_rate: 0.007500% averaged train nll: 0.041763 averaged train loss: 0.329962\n",
      "At minibatch 73700, batch loss 0.324915, batch nll 0.037278, batch error rate 0.000000%\n",
      "At minibatch 73800, batch loss 0.328084, batch nll 0.040612, batch error rate 0.000000%\n",
      "At minibatch 73900, batch loss 0.320356, batch nll 0.033036, batch error rate 0.000000%\n",
      "At minibatch 74000, batch loss 0.315301, batch nll 0.028119, batch error rate 0.000000%\n",
      "At minibatch 74100, batch loss 0.323920, batch nll 0.036859, batch error rate 0.000000%\n",
      "At minibatch 74200, batch loss 0.310356, batch nll 0.023406, batch error rate 0.000000%\n",
      "At minibatch 74300, batch loss 0.309313, batch nll 0.022488, batch error rate 0.000000%\n",
      "At minibatch 74400, batch loss 0.345603, batch nll 0.058876, batch error rate 0.000000%\n",
      "At minibatch 74500, batch loss 0.357417, batch nll 0.070749, batch error rate 4.000000%\n",
      "At minibatch 74600, batch loss 0.319704, batch nll 0.033095, batch error rate 0.000000%\n",
      "At minibatch 74700, batch loss 0.319255, batch nll 0.032703, batch error rate 0.000000%\n",
      "At minibatch 74800, batch loss 0.331410, batch nll 0.044888, batch error rate 0.000000%\n",
      "At minibatch 74900, batch loss 0.307589, batch nll 0.021112, batch error rate 0.000000%\n",
      "At minibatch 75000, batch loss 0.352074, batch nll 0.065597, batch error rate 0.000000%\n",
      "At minibatch 75100, batch loss 0.315647, batch nll 0.029155, batch error rate 0.000000%\n",
      "At minibatch 75200, batch loss 0.313482, batch nll 0.026988, batch error rate 0.000000%\n",
      "After epoch 47: valid_err_rate: 30.300000% currently going to do 50 epochs\n",
      "After epoch 47: averaged train_err_rate: 0.017500% averaged train nll: 0.041709 averaged train loss: 0.328588\n",
      "At minibatch 75300, batch loss 0.339705, batch nll 0.053348, batch error rate 0.000000%\n",
      "At minibatch 75400, batch loss 0.315867, batch nll 0.029640, batch error rate 0.000000%\n",
      "At minibatch 75500, batch loss 0.322662, batch nll 0.036565, batch error rate 0.000000%\n",
      "At minibatch 75600, batch loss 0.329090, batch nll 0.043133, batch error rate 0.000000%\n",
      "At minibatch 75700, batch loss 0.304997, batch nll 0.019162, batch error rate 0.000000%\n",
      "At minibatch 75800, batch loss 0.325672, batch nll 0.039918, batch error rate 0.000000%\n",
      "At minibatch 75900, batch loss 0.317165, batch nll 0.031495, batch error rate 0.000000%\n",
      "At minibatch 76000, batch loss 0.315515, batch nll 0.029910, batch error rate 0.000000%\n",
      "At minibatch 76100, batch loss 0.342754, batch nll 0.057234, batch error rate 0.000000%\n",
      "At minibatch 76200, batch loss 0.311546, batch nll 0.026092, batch error rate 0.000000%\n",
      "At minibatch 76300, batch loss 0.310561, batch nll 0.025177, batch error rate 0.000000%\n",
      "At minibatch 76400, batch loss 0.342862, batch nll 0.057548, batch error rate 0.000000%\n",
      "At minibatch 76500, batch loss 0.328198, batch nll 0.042908, batch error rate 0.000000%\n",
      "At minibatch 76600, batch loss 0.311170, batch nll 0.025879, batch error rate 0.000000%\n",
      "At minibatch 76700, batch loss 0.357949, batch nll 0.072676, batch error rate 0.000000%\n",
      "At minibatch 76800, batch loss 0.304073, batch nll 0.018796, batch error rate 0.000000%\n",
      "After epoch 48: valid_err_rate: 30.380000% currently going to do 50 epochs\n",
      "After epoch 48: averaged train_err_rate: 0.012500% averaged train nll: 0.041244 averaged train loss: 0.326927\n",
      "At minibatch 76900, batch loss 0.307226, batch nll 0.022096, batch error rate 0.000000%\n",
      "At minibatch 77000, batch loss 0.355289, batch nll 0.070274, batch error rate 0.000000%\n",
      "At minibatch 77100, batch loss 0.327339, batch nll 0.042453, batch error rate 0.000000%\n",
      "At minibatch 77200, batch loss 0.321793, batch nll 0.037029, batch error rate 0.000000%\n",
      "At minibatch 77300, batch loss 0.327740, batch nll 0.043094, batch error rate 0.000000%\n",
      "At minibatch 77400, batch loss 0.317484, batch nll 0.032947, batch error rate 0.000000%\n",
      "At minibatch 77500, batch loss 0.323111, batch nll 0.038661, batch error rate 0.000000%\n",
      "At minibatch 77600, batch loss 0.338520, batch nll 0.054169, batch error rate 0.000000%\n",
      "At minibatch 77700, batch loss 0.319865, batch nll 0.035573, batch error rate 0.000000%\n",
      "At minibatch 77800, batch loss 0.391857, batch nll 0.107608, batch error rate 0.000000%\n",
      "At minibatch 77900, batch loss 0.340953, batch nll 0.056750, batch error rate 0.000000%\n",
      "At minibatch 78000, batch loss 0.317855, batch nll 0.033675, batch error rate 0.000000%\n",
      "At minibatch 78100, batch loss 0.332043, batch nll 0.047894, batch error rate 0.000000%\n",
      "At minibatch 78200, batch loss 0.328428, batch nll 0.044289, batch error rate 0.000000%\n",
      "At minibatch 78300, batch loss 0.350655, batch nll 0.066556, batch error rate 0.000000%\n",
      "At minibatch 78400, batch loss 0.351354, batch nll 0.067255, batch error rate 0.000000%\n",
      "After epoch 49: valid_err_rate: 30.210000% currently going to do 50 epochs\n",
      "After epoch 49: averaged train_err_rate: 0.005000% averaged train nll: 0.040886 averaged train loss: 0.325371\n",
      "At minibatch 78500, batch loss 0.316461, batch nll 0.032463, batch error rate 0.000000%\n",
      "At minibatch 78600, batch loss 0.306842, batch nll 0.022978, batch error rate 0.000000%\n",
      "At minibatch 78700, batch loss 0.306273, batch nll 0.022535, batch error rate 0.000000%\n",
      "At minibatch 78800, batch loss 0.330686, batch nll 0.047072, batch error rate 0.000000%\n",
      "At minibatch 78900, batch loss 0.330619, batch nll 0.047100, batch error rate 0.000000%\n",
      "At minibatch 79000, batch loss 0.304604, batch nll 0.021147, batch error rate 0.000000%\n",
      "At minibatch 79100, batch loss 0.305240, batch nll 0.021856, batch error rate 0.000000%\n",
      "At minibatch 79200, batch loss 0.320854, batch nll 0.037593, batch error rate 0.000000%\n",
      "At minibatch 79300, batch loss 0.309696, batch nll 0.026488, batch error rate 0.000000%\n",
      "At minibatch 79400, batch loss 0.319302, batch nll 0.036169, batch error rate 0.000000%\n",
      "At minibatch 79500, batch loss 0.337522, batch nll 0.054446, batch error rate 0.000000%\n",
      "At minibatch 79600, batch loss 0.332547, batch nll 0.049505, batch error rate 0.000000%\n",
      "At minibatch 79700, batch loss 0.317365, batch nll 0.034367, batch error rate 0.000000%\n",
      "At minibatch 79800, batch loss 0.322944, batch nll 0.039969, batch error rate 0.000000%\n",
      "At minibatch 79900, batch loss 0.320771, batch nll 0.037811, batch error rate 0.000000%\n",
      "At minibatch 80000, batch loss 0.333831, batch nll 0.050882, batch error rate 0.000000%\n",
      "After epoch 50: valid_err_rate: 30.660000% currently going to do 50 epochs\n",
      "After epoch 50: averaged train_err_rate: 0.007500% averaged train nll: 0.040540 averaged train loss: 0.323899\n",
      "At minibatch 80100, batch loss 0.313978, batch nll 0.031128, batch error rate 0.000000%\n",
      "At minibatch 80200, batch loss 0.319529, batch nll 0.036792, batch error rate 0.000000%\n",
      "At minibatch 80300, batch loss 0.312066, batch nll 0.029441, batch error rate 0.000000%\n",
      "At minibatch 80400, batch loss 0.311100, batch nll 0.028598, batch error rate 0.000000%\n",
      "At minibatch 80500, batch loss 0.322893, batch nll 0.040492, batch error rate 0.000000%\n",
      "At minibatch 80600, batch loss 0.327560, batch nll 0.045247, batch error rate 0.000000%\n",
      "At minibatch 80700, batch loss 0.353106, batch nll 0.070870, batch error rate 0.000000%\n",
      "At minibatch 80800, batch loss 0.314011, batch nll 0.031808, batch error rate 0.000000%\n",
      "At minibatch 80900, batch loss 0.318188, batch nll 0.036036, batch error rate 0.000000%\n",
      "At minibatch 81000, batch loss 0.329616, batch nll 0.047536, batch error rate 0.000000%\n",
      "At minibatch 81100, batch loss 0.330049, batch nll 0.048019, batch error rate 0.000000%\n",
      "At minibatch 81200, batch loss 0.309657, batch nll 0.027679, batch error rate 0.000000%\n",
      "At minibatch 81300, batch loss 0.342525, batch nll 0.060586, batch error rate 0.000000%\n",
      "At minibatch 81400, batch loss 0.309966, batch nll 0.028049, batch error rate 0.000000%\n",
      "At minibatch 81500, batch loss 0.341224, batch nll 0.059331, batch error rate 0.000000%\n",
      "At minibatch 81600, batch loss 0.319628, batch nll 0.037758, batch error rate 0.000000%\n",
      "After epoch 51: valid_err_rate: 30.550000% currently going to do 50 epochs\n",
      "After epoch 51: averaged train_err_rate: 0.010000% averaged train nll: 0.040390 averaged train loss: 0.322653\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "while e < number_of_epochs: # This loop goes over epochs\n",
    "    e += 1\n",
    "    # First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar10_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 28e-3 * K / np.maximum(K, i)\n",
    "        momentum = 0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        # print [p.get_value().ravel()[: 10] for p in model_parameters]\n",
    "        # print [p.get_value().ravel()[: 10] for p in velocities]\n",
    "        \n",
    "        train_loss.append((i, L))\n",
    "        train_erros.append((i, err_rate))\n",
    "        train_nll.append((i, nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate * 100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar10_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion + 1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i, val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" \\\n",
    "          % (e,val_error_rate * 100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" \\\n",
    "          % (e,\n",
    "             np.mean(np.asarray(train_erros)[epoch_start_i :, 1]) * 100, \n",
    "             np.mean(np.asarray(train_nll)[epoch_start_i :, 1]),\n",
    "             np.mean(np.asarray(train_loss)[epoch_start_i :, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 31\n",
      "Test error rate is 31.210000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa0440d8710>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBkFwzAMI8T/B4YbtwsQgekIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa05e41a990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" % (best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" % (compute_error_rate(cifar10_test_stream) * 100.0,)\n",
    "\n",
    "subplot(2, 1, 1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:, 0], train_nll_a[:, 1], label = \"batch train nll\")\n",
    "legend()\n",
    "\n",
    "subplot(2, 1, 2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:, 0], train_erros_a[:, 1], label = \"batch train error rate\")\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:, 0], validation_errors_a[:, 1], label = \"validation error rate\", color = \"r\")\n",
    "ylim(0, 0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results:\n",
      "\n",
      "  | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
      "--+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
      "1 |  1e-3 * ... ||       |          |         |      |           |\n",
      "2 |  2e-3 * ... || 34.410000%      | 27         | 42        | 34.250000%     | 20.372500%         |\n",
      "3 |  4e-3 * ... || 32.910000%      | 15         | 24        | 33.830000%     |  9.650000%         |\n",
      "4 |  8e-3 * ... || 32.050000%      | 14         | 22        | 30.990000%     |  0.352500%         |\n",
      "  | 12e-3 * ... || 32.450000%      | 10         | 16        | 32.400000%     |  1.422500%         |\n",
      "5 | 16e-3 * ... || 31.460000%      |  9         | 15        | 31.190000%     |  2.002500%         |\n",
      "  | 20e-3 * ... || 31.210000%      | 31         | 48        | 30.280000%     | 0.005000%          |\n",
      "  | 24e-3 * ... || 31.160000%      | 26         | 40        | 31.480000%     |  0.047500%         |\n",
      "  | 28e-3 * ... ||       |           |         |      |           |\n",
      "6 | 32e-3 * ... || 32.760000%      | 11         | 18        | 32.590000%     |  5.222500%         |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ''' Results:\n",
    "\n",
    "  | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
    "--+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
    "  |  2e-3 * ... || 34.410000%      | 27         | 42        | 34.250000%     | 20.372500%         |\n",
    "  |  4e-3 * ... || 32.910000%      | 15         | 24        | 33.830000%     |  9.650000%         |\n",
    "  |  8e-3 * ... || 32.050000%      | 14         | 22        | 30.990000%     |  0.352500%         |\n",
    "  | 12e-3 * ... || 32.450000%      | 10         | 16        | 32.400000%     |  1.422500%         |\n",
    "  | 16e-3 * ... || 31.460000%      |  9         | 15        | 31.190000%     |  2.002500%         |\n",
    "  | 20e-3 * ... || 31.210000%      | 31         | 48        | 30.280000%     | 0.005000%          |\n",
    "  | 24e-3 * ... || 31.160000%      | 26         | 40        | 31.480000%     |  0.047500%         |\n",
    "  | 28e-3 * ... ||       |           |         |      |           |\n",
    "  | 32e-3 * ... || 32.760000%      | 11         | 18        | 32.590000%     |  5.222500%         |\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
