{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 27 days\n",
      "Vendor:  Continuum Analytics, Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package: mkl\n",
      "Message: trial mode expires in 27 days\n"
     ]
    }
   ],
   "source": [
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a parameters' dictionary for the sake of easy usage and fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"K\" : 3000,               # was 2000\n",
    "          \"momentum\" : 0.8,         # was 0.9\n",
    "          \"lrate_const\" : 24e-3,    # was 4e-3\n",
    "          \"num_filters_1\" : 50,     # was 10\n",
    "          \"num_filters_2\" : 75,     # was 25\n",
    "          \"num_fw3_hidden\" : 500,   # was 500\n",
    "          \"gauss\" : 0.025}          # was 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will now build a convolutional network for the CIFAR-10 data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = ((ScaleAndShift, [2.0 / 255.0, -1], {\"which_sources\" : \"features\"}),\n",
    "                                (Cast, [np.float32], {\"which_sources\" : \"features\"}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset = slice(None, 40000))\n",
    "# this stream will shuffle the CIFAR-10 set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(cifar10_train,\n",
    "                                                 iteration_scheme = ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset = slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these don't do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(cifar10_validation,\n",
    "                                                      iteration_scheme = SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(cifar10_test,\n",
    "                                                iteration_scheme = SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are taken from https://github.com/mila-udem/blocks.\n",
    "class Constant():\n",
    "    '''Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    '''\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype = np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    '''Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    '''\n",
    "    def __init__(self, std = 1, mean = 0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size = shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    '''Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width / 2, mean + width / 2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    '''\n",
    "    def __init__(self, mean = 0., width = None, std = None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1 / 12 * width ^ 2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size = shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4(\"X\")\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix(\"Y\", dtype = \"uint8\")\n",
    "\n",
    "# The tag values are useful during debugging the creation of Theano graphs\n",
    "X_test_value, Y_test_value = next(cifar10_train_stream.get_epoch_iterator())\n",
    "\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values\n",
    "\n",
    "X.tag.test_value = X_test_value[: 3]\n",
    "Y.tag.test_value = Y_test_value[: 3]\n",
    "\n",
    "print \"X shape: %s\" % (X.tag.test_value.shape,)\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "# The first convolutional layer\n",
    "# The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = params[\"num_filters_1\"] # we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1, 3, 5, 5), dtype = \"float32\"),\n",
    "                    name = \"CW1\")\n",
    "# please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype = \"float32\"),\n",
    "                    name = \"CB1\")\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2, 2), ignore_border = True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "num_filters_2 = params[\"num_filters_2\"] # we will compute ten convolution filters in the first layer # was 25\n",
    "CW2 = theano.shared(np.zeros((num_filters_2, num_filters_1, 5, 5), dtype = \"float32\"),\n",
    "                   name = \"CW2\")\n",
    "CW2.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype = \"float32\"),\n",
    "                    name = \"CB2\")\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2, 2), ignore_border = True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "# Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden = params[\"num_fw3_hidden\"]\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW3\")\n",
    "FW3.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB3\")\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle(\"x\", 0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "num_fw4_hidden = 10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW4\")\n",
    "FW4.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB4\")\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle(\"x\", 0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis = 1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions, Y.ravel()).mean()\n",
    "nll = -theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1] == \"W\":\n",
    "        weight_decay += 1e-3 * (p ** 2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "# At this point stop computing test values\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "# svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar(\"lrate\", dtype = \"float32\")\n",
    "momentum = theano.tensor.scalar(\"momentum\", dtype = \"float32\")\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "# initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name = \"V_%s\" % (p.name,)) for p in model_parameters]\n",
    "\n",
    "for p, g, v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v, v_new), (p, p_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile theano functions\n",
    "\n",
    "# each call to train step will make one SGD step\n",
    "train_step = theano.function([X, Y, lrate, momentum], [cost, error_rate, nll, weight_decay], updates = updates)\n",
    "# each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X) != Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities to save values of parameters and to load them\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow = False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.476597, batch nll 1.850605, batch error rate 56.000000%\n",
      "At minibatch 200, batch loss 2.211231, batch nll 1.607272, batch error rate 56.000000%\n",
      "At minibatch 300, batch loss 2.092952, batch nll 1.510201, batch error rate 44.000000%\n",
      "At minibatch 400, batch loss 2.198758, batch nll 1.634651, batch error rate 60.000000%\n",
      "At minibatch 500, batch loss 2.006610, batch nll 1.459856, batch error rate 64.000000%\n",
      "At minibatch 600, batch loss 1.961200, batch nll 1.431306, batch error rate 56.000000%\n",
      "At minibatch 700, batch loss 1.883324, batch nll 1.369465, batch error rate 52.000000%\n",
      "At minibatch 800, batch loss 1.868005, batch nll 1.369114, batch error rate 56.000000%\n",
      "At minibatch 900, batch loss 1.932679, batch nll 1.447400, batch error rate 56.000000%\n",
      "At minibatch 1000, batch loss 2.038506, batch nll 1.565564, batch error rate 60.000000%\n",
      "At minibatch 1100, batch loss 2.147186, batch nll 1.686222, batch error rate 52.000000%\n",
      "At minibatch 1200, batch loss 1.667097, batch nll 1.216070, batch error rate 44.000000%\n",
      "At minibatch 1300, batch loss 2.080962, batch nll 1.639757, batch error rate 64.000000%\n",
      "At minibatch 1400, batch loss 1.599412, batch nll 1.168062, batch error rate 40.000000%\n",
      "At minibatch 1500, batch loss 1.634205, batch nll 1.209568, batch error rate 48.000000%\n",
      "At minibatch 1600, batch loss 1.338082, batch nll 0.920975, batch error rate 32.000000%\n",
      "After epoch 1: valid_err_rate: 43.960000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 54.560000% averaged train nll: 1.511894 averaged train loss: 2.022213\n",
      "At minibatch 1700, batch loss 1.608294, batch nll 1.196349, batch error rate 48.000000%\n",
      "At minibatch 1800, batch loss 1.217014, batch nll 0.812124, batch error rate 32.000000%\n",
      "At minibatch 1900, batch loss 1.567464, batch nll 1.167821, batch error rate 44.000000%\n",
      "At minibatch 2000, batch loss 2.098874, batch nll 1.703369, batch error rate 72.000000%\n",
      "At minibatch 2100, batch loss 1.586644, batch nll 1.195317, batch error rate 40.000000%\n",
      "At minibatch 2200, batch loss 1.658389, batch nll 1.270813, batch error rate 56.000000%\n",
      "At minibatch 2300, batch loss 2.161762, batch nll 1.779264, batch error rate 68.000000%\n",
      "At minibatch 2400, batch loss 1.314463, batch nll 0.935747, batch error rate 36.000000%\n",
      "At minibatch 2500, batch loss 2.004909, batch nll 1.630282, batch error rate 64.000000%\n",
      "At minibatch 2600, batch loss 1.739794, batch nll 1.367044, batch error rate 36.000000%\n",
      "At minibatch 2700, batch loss 1.386898, batch nll 1.017314, batch error rate 24.000000%\n",
      "At minibatch 2800, batch loss 1.842074, batch nll 1.476064, batch error rate 64.000000%\n",
      "At minibatch 2900, batch loss 1.687866, batch nll 1.324528, batch error rate 48.000000%\n",
      "At minibatch 3000, batch loss 1.137359, batch nll 0.775824, batch error rate 36.000000%\n",
      "At minibatch 3100, batch loss 1.747223, batch nll 1.387666, batch error rate 36.000000%\n",
      "At minibatch 3200, batch loss 1.501243, batch nll 1.143991, batch error rate 40.000000%\n",
      "After epoch 2: valid_err_rate: 34.790000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 39.085000% averaged train nll: 1.115775 averaged train loss: 1.497425\n",
      "At minibatch 3300, batch loss 1.105065, batch nll 0.746906, batch error rate 32.000000%\n",
      "At minibatch 3400, batch loss 0.922452, batch nll 0.563953, batch error rate 16.000000%\n",
      "At minibatch 3500, batch loss 1.383610, batch nll 1.024933, batch error rate 28.000000%\n",
      "At minibatch 3600, batch loss 1.421328, batch nll 1.064056, batch error rate 28.000000%\n",
      "At minibatch 3700, batch loss 1.339924, batch nll 0.982398, batch error rate 40.000000%\n",
      "At minibatch 3800, batch loss 1.434669, batch nll 1.078188, batch error rate 32.000000%\n",
      "At minibatch 3900, batch loss 1.716062, batch nll 1.361745, batch error rate 40.000000%\n",
      "At minibatch 4000, batch loss 1.347578, batch nll 0.994272, batch error rate 28.000000%\n",
      "At minibatch 4100, batch loss 1.122890, batch nll 0.772024, batch error rate 28.000000%\n",
      "At minibatch 4200, batch loss 1.051264, batch nll 0.701659, batch error rate 24.000000%\n",
      "At minibatch 4300, batch loss 1.230135, batch nll 0.882330, batch error rate 32.000000%\n",
      "At minibatch 4400, batch loss 1.120700, batch nll 0.774942, batch error rate 36.000000%\n",
      "At minibatch 4500, batch loss 1.252176, batch nll 0.908126, batch error rate 36.000000%\n",
      "At minibatch 4600, batch loss 1.236712, batch nll 0.894823, batch error rate 28.000000%\n",
      "At minibatch 4700, batch loss 1.050008, batch nll 0.709640, batch error rate 28.000000%\n",
      "At minibatch 4800, batch loss 1.142138, batch nll 0.803785, batch error rate 40.000000%\n",
      "After epoch 3: valid_err_rate: 29.860000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 30.667500% averaged train nll: 0.877964 averaged train loss: 1.229409\n",
      "At minibatch 4900, batch loss 0.908563, batch nll 0.568570, batch error rate 16.000000%\n",
      "At minibatch 5000, batch loss 1.330659, batch nll 0.989914, batch error rate 32.000000%\n",
      "At minibatch 5100, batch loss 0.757750, batch nll 0.415762, batch error rate 16.000000%\n",
      "At minibatch 5200, batch loss 1.113591, batch nll 0.771239, batch error rate 28.000000%\n",
      "At minibatch 5300, batch loss 1.168977, batch nll 0.826412, batch error rate 16.000000%\n",
      "At minibatch 5400, batch loss 1.066460, batch nll 0.723669, batch error rate 16.000000%\n",
      "At minibatch 5500, batch loss 1.057012, batch nll 0.714719, batch error rate 24.000000%\n",
      "At minibatch 5600, batch loss 1.033236, batch nll 0.690639, batch error rate 16.000000%\n",
      "At minibatch 5700, batch loss 0.950622, batch nll 0.608049, batch error rate 20.000000%\n",
      "At minibatch 5800, batch loss 0.732338, batch nll 0.389835, batch error rate 12.000000%\n",
      "At minibatch 5900, batch loss 1.248787, batch nll 0.907209, batch error rate 36.000000%\n",
      "At minibatch 6000, batch loss 1.013713, batch nll 0.673092, batch error rate 24.000000%\n",
      "At minibatch 6100, batch loss 0.799227, batch nll 0.459413, batch error rate 20.000000%\n",
      "At minibatch 6200, batch loss 0.752141, batch nll 0.412783, batch error rate 20.000000%\n",
      "At minibatch 6300, batch loss 0.877205, batch nll 0.538811, batch error rate 24.000000%\n",
      "At minibatch 6400, batch loss 1.327540, batch nll 0.990041, batch error rate 32.000000%\n",
      "After epoch 4: valid_err_rate: 28.780000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 24.170000% averaged train nll: 0.686942 averaged train loss: 1.028073\n",
      "At minibatch 6500, batch loss 1.001886, batch nll 0.662602, batch error rate 20.000000%\n",
      "At minibatch 6600, batch loss 1.217708, batch nll 0.877762, batch error rate 32.000000%\n",
      "At minibatch 6700, batch loss 0.644855, batch nll 0.304289, batch error rate 8.000000%\n",
      "At minibatch 6800, batch loss 0.994265, batch nll 0.653243, batch error rate 28.000000%\n",
      "At minibatch 6900, batch loss 1.046673, batch nll 0.704794, batch error rate 28.000000%\n",
      "At minibatch 7000, batch loss 1.275861, batch nll 0.933758, batch error rate 28.000000%\n",
      "At minibatch 7100, batch loss 1.319465, batch nll 0.976751, batch error rate 28.000000%\n",
      "At minibatch 7200, batch loss 1.042278, batch nll 0.698952, batch error rate 16.000000%\n",
      "At minibatch 7300, batch loss 0.751807, batch nll 0.408019, batch error rate 16.000000%\n",
      "At minibatch 7400, batch loss 1.086945, batch nll 0.743337, batch error rate 24.000000%\n",
      "At minibatch 7500, batch loss 1.049099, batch nll 0.705228, batch error rate 16.000000%\n",
      "At minibatch 7600, batch loss 0.709932, batch nll 0.366125, batch error rate 8.000000%\n",
      "At minibatch 7700, batch loss 0.741225, batch nll 0.397417, batch error rate 12.000000%\n",
      "At minibatch 7800, batch loss 0.788186, batch nll 0.443855, batch error rate 16.000000%\n",
      "At minibatch 7900, batch loss 1.128856, batch nll 0.784439, batch error rate 32.000000%\n",
      "At minibatch 8000, batch loss 0.880060, batch nll 0.535785, batch error rate 12.000000%\n",
      "After epoch 5: valid_err_rate: 26.960000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 19.020000% averaged train nll: 0.548482 averaged train loss: 0.890917\n",
      "At minibatch 8100, batch loss 0.716963, batch nll 0.371441, batch error rate 8.000000%\n",
      "At minibatch 8200, batch loss 0.576958, batch nll 0.230010, batch error rate 12.000000%\n",
      "At minibatch 8300, batch loss 0.528039, batch nll 0.180308, batch error rate 4.000000%\n",
      "At minibatch 8400, batch loss 0.801358, batch nll 0.453173, batch error rate 28.000000%\n",
      "At minibatch 8500, batch loss 0.792629, batch nll 0.443644, batch error rate 20.000000%\n",
      "At minibatch 8600, batch loss 0.633669, batch nll 0.283981, batch error rate 12.000000%\n",
      "At minibatch 8700, batch loss 0.596699, batch nll 0.246505, batch error rate 8.000000%\n",
      "At minibatch 8800, batch loss 0.653553, batch nll 0.302742, batch error rate 4.000000%\n",
      "At minibatch 8900, batch loss 0.772903, batch nll 0.421196, batch error rate 12.000000%\n",
      "At minibatch 9000, batch loss 0.932854, batch nll 0.580149, batch error rate 20.000000%\n",
      "At minibatch 9100, batch loss 0.809403, batch nll 0.456702, batch error rate 12.000000%\n",
      "At minibatch 9200, batch loss 0.677461, batch nll 0.324599, batch error rate 4.000000%\n",
      "At minibatch 9300, batch loss 0.679431, batch nll 0.326383, batch error rate 8.000000%\n",
      "At minibatch 9400, batch loss 0.867911, batch nll 0.514581, batch error rate 20.000000%\n",
      "At minibatch 9500, batch loss 0.901146, batch nll 0.547649, batch error rate 12.000000%\n",
      "At minibatch 9600, batch loss 1.137172, batch nll 0.783648, batch error rate 20.000000%\n",
      "After epoch 6: valid_err_rate: 27.770000% currently going to do 8 epochs\n",
      "After epoch 6: averaged train_err_rate: 14.612500% averaged train nll: 0.429691 averaged train loss: 0.780164\n",
      "At minibatch 9700, batch loss 0.888121, batch nll 0.533173, batch error rate 12.000000%\n",
      "At minibatch 9800, batch loss 0.563717, batch nll 0.207650, batch error rate 4.000000%\n",
      "At minibatch 9900, batch loss 0.654724, batch nll 0.297687, batch error rate 12.000000%\n",
      "At minibatch 10000, batch loss 0.699642, batch nll 0.341559, batch error rate 12.000000%\n",
      "At minibatch 10100, batch loss 0.638926, batch nll 0.280270, batch error rate 8.000000%\n",
      "At minibatch 10200, batch loss 0.572775, batch nll 0.213688, batch error rate 4.000000%\n",
      "At minibatch 10300, batch loss 0.836587, batch nll 0.476874, batch error rate 16.000000%\n",
      "At minibatch 10400, batch loss 0.652301, batch nll 0.292037, batch error rate 12.000000%\n",
      "At minibatch 10500, batch loss 0.714745, batch nll 0.354172, batch error rate 8.000000%\n",
      "At minibatch 10600, batch loss 0.794520, batch nll 0.433551, batch error rate 8.000000%\n",
      "At minibatch 10700, batch loss 0.522877, batch nll 0.161676, batch error rate 4.000000%\n",
      "At minibatch 10800, batch loss 0.579470, batch nll 0.217584, batch error rate 8.000000%\n",
      "At minibatch 10900, batch loss 0.876354, batch nll 0.514186, batch error rate 16.000000%\n",
      "At minibatch 11000, batch loss 0.946163, batch nll 0.584057, batch error rate 24.000000%\n",
      "At minibatch 11100, batch loss 0.653349, batch nll 0.290907, batch error rate 12.000000%\n",
      "At minibatch 11200, batch loss 0.662916, batch nll 0.300552, batch error rate 12.000000%\n",
      "After epoch 7: valid_err_rate: 26.800000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 11.040000% averaged train nll: 0.334447 averaged train loss: 0.694013\n",
      "At minibatch 11300, batch loss 0.648972, batch nll 0.285860, batch error rate 8.000000%\n",
      "At minibatch 11400, batch loss 0.617369, batch nll 0.253737, batch error rate 4.000000%\n",
      "At minibatch 11500, batch loss 0.602721, batch nll 0.238735, batch error rate 8.000000%\n",
      "At minibatch 11600, batch loss 0.504280, batch nll 0.140082, batch error rate 4.000000%\n",
      "At minibatch 11700, batch loss 0.604472, batch nll 0.239857, batch error rate 4.000000%\n",
      "At minibatch 11800, batch loss 0.592687, batch nll 0.227532, batch error rate 8.000000%\n",
      "At minibatch 11900, batch loss 0.465161, batch nll 0.099656, batch error rate 0.000000%\n",
      "At minibatch 12000, batch loss 0.673156, batch nll 0.307152, batch error rate 12.000000%\n",
      "At minibatch 12100, batch loss 0.587716, batch nll 0.221446, batch error rate 4.000000%\n",
      "At minibatch 12200, batch loss 0.570805, batch nll 0.204248, batch error rate 8.000000%\n",
      "At minibatch 12300, batch loss 0.707150, batch nll 0.340405, batch error rate 12.000000%\n",
      "At minibatch 12400, batch loss 0.782230, batch nll 0.415231, batch error rate 12.000000%\n",
      "At minibatch 12500, batch loss 0.545467, batch nll 0.178198, batch error rate 4.000000%\n",
      "At minibatch 12600, batch loss 0.793183, batch nll 0.425773, batch error rate 20.000000%\n",
      "At minibatch 12700, batch loss 0.523667, batch nll 0.156184, batch error rate 4.000000%\n",
      "At minibatch 12800, batch loss 0.563728, batch nll 0.195973, batch error rate 4.000000%\n",
      "After epoch 8: valid_err_rate: 25.520000% currently going to do 13 epochs\n",
      "After epoch 8: averaged train_err_rate: 7.822500% averaged train nll: 0.251177 averaged train loss: 0.616791\n",
      "At minibatch 12900, batch loss 0.540432, batch nll 0.172452, batch error rate 4.000000%\n",
      "At minibatch 13000, batch loss 0.478647, batch nll 0.110540, batch error rate 4.000000%\n",
      "At minibatch 13100, batch loss 0.562469, batch nll 0.194528, batch error rate 8.000000%\n",
      "At minibatch 13200, batch loss 0.566381, batch nll 0.198291, batch error rate 8.000000%\n",
      "At minibatch 13300, batch loss 0.461336, batch nll 0.093090, batch error rate 0.000000%\n",
      "At minibatch 13400, batch loss 0.521863, batch nll 0.153737, batch error rate 4.000000%\n",
      "At minibatch 13500, batch loss 0.460778, batch nll 0.092404, batch error rate 0.000000%\n",
      "At minibatch 13600, batch loss 0.478981, batch nll 0.110718, batch error rate 0.000000%\n",
      "At minibatch 13700, batch loss 0.498333, batch nll 0.130089, batch error rate 4.000000%\n",
      "At minibatch 13800, batch loss 0.451758, batch nll 0.083094, batch error rate 0.000000%\n",
      "At minibatch 13900, batch loss 0.685210, batch nll 0.316596, batch error rate 8.000000%\n",
      "At minibatch 14000, batch loss 0.406942, batch nll 0.038233, batch error rate 0.000000%\n",
      "At minibatch 14100, batch loss 0.481445, batch nll 0.112402, batch error rate 4.000000%\n",
      "At minibatch 14200, batch loss 0.559774, batch nll 0.190496, batch error rate 4.000000%\n",
      "At minibatch 14300, batch loss 0.671505, batch nll 0.301953, batch error rate 16.000000%\n",
      "At minibatch 14400, batch loss 0.744678, batch nll 0.375027, batch error rate 12.000000%\n",
      "After epoch 9: valid_err_rate: 24.780000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 5.280000% averaged train nll: 0.186313 averaged train loss: 0.554797\n",
      "At minibatch 14500, batch loss 0.463442, batch nll 0.093954, batch error rate 4.000000%\n",
      "At minibatch 14600, batch loss 0.415190, batch nll 0.046061, batch error rate 0.000000%\n",
      "At minibatch 14700, batch loss 0.555771, batch nll 0.186855, batch error rate 8.000000%\n",
      "At minibatch 14800, batch loss 0.450794, batch nll 0.082123, batch error rate 0.000000%\n",
      "At minibatch 14900, batch loss 0.470493, batch nll 0.102055, batch error rate 0.000000%\n",
      "At minibatch 15000, batch loss 0.445780, batch nll 0.077478, batch error rate 0.000000%\n",
      "At minibatch 15100, batch loss 0.465787, batch nll 0.097772, batch error rate 0.000000%\n",
      "At minibatch 15200, batch loss 0.606300, batch nll 0.238327, batch error rate 8.000000%\n",
      "At minibatch 15300, batch loss 0.547422, batch nll 0.179551, batch error rate 4.000000%\n",
      "At minibatch 15400, batch loss 0.510581, batch nll 0.142820, batch error rate 8.000000%\n",
      "At minibatch 15500, batch loss 0.754700, batch nll 0.387150, batch error rate 8.000000%\n",
      "At minibatch 15600, batch loss 0.434894, batch nll 0.067568, batch error rate 0.000000%\n",
      "At minibatch 15700, batch loss 0.570769, batch nll 0.203537, batch error rate 8.000000%\n",
      "At minibatch 15800, batch loss 0.550933, batch nll 0.183740, batch error rate 4.000000%\n",
      "At minibatch 15900, batch loss 0.466816, batch nll 0.099674, batch error rate 0.000000%\n",
      "At minibatch 16000, batch loss 0.486978, batch nll 0.119787, batch error rate 4.000000%\n",
      "After epoch 10: valid_err_rate: 24.870000% currently going to do 14 epochs\n",
      "After epoch 10: averaged train_err_rate: 3.342500% averaged train nll: 0.136801 averaged train loss: 0.504873\n",
      "At minibatch 16100, batch loss 0.399362, batch nll 0.032649, batch error rate 0.000000%\n",
      "At minibatch 16200, batch loss 0.431104, batch nll 0.064982, batch error rate 0.000000%\n",
      "At minibatch 16300, batch loss 0.514938, batch nll 0.149394, batch error rate 4.000000%\n",
      "At minibatch 16400, batch loss 0.497263, batch nll 0.132312, batch error rate 0.000000%\n",
      "At minibatch 16500, batch loss 0.454919, batch nll 0.090384, batch error rate 0.000000%\n",
      "At minibatch 16600, batch loss 0.418366, batch nll 0.054238, batch error rate 0.000000%\n",
      "At minibatch 16700, batch loss 0.452085, batch nll 0.088430, batch error rate 0.000000%\n",
      "At minibatch 16800, batch loss 0.459928, batch nll 0.096627, batch error rate 0.000000%\n",
      "At minibatch 16900, batch loss 0.453161, batch nll 0.090138, batch error rate 4.000000%\n",
      "At minibatch 17000, batch loss 0.468549, batch nll 0.105914, batch error rate 0.000000%\n",
      "At minibatch 17100, batch loss 0.557819, batch nll 0.195435, batch error rate 8.000000%\n",
      "At minibatch 17200, batch loss 0.427375, batch nll 0.065118, batch error rate 4.000000%\n",
      "At minibatch 17300, batch loss 0.433303, batch nll 0.071174, batch error rate 0.000000%\n",
      "At minibatch 17400, batch loss 0.479028, batch nll 0.117136, batch error rate 0.000000%\n",
      "At minibatch 17500, batch loss 0.445924, batch nll 0.084148, batch error rate 4.000000%\n",
      "At minibatch 17600, batch loss 0.478752, batch nll 0.117156, batch error rate 4.000000%\n",
      "After epoch 11: valid_err_rate: 25.070000% currently going to do 14 epochs\n",
      "After epoch 11: averaged train_err_rate: 1.895000% averaged train nll: 0.101276 averaged train loss: 0.464973\n",
      "At minibatch 17700, batch loss 0.420160, batch nll 0.059310, batch error rate 0.000000%\n",
      "At minibatch 17800, batch loss 0.473557, batch nll 0.113314, batch error rate 4.000000%\n",
      "At minibatch 17900, batch loss 0.395562, batch nll 0.035888, batch error rate 0.000000%\n",
      "At minibatch 18000, batch loss 0.410486, batch nll 0.051365, batch error rate 0.000000%\n",
      "At minibatch 18100, batch loss 0.464850, batch nll 0.106403, batch error rate 4.000000%\n",
      "At minibatch 18200, batch loss 0.484482, batch nll 0.126427, batch error rate 0.000000%\n",
      "At minibatch 18300, batch loss 0.449103, batch nll 0.091647, batch error rate 0.000000%\n",
      "At minibatch 18400, batch loss 0.473998, batch nll 0.117073, batch error rate 4.000000%\n",
      "At minibatch 18500, batch loss 0.412493, batch nll 0.055986, batch error rate 0.000000%\n",
      "At minibatch 18600, batch loss 0.491565, batch nll 0.135565, batch error rate 0.000000%\n",
      "At minibatch 18700, batch loss 0.434325, batch nll 0.078713, batch error rate 4.000000%\n",
      "At minibatch 18800, batch loss 0.479984, batch nll 0.124577, batch error rate 4.000000%\n",
      "At minibatch 18900, batch loss 0.511824, batch nll 0.156757, batch error rate 8.000000%\n",
      "At minibatch 19000, batch loss 0.418378, batch nll 0.063653, batch error rate 0.000000%\n",
      "At minibatch 19100, batch loss 0.439204, batch nll 0.084838, batch error rate 4.000000%\n",
      "At minibatch 19200, batch loss 0.467677, batch nll 0.113616, batch error rate 4.000000%\n",
      "After epoch 12: valid_err_rate: 24.230000% currently going to do 19 epochs\n",
      "After epoch 12: averaged train_err_rate: 1.127500% averaged train nll: 0.079037 averaged train loss: 0.436302\n",
      "At minibatch 19300, batch loss 0.386141, batch nll 0.032796, batch error rate 0.000000%\n",
      "At minibatch 19400, batch loss 0.437669, batch nll 0.085108, batch error rate 0.000000%\n",
      "At minibatch 19500, batch loss 0.451403, batch nll 0.099620, batch error rate 0.000000%\n",
      "At minibatch 19600, batch loss 0.389090, batch nll 0.038093, batch error rate 0.000000%\n",
      "At minibatch 19700, batch loss 0.418438, batch nll 0.068077, batch error rate 0.000000%\n",
      "At minibatch 19800, batch loss 0.384227, batch nll 0.034620, batch error rate 0.000000%\n",
      "At minibatch 19900, batch loss 0.449037, batch nll 0.100050, batch error rate 0.000000%\n",
      "At minibatch 20000, batch loss 0.419418, batch nll 0.071029, batch error rate 0.000000%\n",
      "At minibatch 20100, batch loss 0.403382, batch nll 0.055508, batch error rate 0.000000%\n",
      "At minibatch 20200, batch loss 0.407344, batch nll 0.059838, batch error rate 0.000000%\n",
      "At minibatch 20300, batch loss 0.409502, batch nll 0.062532, batch error rate 0.000000%\n",
      "At minibatch 20400, batch loss 0.429520, batch nll 0.083019, batch error rate 4.000000%\n",
      "At minibatch 20500, batch loss 0.430201, batch nll 0.084074, batch error rate 0.000000%\n",
      "At minibatch 20600, batch loss 0.446025, batch nll 0.100212, batch error rate 4.000000%\n",
      "At minibatch 20700, batch loss 0.408311, batch nll 0.062676, batch error rate 0.000000%\n",
      "At minibatch 20800, batch loss 0.469354, batch nll 0.124023, batch error rate 4.000000%\n",
      "After epoch 13: valid_err_rate: 25.040000% currently going to do 19 epochs\n",
      "After epoch 13: averaged train_err_rate: 0.587500% averaged train nll: 0.063115 averaged train loss: 0.411997\n",
      "At minibatch 20900, batch loss 0.398269, batch nll 0.053644, batch error rate 0.000000%\n",
      "At minibatch 21000, batch loss 0.374061, batch nll 0.030095, batch error rate 0.000000%\n",
      "At minibatch 21100, batch loss 0.376675, batch nll 0.033400, batch error rate 0.000000%\n",
      "At minibatch 21200, batch loss 0.382545, batch nll 0.040072, batch error rate 0.000000%\n",
      "At minibatch 21300, batch loss 0.360342, batch nll 0.018463, batch error rate 0.000000%\n",
      "At minibatch 21400, batch loss 0.387485, batch nll 0.046347, batch error rate 0.000000%\n",
      "At minibatch 21500, batch loss 0.392828, batch nll 0.052346, batch error rate 0.000000%\n",
      "At minibatch 21600, batch loss 0.369784, batch nll 0.029843, batch error rate 0.000000%\n",
      "At minibatch 21700, batch loss 0.400117, batch nll 0.060661, batch error rate 0.000000%\n",
      "At minibatch 21800, batch loss 0.403637, batch nll 0.064629, batch error rate 0.000000%\n",
      "At minibatch 21900, batch loss 0.392071, batch nll 0.053481, batch error rate 0.000000%\n",
      "At minibatch 22000, batch loss 0.373372, batch nll 0.035135, batch error rate 0.000000%\n",
      "At minibatch 22100, batch loss 0.405342, batch nll 0.067561, batch error rate 0.000000%\n",
      "At minibatch 22200, batch loss 0.428699, batch nll 0.091259, batch error rate 0.000000%\n",
      "At minibatch 22300, batch loss 0.363751, batch nll 0.026602, batch error rate 0.000000%\n",
      "At minibatch 22400, batch loss 0.365213, batch nll 0.028406, batch error rate 0.000000%\n",
      "After epoch 14: valid_err_rate: 24.450000% currently going to do 19 epochs\n",
      "After epoch 14: averaged train_err_rate: 0.365000% averaged train nll: 0.054936 averaged train loss: 0.395339\n",
      "At minibatch 22500, batch loss 0.368745, batch nll 0.032658, batch error rate 0.000000%\n",
      "At minibatch 22600, batch loss 0.367490, batch nll 0.032122, batch error rate 0.000000%\n",
      "At minibatch 22700, batch loss 0.375973, batch nll 0.041262, batch error rate 0.000000%\n",
      "At minibatch 22800, batch loss 0.353004, batch nll 0.018884, batch error rate 0.000000%\n",
      "At minibatch 22900, batch loss 0.381588, batch nll 0.048089, batch error rate 0.000000%\n",
      "At minibatch 23000, batch loss 0.372932, batch nll 0.040006, batch error rate 0.000000%\n",
      "At minibatch 23100, batch loss 0.370835, batch nll 0.038494, batch error rate 0.000000%\n",
      "At minibatch 23200, batch loss 0.360900, batch nll 0.029071, batch error rate 0.000000%\n",
      "At minibatch 23300, batch loss 0.375957, batch nll 0.044560, batch error rate 0.000000%\n",
      "At minibatch 23400, batch loss 0.379736, batch nll 0.048813, batch error rate 0.000000%\n",
      "At minibatch 23500, batch loss 0.357947, batch nll 0.027432, batch error rate 0.000000%\n",
      "At minibatch 23600, batch loss 0.410872, batch nll 0.080757, batch error rate 0.000000%\n",
      "At minibatch 23700, batch loss 0.369427, batch nll 0.039636, batch error rate 0.000000%\n",
      "At minibatch 23800, batch loss 0.374017, batch nll 0.044606, batch error rate 0.000000%\n",
      "At minibatch 23900, batch loss 0.397083, batch nll 0.067966, batch error rate 0.000000%\n",
      "At minibatch 24000, batch loss 0.347181, batch nll 0.018352, batch error rate 0.000000%\n",
      "After epoch 15: valid_err_rate: 24.470000% currently going to do 19 epochs\n",
      "After epoch 15: averaged train_err_rate: 0.202500% averaged train nll: 0.048931 averaged train loss: 0.381116\n",
      "At minibatch 24100, batch loss 0.438442, batch nll 0.110235, batch error rate 4.000000%\n",
      "At minibatch 24200, batch loss 0.363789, batch nll 0.036156, batch error rate 0.000000%\n",
      "At minibatch 24300, batch loss 0.363853, batch nll 0.036843, batch error rate 0.000000%\n",
      "At minibatch 24400, batch loss 0.353710, batch nll 0.027273, batch error rate 0.000000%\n",
      "At minibatch 24500, batch loss 0.387926, batch nll 0.062064, batch error rate 4.000000%\n",
      "At minibatch 24600, batch loss 0.367959, batch nll 0.042658, batch error rate 0.000000%\n",
      "At minibatch 24700, batch loss 0.391116, batch nll 0.066258, batch error rate 4.000000%\n",
      "At minibatch 24800, batch loss 0.357114, batch nll 0.032766, batch error rate 0.000000%\n",
      "At minibatch 24900, batch loss 0.340460, batch nll 0.016561, batch error rate 0.000000%\n",
      "At minibatch 25000, batch loss 0.370330, batch nll 0.046891, batch error rate 0.000000%\n",
      "At minibatch 25100, batch loss 0.367908, batch nll 0.044818, batch error rate 0.000000%\n",
      "At minibatch 25200, batch loss 0.398131, batch nll 0.075332, batch error rate 4.000000%\n",
      "At minibatch 25300, batch loss 0.367765, batch nll 0.045293, batch error rate 0.000000%\n",
      "At minibatch 25400, batch loss 0.369332, batch nll 0.047171, batch error rate 0.000000%\n",
      "At minibatch 25500, batch loss 0.406755, batch nll 0.084871, batch error rate 0.000000%\n",
      "At minibatch 25600, batch loss 0.368007, batch nll 0.046403, batch error rate 0.000000%\n",
      "After epoch 16: valid_err_rate: 24.350000% currently going to do 19 epochs\n",
      "After epoch 16: averaged train_err_rate: 0.140000% averaged train nll: 0.045811 averaged train loss: 0.370472\n",
      "At minibatch 25700, batch loss 0.348759, batch nll 0.027657, batch error rate 0.000000%\n",
      "At minibatch 25800, batch loss 0.344991, batch nll 0.024404, batch error rate 0.000000%\n",
      "At minibatch 25900, batch loss 0.346415, batch nll 0.026389, batch error rate 0.000000%\n",
      "At minibatch 26000, batch loss 0.350415, batch nll 0.030893, batch error rate 0.000000%\n",
      "At minibatch 26100, batch loss 0.357391, batch nll 0.038280, batch error rate 0.000000%\n",
      "At minibatch 26200, batch loss 0.356851, batch nll 0.038150, batch error rate 0.000000%\n",
      "At minibatch 26300, batch loss 0.360206, batch nll 0.042024, batch error rate 0.000000%\n",
      "At minibatch 26400, batch loss 0.379439, batch nll 0.061616, batch error rate 0.000000%\n",
      "At minibatch 26500, batch loss 0.368647, batch nll 0.051197, batch error rate 0.000000%\n",
      "At minibatch 26600, batch loss 0.368393, batch nll 0.051371, batch error rate 0.000000%\n",
      "At minibatch 26700, batch loss 0.389865, batch nll 0.073174, batch error rate 0.000000%\n",
      "At minibatch 26800, batch loss 0.342158, batch nll 0.025731, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.389747, batch nll 0.073659, batch error rate 0.000000%\n",
      "At minibatch 27000, batch loss 0.369037, batch nll 0.053228, batch error rate 0.000000%\n",
      "At minibatch 27100, batch loss 0.352386, batch nll 0.036840, batch error rate 0.000000%\n",
      "At minibatch 27200, batch loss 0.421280, batch nll 0.106043, batch error rate 4.000000%\n",
      "After epoch 17: valid_err_rate: 24.540000% currently going to do 19 epochs\n",
      "After epoch 17: averaged train_err_rate: 0.115000% averaged train nll: 0.043977 averaged train loss: 0.362007\n",
      "At minibatch 27300, batch loss 0.367094, batch nll 0.052358, batch error rate 0.000000%\n",
      "At minibatch 27400, batch loss 0.335174, batch nll 0.020983, batch error rate 0.000000%\n",
      "At minibatch 27500, batch loss 0.350356, batch nll 0.036625, batch error rate 0.000000%\n",
      "At minibatch 27600, batch loss 0.338882, batch nll 0.025628, batch error rate 0.000000%\n",
      "At minibatch 27700, batch loss 0.424672, batch nll 0.111866, batch error rate 4.000000%\n",
      "At minibatch 27800, batch loss 0.333491, batch nll 0.021030, batch error rate 0.000000%\n",
      "At minibatch 27900, batch loss 0.357641, batch nll 0.045495, batch error rate 0.000000%\n",
      "At minibatch 28000, batch loss 0.339587, batch nll 0.027836, batch error rate 0.000000%\n",
      "At minibatch 28100, batch loss 0.324604, batch nll 0.013217, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.359906, batch nll 0.048775, batch error rate 0.000000%\n",
      "At minibatch 28300, batch loss 0.354911, batch nll 0.044078, batch error rate 0.000000%\n",
      "At minibatch 28400, batch loss 0.363389, batch nll 0.052794, batch error rate 0.000000%\n",
      "At minibatch 28500, batch loss 0.361074, batch nll 0.050781, batch error rate 0.000000%\n",
      "At minibatch 28600, batch loss 0.339177, batch nll 0.029094, batch error rate 0.000000%\n",
      "At minibatch 28700, batch loss 0.353486, batch nll 0.043669, batch error rate 0.000000%\n",
      "At minibatch 28800, batch loss 0.335008, batch nll 0.025427, batch error rate 0.000000%\n",
      "After epoch 18: valid_err_rate: 24.400000% currently going to do 19 epochs\n",
      "After epoch 18: averaged train_err_rate: 0.065000% averaged train nll: 0.042448 averaged train loss: 0.354421\n",
      "At minibatch 28900, batch loss 0.352858, batch nll 0.043729, batch error rate 0.000000%\n",
      "At minibatch 29000, batch loss 0.341276, batch nll 0.032579, batch error rate 0.000000%\n",
      "At minibatch 29100, batch loss 0.343505, batch nll 0.035231, batch error rate 0.000000%\n",
      "At minibatch 29200, batch loss 0.346187, batch nll 0.038264, batch error rate 0.000000%\n",
      "At minibatch 29300, batch loss 0.335521, batch nll 0.027997, batch error rate 0.000000%\n",
      "At minibatch 29400, batch loss 0.342027, batch nll 0.034863, batch error rate 0.000000%\n",
      "At minibatch 29500, batch loss 0.347470, batch nll 0.040649, batch error rate 0.000000%\n",
      "At minibatch 29600, batch loss 0.377800, batch nll 0.071312, batch error rate 0.000000%\n",
      "At minibatch 29700, batch loss 0.358916, batch nll 0.052731, batch error rate 0.000000%\n",
      "At minibatch 29800, batch loss 0.348815, batch nll 0.042949, batch error rate 0.000000%\n",
      "At minibatch 29900, batch loss 0.346563, batch nll 0.040977, batch error rate 0.000000%\n",
      "At minibatch 30000, batch loss 0.383863, batch nll 0.078539, batch error rate 4.000000%\n",
      "At minibatch 30100, batch loss 0.365566, batch nll 0.060487, batch error rate 0.000000%\n",
      "At minibatch 30200, batch loss 0.348647, batch nll 0.043784, batch error rate 0.000000%\n",
      "At minibatch 30300, batch loss 0.332950, batch nll 0.028236, batch error rate 0.000000%\n",
      "At minibatch 30400, batch loss 0.365213, batch nll 0.060642, batch error rate 0.000000%\n",
      "After epoch 19: valid_err_rate: 24.490000% currently going to do 19 epochs\n",
      "After epoch 19: averaged train_err_rate: 0.057500% averaged train nll: 0.041491 averaged train loss: 0.348160\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while e < number_of_epochs: # This loop goes over epochs\n",
    "    e += 1\n",
    "    \n",
    "    # First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar10_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = params[\"K\"]\n",
    "        lrate = params[\"lrate_const\"] * K / np.maximum(K, i)    # was 4e-3 * ...\n",
    "        momentum = params[\"momentum\"]\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        # print [p.get_value().ravel()[: 10] for p in model_parameters]\n",
    "        # print [p.get_value().ravel()[: 10] for p in velocities]\n",
    "        \n",
    "        train_loss.append((i, L))\n",
    "        train_erros.append((i, err_rate))\n",
    "        train_nll.append((i, nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate * 100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar10_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion + 1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i, val_error_rate))\n",
    "    \n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" \\\n",
    "          % (e,val_error_rate * 100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" \\\n",
    "          % (e,\n",
    "             np.mean(np.asarray(train_erros)[epoch_start_i :, 1]) * 100, \n",
    "             np.mean(np.asarray(train_nll)[epoch_start_i :, 1]),\n",
    "             np.mean(np.asarray(train_loss)[epoch_start_i :, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 12\n",
      "Test error rate is 25.340000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1ffa2a5510>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nphQMwzAMH/8PEWsgyslocdcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f201d3ee410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" % (best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" % (compute_error_rate(cifar10_test_stream) * 100.0,)\n",
    "\n",
    "subplot(2, 1, 1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:, 0], train_nll_a[:, 1], label = \"batch train nll\")\n",
    "legend()\n",
    "\n",
    "subplot(2, 1, 2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:, 0], train_erros_a[:, 1], label = \"batch train error rate\")\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:, 0], validation_errors_a[:, 1], label = \"validation error rate\", color = \"r\")\n",
    "ylim(0, 0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results:\n",
      "\n",
      " momentum | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
      "==========+=============++=================+============+===========+================+====================+\n",
      " 0.9      |  2e-3 * ... ||      34.410000% |         27 |        42 |     34.250000% |         20.372500% |\n",
      "          |  4e-3 * ... ||      32.910000% |         15 |        24 |     33.830000% |          9.650000% |\n",
      "          |  8e-3 * ... ||      32.050000% |         14 |        22 |     30.990000% |          0.352500% |\n",
      "          | 12e-3 * ... ||      32.450000% |         10 |        16 |     32.400000% |          1.422500% |\n",
      "          | 16e-3 * ... ||      31.460000% |          9 |        15 |     31.190000% |          2.002500% |\n",
      "          | 20e-3 * ... ||      31.210000% |         31 |        48 |     30.280000% |          0.005000% |\n",
      "          | 24e-3 * ... ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
      "          | 28e-3 * ... ||      31.240000% |         33 |        51 |     30.550000% |          0.010000% |\n",
      "          | 32e-3 * ... ||      32.760000% |         11 |        18 |     32.590000% |          5.222500% |\n",
      "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
      " 0.7      | 24e-3 * ... ||      31.470000% |          9 |        14 |     31.230000% |          4.807500% |\n",
      " 0.75     |             ||      31.410000% |         16 |        25 |     31.010000% |          0.082500% |\n",
      " 0.8      |             ||      30.950000% |          9 |        14 |     30.980000% |          1.715000% |\n",
      " 0.85     |             ||      31.090000% |         26 |        40 |     31.340000% |          0.007500% |\n",
      " 0.9      |             ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
      "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ''' Results:\n",
    "\n",
    " momentum | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
    "==========+=============++=================+============+===========+================+====================+\n",
    " 0.9      |  2e-3 * ... ||      34.410000% |         27 |        42 |     34.250000% |         20.372500% |\n",
    "          |  4e-3 * ... ||      32.910000% |         15 |        24 |     33.830000% |          9.650000% |\n",
    "          |  8e-3 * ... ||      32.050000% |         14 |        22 |     30.990000% |          0.352500% |\n",
    "          | 12e-3 * ... ||      32.450000% |         10 |        16 |     32.400000% |          1.422500% |\n",
    "          | 16e-3 * ... ||      31.460000% |          9 |        15 |     31.190000% |          2.002500% |\n",
    "          | 20e-3 * ... ||      31.210000% |         31 |        48 |     30.280000% |          0.005000% |\n",
    "          | 24e-3 * ... ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "          | 28e-3 * ... ||      31.240000% |         33 |        51 |     30.550000% |          0.010000% |\n",
    "          | 32e-3 * ... ||      32.760000% |         11 |        18 |     32.590000% |          5.222500% |\n",
    "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
    " 0.7      | 24e-3 * ... ||      31.470000% |          9 |        14 |     31.230000% |          4.807500% |\n",
    " 0.75     |             ||      31.410000% |         16 |        25 |     31.010000% |          0.082500% |\n",
    " 0.8      |             ||      30.950000% |          9 |        14 |     30.980000% |          1.715000% |\n",
    " 0.85     |             ||      31.090000% |         26 |        40 |     31.340000% |          0.007500% |\n",
    " 0.9      |             ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_fw3_hidden': 500, 'K': 3000, 'gauss': 0.025, 'momentum': 0.8, 'lrate_const': 0.024, 'num_filters_1': 50, 'num_filters_2': 75}\n"
     ]
    }
   ],
   "source": [
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results:\n",
      "\n",
      " K    | lrate_const | momentum | num_filters_1 | num_filters_2 | num_fw3_hidden | gauss || test error rate\n",
      "======+=============+==========+===============+===============+================+=======++=================\n",
      " 2000 |       0.004 |      0.9 |            10 |            25 |            500 | 0.05  ||        original\n",
      "------+-------------+----------+---------------+---------------+----------------+-------++-----------------\n",
      "      |       0.024 |      0.8 |               |            35 |                |       ||      30.080000%\n",
      "      |       0.024 |      0.8 |            15 |            50 |                |       ||      28.350000%\n",
      "      |       0.024 |      0.8 |            15 |            50 |                | 0.025 ||      27.530000%\n",
      "      |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.910000%\n",
      " 4000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      26.030000%\n",
      " 3000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.690000%\n"
     ]
    }
   ],
   "source": [
    "print ''' Results:\n",
    "\n",
    " K    | lrate_const | momentum | num_filters_1 | num_filters_2 | num_fw3_hidden | gauss || test error rate\n",
    "======+=============+==========+===============+===============+================+=======++=================\n",
    " 2000 |       0.004 |      0.9 |            10 |            25 |            500 | 0.05  ||        original\n",
    "------+-------------+----------+---------------+---------------+----------------+-------++-----------------\n",
    "      |       0.024 |      0.8 |               |            35 |                |       ||      30.080000%\n",
    "      |       0.024 |      0.8 |            15 |            50 |                |       ||      28.350000%\n",
    "      |       0.024 |      0.8 |            15 |            50 |                | 0.025 ||      27.530000%\n",
    "      |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.910000%\n",
    " 4000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      26.030000%\n",
    " 3000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.690000%\n",
    " 3000 |       0.024 |      0.8 |            50 |            75 |                | 0.025 ||      25.340000%'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
