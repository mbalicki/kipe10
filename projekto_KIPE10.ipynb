{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 23 days\n",
      "Vendor:  Continuum Analytics, Inc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package: mkl\n",
      "Message: trial mode expires in 23 days\n"
     ]
    }
   ],
   "source": [
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a parameters' dictionary for the sake of easy usage and fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\"K\" : 3000,               # was 2000\n",
    "          \"momentum\" : 0.8,         # was 0.9\n",
    "          \"lrate_const\" : 24e-3,    # was 4e-3\n",
    "          \"num_filters_1\" : 50,     # was 10\n",
    "          \"num_filters_2\" : 75,     # was 25\n",
    "          \"num_fw3_hidden\" : 500,   # was 500\n",
    "          \"gauss\" : 0.025}          # was 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will now build a convolutional network for the CIFAR-10 data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = ((ScaleAndShift, [2.0 / 255.0, -1], {\"which_sources\" : \"features\"}),\n",
    "                                (Cast, [np.float32], {\"which_sources\" : \"features\"}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset = slice(None, 40000))\n",
    "# this stream will shuffle the CIFAR-10 set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(cifar10_train,\n",
    "                                                 iteration_scheme = ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset = slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these don't do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(cifar10_validation,\n",
    "                                                      iteration_scheme = SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(cifar10_test,\n",
    "                                                iteration_scheme = SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are taken from https://github.com/mila-udem/blocks.\n",
    "class Constant():\n",
    "    '''Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    '''\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype = np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    '''Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    '''\n",
    "    def __init__(self, std = 1, mean = 0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size = shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    '''Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width / 2, mean + width / 2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    '''\n",
    "    def __init__(self, mean = 0., width = None, std = None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1 / 12 * width ^ 2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size = shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4(\"X\")\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix(\"Y\", dtype = \"uint8\")\n",
    "\n",
    "# The tag values are useful during debugging the creation of Theano graphs\n",
    "X_test_value, Y_test_value = next(cifar10_train_stream.get_epoch_iterator())\n",
    "\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values\n",
    "\n",
    "X.tag.test_value = X_test_value[: 3]\n",
    "Y.tag.test_value = Y_test_value[: 3]\n",
    "\n",
    "print \"X shape: %s\" % (X.tag.test_value.shape,)\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "# The first convolutional layer\n",
    "# The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = params[\"num_filters_1\"] # we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1, 3, 5, 5), dtype = \"float32\"),\n",
    "                    name = \"CW1\")\n",
    "# please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype = \"float32\"),\n",
    "                    name = \"CB1\")\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2, 2), ignore_border = True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "num_filters_2 = params[\"num_filters_2\"] # we will compute ten convolution filters in the first layer # was 25\n",
    "CW2 = theano.shared(np.zeros((num_filters_2, num_filters_1, 5, 5), dtype = \"float32\"),\n",
    "                   name = \"CW2\")\n",
    "CW2.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype = \"float32\"),\n",
    "                    name = \"CB2\")\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2, 2), ignore_border = True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "# Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden = params[\"num_fw3_hidden\"]\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW3\")\n",
    "FW3.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB3\")\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle(\"x\", 0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "num_fw4_hidden = 10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW4\")\n",
    "FW4.tag.initializer = IsotropicGaussian(params[\"gauss\"])\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB4\")\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle(\"x\", 0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis = 1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions, Y.ravel()).mean()\n",
    "nll = -theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1] == \"W\":\n",
    "        weight_decay += 1e-3 * (p ** 2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "# At this point stop computing test values\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "# svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar(\"lrate\", dtype = \"float32\")\n",
    "momentum = theano.tensor.scalar(\"momentum\", dtype = \"float32\")\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "# initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name = \"V_%s\" % (p.name,)) for p in model_parameters]\n",
    "\n",
    "for p, g, v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v, v_new), (p, p_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile theano functions\n",
    "\n",
    "# each call to train step will make one SGD step\n",
    "train_step = theano.function([X, Y, lrate, momentum], [cost, error_rate, nll, weight_decay], updates = updates)\n",
    "# each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X) != Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities to save values of parameters and to load them\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow = False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.490268, batch nll 1.864197, batch error rate 68.000000%\n",
      "At minibatch 200, batch loss 2.203887, batch nll 1.599607, batch error rate 56.000000%\n",
      "At minibatch 300, batch loss 2.085080, batch nll 1.501891, batch error rate 56.000000%\n",
      "At minibatch 400, batch loss 2.233510, batch nll 1.669094, batch error rate 68.000000%\n",
      "At minibatch 500, batch loss 2.063632, batch nll 1.516712, batch error rate 64.000000%\n",
      "At minibatch 600, batch loss 1.883293, batch nll 1.353465, batch error rate 60.000000%\n",
      "At minibatch 700, batch loss 1.825593, batch nll 1.312013, batch error rate 52.000000%\n",
      "At minibatch 800, batch loss 1.832383, batch nll 1.333512, batch error rate 52.000000%\n",
      "At minibatch 900, batch loss 1.856863, batch nll 1.371552, batch error rate 44.000000%\n",
      "At minibatch 1000, batch loss 2.057420, batch nll 1.584002, batch error rate 60.000000%\n",
      "At minibatch 1100, batch loss 2.013598, batch nll 1.552217, batch error rate 52.000000%\n",
      "At minibatch 1200, batch loss 1.637789, batch nll 1.186274, batch error rate 52.000000%\n",
      "At minibatch 1300, batch loss 2.132825, batch nll 1.691521, batch error rate 72.000000%\n",
      "At minibatch 1400, batch loss 1.537763, batch nll 1.106901, batch error rate 36.000000%\n",
      "At minibatch 1500, batch loss 1.688456, batch nll 1.266200, batch error rate 40.000000%\n",
      "At minibatch 1600, batch loss 1.271274, batch nll 0.856114, batch error rate 36.000000%\n",
      "After epoch 1: valid_err_rate: 42.480000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 54.752500% averaged train nll: 1.513385 averaged train loss: 2.023652\n",
      "At minibatch 1700, batch loss 1.686696, batch nll 1.277797, batch error rate 40.000000%\n",
      "At minibatch 1800, batch loss 1.392559, batch nll 0.991420, batch error rate 48.000000%\n",
      "At minibatch 1900, batch loss 1.632801, batch nll 1.237150, batch error rate 48.000000%\n",
      "At minibatch 2000, batch loss 2.013593, batch nll 1.623398, batch error rate 64.000000%\n",
      "At minibatch 2100, batch loss 1.583383, batch nll 1.198183, batch error rate 52.000000%\n",
      "At minibatch 2200, batch loss 1.625903, batch nll 1.245301, batch error rate 52.000000%\n",
      "At minibatch 2300, batch loss 1.822073, batch nll 1.447604, batch error rate 52.000000%\n",
      "At minibatch 2400, batch loss 1.718376, batch nll 1.348072, batch error rate 44.000000%\n",
      "At minibatch 2500, batch loss 2.187124, batch nll 1.820945, batch error rate 68.000000%\n",
      "At minibatch 2600, batch loss 1.616194, batch nll 1.252383, batch error rate 32.000000%\n",
      "At minibatch 2700, batch loss 1.606431, batch nll 1.245678, batch error rate 36.000000%\n",
      "At minibatch 2800, batch loss 1.727230, batch nll 1.369087, batch error rate 44.000000%\n",
      "At minibatch 2900, batch loss 1.714295, batch nll 1.360008, batch error rate 48.000000%\n",
      "At minibatch 3000, batch loss 1.008254, batch nll 0.655827, batch error rate 24.000000%\n",
      "At minibatch 3100, batch loss 1.737129, batch nll 1.387792, batch error rate 40.000000%\n",
      "At minibatch 3200, batch loss 1.141129, batch nll 0.793290, batch error rate 32.000000%\n",
      "After epoch 2: valid_err_rate: 35.640000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 40.120000% averaged train nll: 1.139546 averaged train loss: 1.514086\n",
      "At minibatch 3300, batch loss 1.096175, batch nll 0.748700, batch error rate 32.000000%\n",
      "At minibatch 3400, batch loss 1.042982, batch nll 0.696259, batch error rate 24.000000%\n",
      "At minibatch 3500, batch loss 1.346459, batch nll 1.000648, batch error rate 32.000000%\n",
      "At minibatch 3600, batch loss 1.364535, batch nll 1.020497, batch error rate 24.000000%\n",
      "At minibatch 3700, batch loss 1.404998, batch nll 1.061739, batch error rate 36.000000%\n",
      "At minibatch 3800, batch loss 1.492066, batch nll 1.150951, batch error rate 36.000000%\n",
      "At minibatch 3900, batch loss 1.656098, batch nll 1.317236, batch error rate 32.000000%\n",
      "At minibatch 4000, batch loss 1.222206, batch nll 0.885261, batch error rate 28.000000%\n",
      "At minibatch 4100, batch loss 1.361514, batch nll 1.026697, batch error rate 36.000000%\n",
      "At minibatch 4200, batch loss 0.906160, batch nll 0.573551, batch error rate 12.000000%\n",
      "At minibatch 4300, batch loss 1.170887, batch nll 0.840389, batch error rate 24.000000%\n",
      "At minibatch 4400, batch loss 0.990242, batch nll 0.662534, batch error rate 24.000000%\n",
      "At minibatch 4500, batch loss 1.168508, batch nll 0.842833, batch error rate 32.000000%\n",
      "At minibatch 4600, batch loss 1.368142, batch nll 1.045316, batch error rate 44.000000%\n",
      "At minibatch 4700, batch loss 1.106426, batch nll 0.784954, batch error rate 28.000000%\n",
      "At minibatch 4800, batch loss 1.311204, batch nll 0.991407, batch error rate 40.000000%\n",
      "After epoch 3: valid_err_rate: 30.380000% currently going to do 5 epochs\n",
      "After epoch 3: averaged train_err_rate: 32.322500% averaged train nll: 0.929371 averaged train loss: 1.265296\n",
      "At minibatch 4900, batch loss 1.054477, batch nll 0.734119, batch error rate 20.000000%\n",
      "At minibatch 5000, batch loss 1.369477, batch nll 1.049825, batch error rate 44.000000%\n",
      "At minibatch 5100, batch loss 0.722643, batch nll 0.403079, batch error rate 16.000000%\n",
      "At minibatch 5200, batch loss 1.153519, batch nll 0.835526, batch error rate 24.000000%\n",
      "At minibatch 5300, batch loss 1.302223, batch nll 0.984493, batch error rate 28.000000%\n",
      "At minibatch 5400, batch loss 1.119311, batch nll 0.802689, batch error rate 36.000000%\n",
      "At minibatch 5500, batch loss 1.192746, batch nll 0.877382, batch error rate 32.000000%\n",
      "At minibatch 5600, batch loss 0.996696, batch nll 0.681778, batch error rate 24.000000%\n",
      "At minibatch 5700, batch loss 1.022771, batch nll 0.708342, batch error rate 24.000000%\n",
      "At minibatch 5800, batch loss 0.726930, batch nll 0.413572, batch error rate 8.000000%\n",
      "At minibatch 5900, batch loss 1.221174, batch nll 0.909164, batch error rate 32.000000%\n",
      "At minibatch 6000, batch loss 1.016327, batch nll 0.704867, batch error rate 28.000000%\n",
      "At minibatch 6100, batch loss 0.809330, batch nll 0.499275, batch error rate 20.000000%\n",
      "At minibatch 6200, batch loss 0.869010, batch nll 0.559635, batch error rate 20.000000%\n",
      "At minibatch 6300, batch loss 0.871534, batch nll 0.563365, batch error rate 28.000000%\n",
      "At minibatch 6400, batch loss 1.350065, batch nll 1.042999, batch error rate 40.000000%\n",
      "After epoch 4: valid_err_rate: 29.070000% currently going to do 7 epochs\n",
      "After epoch 4: averaged train_err_rate: 27.040000% averaged train nll: 0.773441 averaged train loss: 1.088078\n",
      "At minibatch 6500, batch loss 1.255274, batch nll 0.947958, batch error rate 24.000000%\n",
      "At minibatch 6600, batch loss 1.131579, batch nll 0.824966, batch error rate 28.000000%\n",
      "At minibatch 6700, batch loss 0.732085, batch nll 0.425405, batch error rate 16.000000%\n",
      "At minibatch 6800, batch loss 1.041389, batch nll 0.735025, batch error rate 28.000000%\n",
      "At minibatch 6900, batch loss 0.946223, batch nll 0.640149, batch error rate 20.000000%\n",
      "At minibatch 7000, batch loss 1.135888, batch nll 0.830173, batch error rate 32.000000%\n",
      "At minibatch 7100, batch loss 1.171946, batch nll 0.866483, batch error rate 28.000000%\n",
      "At minibatch 7200, batch loss 1.234093, batch nll 0.929457, batch error rate 28.000000%\n",
      "At minibatch 7300, batch loss 0.730877, batch nll 0.426923, batch error rate 20.000000%\n",
      "At minibatch 7400, batch loss 1.024828, batch nll 0.721389, batch error rate 28.000000%\n",
      "At minibatch 7500, batch loss 1.277604, batch nll 0.974327, batch error rate 36.000000%\n",
      "At minibatch 7600, batch loss 1.019660, batch nll 0.716795, batch error rate 20.000000%\n",
      "At minibatch 7700, batch loss 0.938474, batch nll 0.635989, batch error rate 24.000000%\n",
      "At minibatch 7800, batch loss 0.926193, batch nll 0.623778, batch error rate 28.000000%\n",
      "At minibatch 7900, batch loss 1.007682, batch nll 0.706021, batch error rate 28.000000%\n",
      "At minibatch 8000, batch loss 0.955472, batch nll 0.654642, batch error rate 24.000000%\n",
      "After epoch 5: valid_err_rate: 26.030000% currently going to do 8 epochs\n",
      "After epoch 5: averaged train_err_rate: 23.607500% averaged train nll: 0.677420 averaged train loss: 0.981937\n",
      "At minibatch 8100, batch loss 0.931668, batch nll 0.630545, batch error rate 20.000000%\n",
      "At minibatch 8200, batch loss 0.822318, batch nll 0.520679, batch error rate 12.000000%\n",
      "At minibatch 8300, batch loss 0.787670, batch nll 0.485719, batch error rate 16.000000%\n",
      "At minibatch 8400, batch loss 1.081751, batch nll 0.780044, batch error rate 32.000000%\n",
      "At minibatch 8500, batch loss 0.832865, batch nll 0.531442, batch error rate 20.000000%\n",
      "At minibatch 8600, batch loss 0.842143, batch nll 0.540399, batch error rate 16.000000%\n",
      "At minibatch 8700, batch loss 0.659337, batch nll 0.357417, batch error rate 8.000000%\n",
      "At minibatch 8800, batch loss 0.855981, batch nll 0.554215, batch error rate 20.000000%\n",
      "At minibatch 8900, batch loss 0.877585, batch nll 0.575832, batch error rate 24.000000%\n",
      "At minibatch 9000, batch loss 0.979283, batch nll 0.677528, batch error rate 28.000000%\n",
      "At minibatch 9100, batch loss 0.951737, batch nll 0.650539, batch error rate 28.000000%\n",
      "At minibatch 9200, batch loss 0.932538, batch nll 0.631619, batch error rate 20.000000%\n",
      "At minibatch 9300, batch loss 0.872690, batch nll 0.571880, batch error rate 32.000000%\n",
      "At minibatch 9400, batch loss 1.067563, batch nll 0.767549, batch error rate 24.000000%\n",
      "At minibatch 9500, batch loss 0.915037, batch nll 0.615243, batch error rate 20.000000%\n",
      "At minibatch 9600, batch loss 1.514614, batch nll 1.215759, batch error rate 36.000000%\n",
      "After epoch 6: valid_err_rate: 25.160000% currently going to do 10 epochs\n",
      "After epoch 6: averaged train_err_rate: 20.997500% averaged train nll: 0.602267 averaged train loss: 0.903472\n",
      "At minibatch 9700, batch loss 0.904581, batch nll 0.605040, batch error rate 24.000000%\n",
      "At minibatch 9800, batch loss 0.595450, batch nll 0.295280, batch error rate 4.000000%\n",
      "At minibatch 9900, batch loss 0.856650, batch nll 0.556306, batch error rate 20.000000%\n",
      "At minibatch 10000, batch loss 0.936066, batch nll 0.635169, batch error rate 28.000000%\n",
      "At minibatch 10100, batch loss 0.991560, batch nll 0.690629, batch error rate 24.000000%\n",
      "At minibatch 10200, batch loss 0.789983, batch nll 0.489063, batch error rate 20.000000%\n",
      "At minibatch 10300, batch loss 1.184319, batch nll 0.883219, batch error rate 28.000000%\n",
      "At minibatch 10400, batch loss 1.031039, batch nll 0.729527, batch error rate 20.000000%\n",
      "At minibatch 10500, batch loss 1.024986, batch nll 0.723391, batch error rate 24.000000%\n",
      "At minibatch 10600, batch loss 1.187851, batch nll 0.886439, batch error rate 32.000000%\n",
      "At minibatch 10700, batch loss 0.668675, batch nll 0.367309, batch error rate 12.000000%\n",
      "At minibatch 10800, batch loss 0.688455, batch nll 0.386920, batch error rate 12.000000%\n",
      "At minibatch 10900, batch loss 0.861493, batch nll 0.560041, batch error rate 20.000000%\n",
      "At minibatch 11000, batch loss 0.971838, batch nll 0.670562, batch error rate 24.000000%\n",
      "At minibatch 11100, batch loss 0.806125, batch nll 0.505179, batch error rate 16.000000%\n",
      "At minibatch 11200, batch loss 0.711475, batch nll 0.410762, batch error rate 20.000000%\n",
      "After epoch 7: valid_err_rate: 23.650000% currently going to do 11 epochs\n",
      "After epoch 7: averaged train_err_rate: 18.815000% averaged train nll: 0.543423 averaged train loss: 0.844359\n",
      "At minibatch 11300, batch loss 0.657823, batch nll 0.356450, batch error rate 12.000000%\n",
      "At minibatch 11400, batch loss 0.692543, batch nll 0.390804, batch error rate 8.000000%\n",
      "At minibatch 11500, batch loss 0.890666, batch nll 0.588689, batch error rate 16.000000%\n",
      "At minibatch 11600, batch loss 0.571049, batch nll 0.269027, batch error rate 8.000000%\n",
      "At minibatch 11700, batch loss 0.904246, batch nll 0.601919, batch error rate 20.000000%\n",
      "At minibatch 11800, batch loss 0.839763, batch nll 0.537237, batch error rate 12.000000%\n",
      "At minibatch 11900, batch loss 0.546644, batch nll 0.244147, batch error rate 4.000000%\n",
      "At minibatch 12000, batch loss 1.022660, batch nll 0.720021, batch error rate 36.000000%\n",
      "At minibatch 12100, batch loss 0.801904, batch nll 0.498949, batch error rate 16.000000%\n",
      "At minibatch 12200, batch loss 1.011860, batch nll 0.708615, batch error rate 36.000000%\n",
      "At minibatch 12300, batch loss 0.910663, batch nll 0.607591, batch error rate 24.000000%\n",
      "At minibatch 12400, batch loss 1.269897, batch nll 0.967013, batch error rate 28.000000%\n",
      "At minibatch 12500, batch loss 0.568256, batch nll 0.265522, batch error rate 4.000000%\n",
      "At minibatch 12600, batch loss 0.981436, batch nll 0.678870, batch error rate 20.000000%\n",
      "At minibatch 12700, batch loss 0.613157, batch nll 0.310812, batch error rate 12.000000%\n",
      "At minibatch 12800, batch loss 0.745043, batch nll 0.442887, batch error rate 16.000000%\n",
      "After epoch 8: valid_err_rate: 24.460000% currently going to do 11 epochs\n",
      "After epoch 8: averaged train_err_rate: 16.912500% averaged train nll: 0.492897 averaged train loss: 0.795276\n",
      "At minibatch 12900, batch loss 0.580260, batch nll 0.277744, batch error rate 8.000000%\n",
      "At minibatch 13000, batch loss 0.686585, batch nll 0.383456, batch error rate 12.000000%\n",
      "At minibatch 13100, batch loss 0.724869, batch nll 0.421613, batch error rate 12.000000%\n",
      "At minibatch 13200, batch loss 0.831301, batch nll 0.527903, batch error rate 16.000000%\n",
      "At minibatch 13300, batch loss 0.531832, batch nll 0.228141, batch error rate 4.000000%\n",
      "At minibatch 13400, batch loss 0.651105, batch nll 0.347219, batch error rate 20.000000%\n",
      "At minibatch 13500, batch loss 0.498539, batch nll 0.194536, batch error rate 0.000000%\n",
      "At minibatch 13600, batch loss 1.125499, batch nll 0.821335, batch error rate 24.000000%\n",
      "At minibatch 13700, batch loss 0.646330, batch nll 0.342234, batch error rate 8.000000%\n",
      "At minibatch 13800, batch loss 0.635340, batch nll 0.331104, batch error rate 12.000000%\n",
      "At minibatch 13900, batch loss 0.782443, batch nll 0.478160, batch error rate 20.000000%\n",
      "At minibatch 14000, batch loss 0.449206, batch nll 0.144953, batch error rate 4.000000%\n",
      "At minibatch 14100, batch loss 0.556049, batch nll 0.251622, batch error rate 4.000000%\n",
      "At minibatch 14200, batch loss 0.775909, batch nll 0.471245, batch error rate 20.000000%\n",
      "At minibatch 14300, batch loss 1.248258, batch nll 0.943502, batch error rate 28.000000%\n",
      "At minibatch 14400, batch loss 1.119641, batch nll 0.814968, batch error rate 24.000000%\n",
      "After epoch 9: valid_err_rate: 22.490000% currently going to do 14 epochs\n",
      "After epoch 9: averaged train_err_rate: 15.352500% averaged train nll: 0.451195 averaged train loss: 0.755080\n",
      "At minibatch 14500, batch loss 0.654408, batch nll 0.349597, batch error rate 16.000000%\n",
      "At minibatch 14600, batch loss 0.606834, batch nll 0.301479, batch error rate 8.000000%\n",
      "At minibatch 14700, batch loss 0.748762, batch nll 0.443153, batch error rate 8.000000%\n",
      "At minibatch 14800, batch loss 0.906029, batch nll 0.599935, batch error rate 16.000000%\n",
      "At minibatch 14900, batch loss 0.747310, batch nll 0.441150, batch error rate 16.000000%\n",
      "At minibatch 15000, batch loss 0.753954, batch nll 0.447391, batch error rate 12.000000%\n",
      "At minibatch 15100, batch loss 0.601480, batch nll 0.294546, batch error rate 4.000000%\n",
      "At minibatch 15200, batch loss 0.806072, batch nll 0.498894, batch error rate 16.000000%\n",
      "At minibatch 15300, batch loss 0.897997, batch nll 0.590524, batch error rate 16.000000%\n",
      "At minibatch 15400, batch loss 0.699144, batch nll 0.391851, batch error rate 20.000000%\n",
      "At minibatch 15500, batch loss 1.053875, batch nll 0.746547, batch error rate 20.000000%\n",
      "At minibatch 15600, batch loss 0.726165, batch nll 0.418777, batch error rate 12.000000%\n",
      "At minibatch 15700, batch loss 0.672957, batch nll 0.365620, batch error rate 12.000000%\n",
      "At minibatch 15800, batch loss 0.906177, batch nll 0.598943, batch error rate 20.000000%\n",
      "At minibatch 15900, batch loss 0.612610, batch nll 0.305166, batch error rate 8.000000%\n",
      "At minibatch 16000, batch loss 0.782797, batch nll 0.475443, batch error rate 16.000000%\n",
      "After epoch 10: valid_err_rate: 22.020000% currently going to do 16 epochs\n",
      "After epoch 10: averaged train_err_rate: 13.797500% averaged train nll: 0.414606 averaged train loss: 0.721240\n",
      "At minibatch 16100, batch loss 0.674878, batch nll 0.367161, batch error rate 12.000000%\n",
      "At minibatch 16200, batch loss 0.699547, batch nll 0.391399, batch error rate 16.000000%\n",
      "At minibatch 16300, batch loss 0.688261, batch nll 0.379916, batch error rate 12.000000%\n",
      "At minibatch 16400, batch loss 0.842925, batch nll 0.534420, batch error rate 16.000000%\n",
      "At minibatch 16500, batch loss 0.577997, batch nll 0.269141, batch error rate 4.000000%\n",
      "At minibatch 16600, batch loss 0.504789, batch nll 0.195676, batch error rate 8.000000%\n",
      "At minibatch 16700, batch loss 0.808139, batch nll 0.498699, batch error rate 16.000000%\n",
      "At minibatch 16800, batch loss 0.740408, batch nll 0.430827, batch error rate 12.000000%\n",
      "At minibatch 16900, batch loss 0.526523, batch nll 0.216559, batch error rate 4.000000%\n",
      "At minibatch 17000, batch loss 0.663672, batch nll 0.353408, batch error rate 8.000000%\n",
      "At minibatch 17100, batch loss 0.909008, batch nll 0.598643, batch error rate 24.000000%\n",
      "At minibatch 17200, batch loss 0.608614, batch nll 0.298146, batch error rate 8.000000%\n",
      "At minibatch 17300, batch loss 0.496854, batch nll 0.186268, batch error rate 12.000000%\n",
      "At minibatch 17400, batch loss 0.714642, batch nll 0.403907, batch error rate 24.000000%\n",
      "At minibatch 17500, batch loss 0.712001, batch nll 0.400839, batch error rate 12.000000%\n",
      "At minibatch 17600, batch loss 0.673217, batch nll 0.362103, batch error rate 12.000000%\n",
      "After epoch 11: valid_err_rate: 22.540000% currently going to do 16 epochs\n",
      "After epoch 11: averaged train_err_rate: 12.542500% averaged train nll: 0.375261 averaged train loss: 0.684792\n",
      "At minibatch 17700, batch loss 0.747656, batch nll 0.436121, batch error rate 24.000000%\n",
      "At minibatch 17800, batch loss 0.633734, batch nll 0.321842, batch error rate 8.000000%\n",
      "At minibatch 17900, batch loss 0.621309, batch nll 0.308979, batch error rate 12.000000%\n",
      "At minibatch 18000, batch loss 0.488083, batch nll 0.175578, batch error rate 4.000000%\n",
      "At minibatch 18100, batch loss 0.643788, batch nll 0.331117, batch error rate 8.000000%\n",
      "At minibatch 18200, batch loss 0.780593, batch nll 0.467786, batch error rate 12.000000%\n",
      "At minibatch 18300, batch loss 0.751727, batch nll 0.438649, batch error rate 16.000000%\n",
      "At minibatch 18400, batch loss 0.661228, batch nll 0.348031, batch error rate 12.000000%\n",
      "At minibatch 18500, batch loss 0.564724, batch nll 0.251293, batch error rate 8.000000%\n",
      "At minibatch 18600, batch loss 0.622859, batch nll 0.309193, batch error rate 8.000000%\n",
      "At minibatch 18700, batch loss 0.541261, batch nll 0.227315, batch error rate 8.000000%\n",
      "At minibatch 18800, batch loss 0.873183, batch nll 0.559105, batch error rate 24.000000%\n",
      "At minibatch 18900, batch loss 0.535849, batch nll 0.221937, batch error rate 4.000000%\n",
      "At minibatch 19000, batch loss 0.621709, batch nll 0.307543, batch error rate 4.000000%\n",
      "At minibatch 19100, batch loss 0.668776, batch nll 0.354639, batch error rate 16.000000%\n",
      "At minibatch 19200, batch loss 0.704181, batch nll 0.389954, batch error rate 12.000000%\n",
      "After epoch 12: valid_err_rate: 22.520000% currently going to do 16 epochs\n",
      "After epoch 12: averaged train_err_rate: 11.472500% averaged train nll: 0.345576 averaged train loss: 0.658697\n",
      "At minibatch 19300, batch loss 0.546933, batch nll 0.232350, batch error rate 8.000000%\n",
      "At minibatch 19400, batch loss 0.760759, batch nll 0.445970, batch error rate 20.000000%\n",
      "At minibatch 19500, batch loss 0.753062, batch nll 0.437869, batch error rate 12.000000%\n",
      "At minibatch 19600, batch loss 0.715910, batch nll 0.400380, batch error rate 16.000000%\n",
      "At minibatch 19700, batch loss 0.762221, batch nll 0.446589, batch error rate 24.000000%\n",
      "At minibatch 19800, batch loss 0.571373, batch nll 0.255534, batch error rate 0.000000%\n",
      "At minibatch 19900, batch loss 0.888574, batch nll 0.572546, batch error rate 24.000000%\n",
      "At minibatch 20000, batch loss 0.631096, batch nll 0.315011, batch error rate 12.000000%\n",
      "At minibatch 20100, batch loss 0.553996, batch nll 0.237764, batch error rate 12.000000%\n",
      "At minibatch 20200, batch loss 0.623756, batch nll 0.307425, batch error rate 8.000000%\n",
      "At minibatch 20300, batch loss 0.587403, batch nll 0.270766, batch error rate 12.000000%\n",
      "At minibatch 20400, batch loss 0.784336, batch nll 0.467536, batch error rate 16.000000%\n",
      "At minibatch 20500, batch loss 0.669855, batch nll 0.352821, batch error rate 12.000000%\n",
      "At minibatch 20600, batch loss 0.455909, batch nll 0.138778, batch error rate 0.000000%\n",
      "At minibatch 20700, batch loss 0.742068, batch nll 0.424597, batch error rate 12.000000%\n",
      "At minibatch 20800, batch loss 0.750341, batch nll 0.432866, batch error rate 16.000000%\n",
      "After epoch 13: valid_err_rate: 22.540000% currently going to do 16 epochs\n",
      "After epoch 13: averaged train_err_rate: 10.287500% averaged train nll: 0.320445 averaged train loss: 0.636521\n",
      "At minibatch 20900, batch loss 0.593611, batch nll 0.275719, batch error rate 8.000000%\n",
      "At minibatch 21000, batch loss 0.405370, batch nll 0.087245, batch error rate 0.000000%\n",
      "At minibatch 21100, batch loss 0.584152, batch nll 0.265651, batch error rate 12.000000%\n",
      "At minibatch 21200, batch loss 0.646874, batch nll 0.328203, batch error rate 12.000000%\n",
      "At minibatch 21300, batch loss 0.478877, batch nll 0.159914, batch error rate 8.000000%\n",
      "At minibatch 21400, batch loss 0.502560, batch nll 0.183477, batch error rate 4.000000%\n",
      "At minibatch 21500, batch loss 0.697168, batch nll 0.378125, batch error rate 16.000000%\n",
      "At minibatch 21600, batch loss 0.517230, batch nll 0.197842, batch error rate 4.000000%\n",
      "At minibatch 21700, batch loss 0.602365, batch nll 0.282981, batch error rate 4.000000%\n",
      "At minibatch 21800, batch loss 0.801192, batch nll 0.481456, batch error rate 16.000000%\n",
      "At minibatch 21900, batch loss 0.555661, batch nll 0.235841, batch error rate 4.000000%\n",
      "At minibatch 22000, batch loss 0.608521, batch nll 0.288488, batch error rate 12.000000%\n",
      "At minibatch 22100, batch loss 0.635623, batch nll 0.315387, batch error rate 4.000000%\n",
      "At minibatch 22200, batch loss 0.657338, batch nll 0.337012, batch error rate 8.000000%\n",
      "At minibatch 22300, batch loss 0.411082, batch nll 0.090486, batch error rate 4.000000%\n",
      "At minibatch 22400, batch loss 0.511060, batch nll 0.190402, batch error rate 4.000000%\n",
      "After epoch 14: valid_err_rate: 21.430000% currently going to do 22 epochs\n",
      "After epoch 14: averaged train_err_rate: 9.187500% averaged train nll: 0.290497 averaged train loss: 0.609802\n",
      "At minibatch 22500, batch loss 0.660850, batch nll 0.339976, batch error rate 12.000000%\n",
      "At minibatch 22600, batch loss 0.468779, batch nll 0.147760, batch error rate 4.000000%\n",
      "At minibatch 22700, batch loss 0.459466, batch nll 0.138203, batch error rate 0.000000%\n",
      "At minibatch 22800, batch loss 0.440900, batch nll 0.119459, batch error rate 4.000000%\n",
      "At minibatch 22900, batch loss 0.765574, batch nll 0.443921, batch error rate 20.000000%\n",
      "At minibatch 23000, batch loss 0.577905, batch nll 0.256017, batch error rate 8.000000%\n",
      "At minibatch 23100, batch loss 0.571784, batch nll 0.249773, batch error rate 8.000000%\n",
      "At minibatch 23200, batch loss 0.407701, batch nll 0.085644, batch error rate 4.000000%\n",
      "At minibatch 23300, batch loss 0.547747, batch nll 0.225631, batch error rate 4.000000%\n",
      "At minibatch 23400, batch loss 0.684773, batch nll 0.362532, batch error rate 16.000000%\n",
      "At minibatch 23500, batch loss 0.502686, batch nll 0.180018, batch error rate 12.000000%\n",
      "At minibatch 23600, batch loss 0.652919, batch nll 0.330058, batch error rate 16.000000%\n",
      "At minibatch 23700, batch loss 0.626790, batch nll 0.303642, batch error rate 8.000000%\n",
      "At minibatch 23800, batch loss 0.675163, batch nll 0.351752, batch error rate 12.000000%\n",
      "At minibatch 23900, batch loss 0.682442, batch nll 0.359004, batch error rate 12.000000%\n",
      "At minibatch 24000, batch loss 0.437943, batch nll 0.114460, batch error rate 4.000000%\n",
      "After epoch 15: valid_err_rate: 21.330000% currently going to do 23 epochs\n",
      "After epoch 15: averaged train_err_rate: 8.247500% averaged train nll: 0.266248 averaged train loss: 0.588385\n",
      "At minibatch 24100, batch loss 0.697569, batch nll 0.373821, batch error rate 16.000000%\n",
      "At minibatch 24200, batch loss 0.517334, batch nll 0.193439, batch error rate 4.000000%\n",
      "At minibatch 24300, batch loss 0.505826, batch nll 0.181743, batch error rate 8.000000%\n",
      "At minibatch 24400, batch loss 0.654040, batch nll 0.329833, batch error rate 12.000000%\n",
      "At minibatch 24500, batch loss 0.740699, batch nll 0.416243, batch error rate 8.000000%\n",
      "At minibatch 24600, batch loss 0.649145, batch nll 0.324595, batch error rate 12.000000%\n",
      "At minibatch 24700, batch loss 0.890633, batch nll 0.565848, batch error rate 12.000000%\n",
      "At minibatch 24800, batch loss 0.518136, batch nll 0.193156, batch error rate 4.000000%\n",
      "At minibatch 24900, batch loss 0.474742, batch nll 0.149796, batch error rate 4.000000%\n",
      "At minibatch 25000, batch loss 0.510375, batch nll 0.185259, batch error rate 12.000000%\n",
      "At minibatch 25100, batch loss 0.437727, batch nll 0.112563, batch error rate 0.000000%\n",
      "At minibatch 25200, batch loss 0.443465, batch nll 0.118129, batch error rate 0.000000%\n",
      "At minibatch 25300, batch loss 0.569998, batch nll 0.244422, batch error rate 8.000000%\n",
      "At minibatch 25400, batch loss 0.481290, batch nll 0.155675, batch error rate 0.000000%\n",
      "At minibatch 25500, batch loss 0.632293, batch nll 0.306665, batch error rate 8.000000%\n",
      "At minibatch 25600, batch loss 0.537041, batch nll 0.211342, batch error rate 12.000000%\n",
      "After epoch 16: valid_err_rate: 20.520000% currently going to do 25 epochs\n",
      "After epoch 16: averaged train_err_rate: 7.532500% averaged train nll: 0.247453 averaged train loss: 0.572250\n",
      "At minibatch 25700, batch loss 0.508027, batch nll 0.182179, batch error rate 4.000000%\n",
      "At minibatch 25800, batch loss 0.645379, batch nll 0.319351, batch error rate 8.000000%\n",
      "At minibatch 25900, batch loss 0.460005, batch nll 0.133854, batch error rate 0.000000%\n",
      "At minibatch 26000, batch loss 0.581896, batch nll 0.255631, batch error rate 12.000000%\n",
      "At minibatch 26100, batch loss 0.724658, batch nll 0.398108, batch error rate 20.000000%\n",
      "At minibatch 26200, batch loss 0.557600, batch nll 0.230888, batch error rate 4.000000%\n",
      "At minibatch 26300, batch loss 0.457497, batch nll 0.130680, batch error rate 0.000000%\n",
      "At minibatch 26400, batch loss 0.666034, batch nll 0.339176, batch error rate 12.000000%\n",
      "At minibatch 26500, batch loss 0.477241, batch nll 0.150337, batch error rate 4.000000%\n",
      "At minibatch 26600, batch loss 0.680307, batch nll 0.353330, batch error rate 16.000000%\n",
      "At minibatch 26700, batch loss 0.714931, batch nll 0.387899, batch error rate 8.000000%\n",
      "At minibatch 26800, batch loss 0.374530, batch nll 0.047413, batch error rate 0.000000%\n",
      "At minibatch 26900, batch loss 0.628363, batch nll 0.301048, batch error rate 16.000000%\n",
      "At minibatch 27000, batch loss 0.552393, batch nll 0.225016, batch error rate 8.000000%\n",
      "At minibatch 27100, batch loss 0.566469, batch nll 0.239059, batch error rate 8.000000%\n",
      "At minibatch 27200, batch loss 0.659541, batch nll 0.332009, batch error rate 8.000000%\n",
      "After epoch 17: valid_err_rate: 20.690000% currently going to do 25 epochs\n",
      "After epoch 17: averaged train_err_rate: 6.692500% averaged train nll: 0.228713 averaged train loss: 0.555461\n",
      "At minibatch 27300, batch loss 0.517125, batch nll 0.189421, batch error rate 8.000000%\n",
      "At minibatch 27400, batch loss 0.468072, batch nll 0.140256, batch error rate 8.000000%\n",
      "At minibatch 27500, batch loss 0.559768, batch nll 0.231827, batch error rate 12.000000%\n",
      "At minibatch 27600, batch loss 0.492912, batch nll 0.164826, batch error rate 0.000000%\n",
      "At minibatch 27700, batch loss 0.674671, batch nll 0.346425, batch error rate 8.000000%\n",
      "At minibatch 27800, batch loss 0.386223, batch nll 0.057854, batch error rate 0.000000%\n",
      "At minibatch 27900, batch loss 0.568500, batch nll 0.239878, batch error rate 8.000000%\n",
      "At minibatch 28000, batch loss 0.589017, batch nll 0.260315, batch error rate 8.000000%\n",
      "At minibatch 28100, batch loss 0.426113, batch nll 0.097419, batch error rate 0.000000%\n",
      "At minibatch 28200, batch loss 0.558578, batch nll 0.229738, batch error rate 8.000000%\n",
      "At minibatch 28300, batch loss 0.623159, batch nll 0.294247, batch error rate 4.000000%\n",
      "At minibatch 28400, batch loss 0.605892, batch nll 0.276843, batch error rate 8.000000%\n",
      "At minibatch 28500, batch loss 0.659689, batch nll 0.330647, batch error rate 16.000000%\n",
      "At minibatch 28600, batch loss 0.488543, batch nll 0.159481, batch error rate 8.000000%\n",
      "At minibatch 28700, batch loss 0.466762, batch nll 0.137708, batch error rate 4.000000%\n",
      "At minibatch 28800, batch loss 0.466748, batch nll 0.137575, batch error rate 4.000000%\n",
      "After epoch 18: valid_err_rate: 20.840000% currently going to do 25 epochs\n",
      "After epoch 18: averaged train_err_rate: 5.965000% averaged train nll: 0.209037 averaged train loss: 0.537574\n",
      "At minibatch 28900, batch loss 0.625920, batch nll 0.296667, batch error rate 8.000000%\n",
      "At minibatch 29000, batch loss 0.557808, batch nll 0.228489, batch error rate 8.000000%\n",
      "At minibatch 29100, batch loss 0.469414, batch nll 0.140004, batch error rate 4.000000%\n",
      "At minibatch 29200, batch loss 0.492074, batch nll 0.162582, batch error rate 4.000000%\n",
      "At minibatch 29300, batch loss 0.527387, batch nll 0.197801, batch error rate 12.000000%\n",
      "At minibatch 29400, batch loss 0.538863, batch nll 0.209195, batch error rate 8.000000%\n",
      "At minibatch 29500, batch loss 0.502815, batch nll 0.173039, batch error rate 8.000000%\n",
      "At minibatch 29600, batch loss 0.721917, batch nll 0.391988, batch error rate 16.000000%\n",
      "At minibatch 29700, batch loss 0.618071, batch nll 0.288156, batch error rate 8.000000%\n",
      "At minibatch 29800, batch loss 0.571017, batch nll 0.241008, batch error rate 8.000000%\n",
      "At minibatch 29900, batch loss 0.596550, batch nll 0.266479, batch error rate 16.000000%\n",
      "At minibatch 30000, batch loss 0.462106, batch nll 0.131939, batch error rate 4.000000%\n",
      "At minibatch 30100, batch loss 0.544570, batch nll 0.214331, batch error rate 8.000000%\n",
      "At minibatch 30200, batch loss 0.521004, batch nll 0.190691, batch error rate 8.000000%\n",
      "At minibatch 30300, batch loss 0.434891, batch nll 0.104412, batch error rate 4.000000%\n",
      "At minibatch 30400, batch loss 0.552749, batch nll 0.222179, batch error rate 8.000000%\n",
      "After epoch 19: valid_err_rate: 20.990000% currently going to do 25 epochs\n",
      "After epoch 19: averaged train_err_rate: 5.462500% averaged train nll: 0.196404 averaged train loss: 0.526244\n",
      "At minibatch 30500, batch loss 0.521679, batch nll 0.191016, batch error rate 4.000000%\n",
      "At minibatch 30600, batch loss 0.580898, batch nll 0.250089, batch error rate 4.000000%\n",
      "At minibatch 30700, batch loss 0.710561, batch nll 0.379713, batch error rate 16.000000%\n",
      "At minibatch 30800, batch loss 0.429903, batch nll 0.098949, batch error rate 0.000000%\n",
      "At minibatch 30900, batch loss 0.494350, batch nll 0.163344, batch error rate 4.000000%\n",
      "At minibatch 31000, batch loss 0.811361, batch nll 0.480228, batch error rate 4.000000%\n",
      "At minibatch 31100, batch loss 0.492843, batch nll 0.161643, batch error rate 4.000000%\n",
      "At minibatch 31200, batch loss 0.557023, batch nll 0.225888, batch error rate 8.000000%\n",
      "At minibatch 31300, batch loss 0.446489, batch nll 0.115268, batch error rate 4.000000%\n",
      "At minibatch 31400, batch loss 0.540525, batch nll 0.209287, batch error rate 4.000000%\n",
      "At minibatch 31500, batch loss 0.601563, batch nll 0.270360, batch error rate 4.000000%\n",
      "At minibatch 31600, batch loss 0.722854, batch nll 0.391580, batch error rate 12.000000%\n",
      "At minibatch 31700, batch loss 0.491115, batch nll 0.159847, batch error rate 8.000000%\n",
      "At minibatch 31800, batch loss 0.544884, batch nll 0.213497, batch error rate 8.000000%\n",
      "At minibatch 31900, batch loss 0.480246, batch nll 0.148737, batch error rate 4.000000%\n",
      "At minibatch 32000, batch loss 0.489607, batch nll 0.158046, batch error rate 4.000000%\n",
      "After epoch 20: valid_err_rate: 20.940000% currently going to do 25 epochs\n",
      "After epoch 20: averaged train_err_rate: 4.852500% averaged train nll: 0.180427 averaged train loss: 0.511547\n",
      "At minibatch 32100, batch loss 0.447106, batch nll 0.115445, batch error rate 0.000000%\n",
      "At minibatch 32200, batch loss 0.549242, batch nll 0.217553, batch error rate 4.000000%\n",
      "At minibatch 32300, batch loss 0.613054, batch nll 0.281276, batch error rate 12.000000%\n",
      "At minibatch 32400, batch loss 0.523952, batch nll 0.192183, batch error rate 4.000000%\n",
      "At minibatch 32500, batch loss 0.426152, batch nll 0.094389, batch error rate 0.000000%\n",
      "At minibatch 32600, batch loss 0.534153, batch nll 0.202404, batch error rate 4.000000%\n",
      "At minibatch 32700, batch loss 0.451633, batch nll 0.119804, batch error rate 0.000000%\n",
      "At minibatch 32800, batch loss 0.572397, batch nll 0.240469, batch error rate 8.000000%\n",
      "At minibatch 32900, batch loss 0.481435, batch nll 0.149530, batch error rate 4.000000%\n",
      "At minibatch 33000, batch loss 0.551178, batch nll 0.219300, batch error rate 4.000000%\n",
      "At minibatch 33100, batch loss 0.464396, batch nll 0.132508, batch error rate 4.000000%\n",
      "At minibatch 33200, batch loss 0.438643, batch nll 0.106714, batch error rate 4.000000%\n",
      "At minibatch 33300, batch loss 0.438968, batch nll 0.106986, batch error rate 4.000000%\n",
      "At minibatch 33400, batch loss 0.654045, batch nll 0.322013, batch error rate 12.000000%\n",
      "At minibatch 33500, batch loss 0.583579, batch nll 0.251431, batch error rate 8.000000%\n",
      "At minibatch 33600, batch loss 0.484414, batch nll 0.152255, batch error rate 0.000000%\n",
      "After epoch 21: valid_err_rate: 20.930000% currently going to do 25 epochs\n",
      "After epoch 21: averaged train_err_rate: 4.385000% averaged train nll: 0.166549 averaged train loss: 0.498407\n",
      "At minibatch 33700, batch loss 0.443557, batch nll 0.111348, batch error rate 0.000000%\n",
      "At minibatch 33800, batch loss 0.376897, batch nll 0.044647, batch error rate 0.000000%\n",
      "At minibatch 33900, batch loss 0.525106, batch nll 0.192908, batch error rate 4.000000%\n",
      "At minibatch 34000, batch loss 0.501522, batch nll 0.169181, batch error rate 4.000000%\n",
      "At minibatch 34100, batch loss 0.405902, batch nll 0.073489, batch error rate 0.000000%\n",
      "At minibatch 34200, batch loss 0.786197, batch nll 0.453824, batch error rate 20.000000%\n",
      "At minibatch 34300, batch loss 0.525215, batch nll 0.192815, batch error rate 4.000000%\n",
      "At minibatch 34400, batch loss 0.440030, batch nll 0.107541, batch error rate 0.000000%\n",
      "At minibatch 34500, batch loss 0.609293, batch nll 0.276743, batch error rate 8.000000%\n",
      "At minibatch 34600, batch loss 0.422998, batch nll 0.090453, batch error rate 0.000000%\n",
      "At minibatch 34700, batch loss 0.550125, batch nll 0.217478, batch error rate 4.000000%\n",
      "At minibatch 34800, batch loss 0.488265, batch nll 0.155621, batch error rate 0.000000%\n",
      "At minibatch 34900, batch loss 0.399955, batch nll 0.067310, batch error rate 0.000000%\n",
      "At minibatch 35000, batch loss 0.570050, batch nll 0.237414, batch error rate 8.000000%\n",
      "At minibatch 35100, batch loss 0.545790, batch nll 0.213136, batch error rate 8.000000%\n",
      "At minibatch 35200, batch loss 0.519594, batch nll 0.186905, batch error rate 4.000000%\n",
      "After epoch 22: valid_err_rate: 21.220000% currently going to do 25 epochs\n",
      "After epoch 22: averaged train_err_rate: 3.850000% averaged train nll: 0.154649 averaged train loss: 0.487111\n",
      "At minibatch 35300, batch loss 0.518930, batch nll 0.186282, batch error rate 4.000000%\n",
      "At minibatch 35400, batch loss 0.386809, batch nll 0.054210, batch error rate 0.000000%\n",
      "At minibatch 35500, batch loss 0.537993, batch nll 0.205433, batch error rate 12.000000%\n",
      "At minibatch 35600, batch loss 0.480361, batch nll 0.147836, batch error rate 8.000000%\n",
      "At minibatch 35700, batch loss 0.425467, batch nll 0.092933, batch error rate 4.000000%\n",
      "At minibatch 35800, batch loss 0.452668, batch nll 0.120111, batch error rate 4.000000%\n",
      "At minibatch 35900, batch loss 0.381245, batch nll 0.048646, batch error rate 0.000000%\n",
      "At minibatch 36000, batch loss 0.433180, batch nll 0.100534, batch error rate 0.000000%\n",
      "At minibatch 36100, batch loss 0.404113, batch nll 0.071509, batch error rate 0.000000%\n",
      "At minibatch 36200, batch loss 0.388928, batch nll 0.056281, batch error rate 0.000000%\n",
      "At minibatch 36300, batch loss 0.491963, batch nll 0.159346, batch error rate 8.000000%\n",
      "At minibatch 36400, batch loss 0.436158, batch nll 0.103550, batch error rate 0.000000%\n",
      "At minibatch 36500, batch loss 0.493155, batch nll 0.160519, batch error rate 0.000000%\n",
      "At minibatch 36600, batch loss 0.451008, batch nll 0.118348, batch error rate 4.000000%\n",
      "At minibatch 36700, batch loss 0.458464, batch nll 0.125651, batch error rate 4.000000%\n",
      "At minibatch 36800, batch loss 0.404394, batch nll 0.071553, batch error rate 0.000000%\n",
      "After epoch 23: valid_err_rate: 21.050000% currently going to do 25 epochs\n",
      "After epoch 23: averaged train_err_rate: 3.312500% averaged train nll: 0.142719 averaged train loss: 0.475343\n",
      "At minibatch 36900, batch loss 0.430924, batch nll 0.098105, batch error rate 0.000000%\n",
      "At minibatch 37000, batch loss 0.531482, batch nll 0.198655, batch error rate 4.000000%\n",
      "At minibatch 37100, batch loss 0.493814, batch nll 0.161010, batch error rate 8.000000%\n",
      "At minibatch 37200, batch loss 0.438833, batch nll 0.106098, batch error rate 0.000000%\n",
      "At minibatch 37300, batch loss 0.429469, batch nll 0.096755, batch error rate 0.000000%\n",
      "At minibatch 37400, batch loss 0.411460, batch nll 0.078750, batch error rate 0.000000%\n",
      "At minibatch 37500, batch loss 0.458822, batch nll 0.126081, batch error rate 0.000000%\n",
      "At minibatch 37600, batch loss 0.434979, batch nll 0.102242, batch error rate 0.000000%\n",
      "At minibatch 37700, batch loss 0.429973, batch nll 0.097210, batch error rate 4.000000%\n",
      "At minibatch 37800, batch loss 0.391893, batch nll 0.059155, batch error rate 0.000000%\n",
      "At minibatch 37900, batch loss 0.472239, batch nll 0.139392, batch error rate 4.000000%\n",
      "At minibatch 38000, batch loss 0.380024, batch nll 0.047217, batch error rate 0.000000%\n",
      "At minibatch 38100, batch loss 0.416373, batch nll 0.083522, batch error rate 0.000000%\n",
      "At minibatch 38200, batch loss 0.421616, batch nll 0.088803, batch error rate 4.000000%\n",
      "At minibatch 38300, batch loss 0.481719, batch nll 0.148954, batch error rate 4.000000%\n",
      "At minibatch 38400, batch loss 0.440665, batch nll 0.107852, batch error rate 0.000000%\n",
      "After epoch 24: valid_err_rate: 20.600000% currently going to do 25 epochs\n",
      "After epoch 24: averaged train_err_rate: 3.012500% averaged train nll: 0.133198 averaged train loss: 0.465981\n",
      "At minibatch 38500, batch loss 0.423866, batch nll 0.091055, batch error rate 0.000000%\n",
      "At minibatch 38600, batch loss 0.397971, batch nll 0.065295, batch error rate 0.000000%\n",
      "At minibatch 38700, batch loss 0.572334, batch nll 0.239691, batch error rate 8.000000%\n",
      "At minibatch 38800, batch loss 0.575944, batch nll 0.243312, batch error rate 8.000000%\n",
      "At minibatch 38900, batch loss 0.726682, batch nll 0.394146, batch error rate 20.000000%\n",
      "At minibatch 39000, batch loss 0.426038, batch nll 0.093515, batch error rate 0.000000%\n",
      "At minibatch 39100, batch loss 0.402129, batch nll 0.069559, batch error rate 0.000000%\n",
      "At minibatch 39200, batch loss 0.411395, batch nll 0.078869, batch error rate 0.000000%\n",
      "At minibatch 39300, batch loss 0.397079, batch nll 0.064547, batch error rate 0.000000%\n",
      "At minibatch 39400, batch loss 0.541843, batch nll 0.209345, batch error rate 4.000000%\n",
      "At minibatch 39500, batch loss 0.535151, batch nll 0.202637, batch error rate 4.000000%\n",
      "At minibatch 39600, batch loss 0.380646, batch nll 0.048144, batch error rate 0.000000%\n",
      "At minibatch 39700, batch loss 0.527807, batch nll 0.195277, batch error rate 0.000000%\n",
      "At minibatch 39800, batch loss 0.388499, batch nll 0.055966, batch error rate 0.000000%\n",
      "At minibatch 39900, batch loss 0.405843, batch nll 0.073307, batch error rate 0.000000%\n",
      "At minibatch 40000, batch loss 0.498419, batch nll 0.165849, batch error rate 0.000000%\n",
      "After epoch 25: valid_err_rate: 21.540000% currently going to do 25 epochs\n",
      "After epoch 25: averaged train_err_rate: 2.712500% averaged train nll: 0.126705 averaged train loss: 0.459281\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "while e < number_of_epochs: # This loop goes over epochs\n",
    "    e += 1\n",
    "    \n",
    "    # First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar10_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = params[\"K\"]\n",
    "        lrate = params[\"lrate_const\"] * K / np.maximum(K, i)    # was 4e-3 * ...\n",
    "        momentum = params[\"momentum\"]\n",
    "        \n",
    "        # \n",
    "        if np.random.randint(2) == 1:\n",
    "            X_batch = X_batch[:, :, :, : : -1]\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        # print [p.get_value().ravel()[: 10] for p in model_parameters]\n",
    "        # print [p.get_value().ravel()[: 10] for p in velocities]\n",
    "        \n",
    "        train_loss.append((i, L))\n",
    "        train_erros.append((i, err_rate))\n",
    "        train_nll.append((i, nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate * 100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar10_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion + 1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i, val_error_rate))\n",
    "    \n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" \\\n",
    "          % (e,val_error_rate * 100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" \\\n",
    "          % (e,\n",
    "             np.mean(np.asarray(train_erros)[epoch_start_i :, 1]) * 100, \n",
    "             np.mean(np.asarray(train_nll)[epoch_start_i :, 1]),\n",
    "             np.mean(np.asarray(train_loss)[epoch_start_i :, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 16\n",
      "Test error rate is 22.090000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2cb986e750>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2cdce5fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" % (best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" % (compute_error_rate(cifar10_test_stream) * 100.0,)\n",
    "\n",
    "subplot(2, 1, 1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:, 0], train_nll_a[:, 1], label = \"batch train nll\")\n",
    "legend()\n",
    "\n",
    "subplot(2, 1, 2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:, 0], train_erros_a[:, 1], label = \"batch train error rate\")\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:, 0], validation_errors_a[:, 1], label = \"validation error rate\", color = \"r\")\n",
    "ylim(0, 0.2)\n",
    "legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results:\n",
      "\n",
      " momentum | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
      "==========+=============++=================+============+===========+================+====================+\n",
      " 0.9      |  2e-3 * ... ||      34.410000% |         27 |        42 |     34.250000% |         20.372500% |\n",
      "          |  4e-3 * ... ||      32.910000% |         15 |        24 |     33.830000% |          9.650000% |\n",
      "          |  8e-3 * ... ||      32.050000% |         14 |        22 |     30.990000% |          0.352500% |\n",
      "          | 12e-3 * ... ||      32.450000% |         10 |        16 |     32.400000% |          1.422500% |\n",
      "          | 16e-3 * ... ||      31.460000% |          9 |        15 |     31.190000% |          2.002500% |\n",
      "          | 20e-3 * ... ||      31.210000% |         31 |        48 |     30.280000% |          0.005000% |\n",
      "          | 24e-3 * ... ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
      "          | 28e-3 * ... ||      31.240000% |         33 |        51 |     30.550000% |          0.010000% |\n",
      "          | 32e-3 * ... ||      32.760000% |         11 |        18 |     32.590000% |          5.222500% |\n",
      "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
      " 0.7      | 24e-3 * ... ||      31.470000% |          9 |        14 |     31.230000% |          4.807500% |\n",
      " 0.75     |             ||      31.410000% |         16 |        25 |     31.010000% |          0.082500% |\n",
      " 0.8      |             ||      30.950000% |          9 |        14 |     30.980000% |          1.715000% |\n",
      " 0.85     |             ||      31.090000% |         26 |        40 |     31.340000% |          0.007500% |\n",
      " 0.9      |             ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
      "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ''' Results:\n",
    "\n",
    " momentum | lrate       || test error rate | best epoch | max epoch | valid_err_rate | avg train_err_rate |\n",
    "==========+=============++=================+============+===========+================+====================+\n",
    " 0.9      |  2e-3 * ... ||      34.410000% |         27 |        42 |     34.250000% |         20.372500% |\n",
    "          |  4e-3 * ... ||      32.910000% |         15 |        24 |     33.830000% |          9.650000% |\n",
    "          |  8e-3 * ... ||      32.050000% |         14 |        22 |     30.990000% |          0.352500% |\n",
    "          | 12e-3 * ... ||      32.450000% |         10 |        16 |     32.400000% |          1.422500% |\n",
    "          | 16e-3 * ... ||      31.460000% |          9 |        15 |     31.190000% |          2.002500% |\n",
    "          | 20e-3 * ... ||      31.210000% |         31 |        48 |     30.280000% |          0.005000% |\n",
    "          | 24e-3 * ... ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "          | 28e-3 * ... ||      31.240000% |         33 |        51 |     30.550000% |          0.010000% |\n",
    "          | 32e-3 * ... ||      32.760000% |         11 |        18 |     32.590000% |          5.222500% |\n",
    "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
    " 0.7      | 24e-3 * ... ||      31.470000% |          9 |        14 |     31.230000% |          4.807500% |\n",
    " 0.75     |             ||      31.410000% |         16 |        25 |     31.010000% |          0.082500% |\n",
    " 0.8      |             ||      30.950000% |          9 |        14 |     30.980000% |          1.715000% |\n",
    " 0.85     |             ||      31.090000% |         26 |        40 |     31.340000% |          0.007500% |\n",
    " 0.9      |             ||      31.160000% |         26 |        40 |     31.480000% |          0.047500% |\n",
    "----------+-------------++-----------------+------------+-----------+----------------+--------------------+\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_fw3_hidden': 500, 'K': 3000, 'gauss': 0.025, 'momentum': 0.8, 'lrate_const': 0.024, 'num_filters_1': 50, 'num_filters_2': 75}\n"
     ]
    }
   ],
   "source": [
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Results:\n",
      "\n",
      " K    | lrate_const | momentum | num_filters_1 | num_filters_2 | num_fw3_hidden | gauss || test error rate\n",
      "======+=============+==========+===============+===============+================+=======++=================\n",
      " 2000 |       0.004 |      0.9 |            10 |            25 |            500 | 0.05  ||        original\n",
      "------+-------------+----------+---------------+---------------+----------------+-------++-----------------\n",
      "      |       0.024 |      0.8 |               |            35 |                |       ||      30.080000%\n",
      "      |       0.024 |      0.8 |            15 |            50 |                |       ||      28.350000%\n",
      "      |       0.024 |      0.8 |            15 |            50 |                | 0.025 ||      27.530000%\n",
      "      |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.910000%\n",
      " 4000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      26.030000%\n",
      " 3000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.690000%\n",
      " 3000 |       0.024 |      0.8 |            50 |            75 |                | 0.025 ||      25.340000%\n"
     ]
    }
   ],
   "source": [
    "print ''' Results:\n",
    "\n",
    " K    | lrate_const | momentum | num_filters_1 | num_filters_2 | num_fw3_hidden | gauss || test error rate\n",
    "======+=============+==========+===============+===============+================+=======++=================\n",
    " 2000 |       0.004 |      0.9 |            10 |            25 |            500 | 0.05  ||        original\n",
    "------+-------------+----------+---------------+---------------+----------------+-------++-----------------\n",
    "      |       0.024 |      0.8 |               |            35 |                |       ||      30.080000%\n",
    "      |       0.024 |      0.8 |            15 |            50 |                |       ||      28.350000%\n",
    "      |       0.024 |      0.8 |            15 |            50 |                | 0.025 ||      27.530000%\n",
    "      |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.910000%\n",
    " 4000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      26.030000%\n",
    " 3000 |       0.024 |      0.8 |            50 |            50 |                | 0.025 ||      25.690000%\n",
    " 3000 |       0.024 |      0.8 |            50 |            75 |                | 0.025 ||      25.340000%'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving the results of the label clasification.\n",
    "results = []\n",
    "labels  = []\n",
    "for X, Y in cifar10_train_stream.get_epoch_iterator():\n",
    "    results += list(predict(X).ravel())\n",
    "    labels  += list(Y.ravel())\n",
    "    \n",
    "with open(\"listo_KIPE10.txt\", \"w\") as listing:\n",
    "    for result, label in zip(results, labels):\n",
    "        listing.write(str(result) + \" \" + str(label) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
