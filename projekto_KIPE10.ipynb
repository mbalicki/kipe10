{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n",
      "Vendor:  Continuum Analytics, Inc.\n",
      "Package: mkl\n",
      "Message: trial mode expires in 29 days\n"
     ]
    }
   ],
   "source": [
    "% pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 780\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "import theano.tensor.signal.downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We will now build a convolutional network for the CIFAR-10 data. We will use Theano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.cifar10 import CIFAR10\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "CIFAR10.default_transformers = ((ScaleAndShift, [2.0 / 255.0, -1], {\"which_sources\" : \"features\"}),\n",
    "                                (Cast, [np.float32], {\"which_sources\" : \"features\"}))\n",
    "\n",
    "cifar10_train = CIFAR10((\"train\",), subset = slice(None, 40000))\n",
    "# this stream will shuffle the CIFAR-10 set and return us batches of 100 examples\n",
    "cifar10_train_stream = DataStream.default_stream(cifar10_train,\n",
    "                                                 iteration_scheme = ShuffledScheme(cifar10_train.num_examples, 25))\n",
    "                                               \n",
    "cifar10_validation = CIFAR10((\"train\",), subset = slice(40000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these don't do a backward pass and reauire less RAM.\n",
    "cifar10_validation_stream = DataStream.default_stream(cifar10_validation,\n",
    "                                                      iteration_scheme = SequentialScheme(cifar10_validation.num_examples, 100))\n",
    "cifar10_test = CIFAR10((\"test\",))\n",
    "cifar10_test_stream = DataStream.default_stream(cifar10_test,\n",
    "                                                iteration_scheme = SequentialScheme(cifar10_test.num_examples, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (25, 3, 32, 32) containing float32\n",
      " - an array of size (25, 1) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (100, 3, 32, 32) containing float32\n",
      " - an array of size (100, 1) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (cifar10_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(cifar10_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(cifar10_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are taken from https://github.com/mila-udem/blocks.\n",
    "\n",
    "class Constant():\n",
    "    '''Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    '''\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype = np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    '''Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    '''\n",
    "    def __init__(self, std = 1, mean = 0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size = shape)\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    '''Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width / 2, mean + width / 2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    '''\n",
    "    def __init__(self, mean = 0., width = None, std = None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1 / 12 * width ^ 2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size = shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (3, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# A theano variable is an entry to the cmputational graph\n",
    "# We will need to provide its value during function call\n",
    "# X is batch_size x num_channels x img_rows x img_columns\n",
    "X = theano.tensor.tensor4(\"X\")\n",
    "\n",
    "# Y is 1D, it lists the targets for all examples\n",
    "Y = theano.tensor.matrix(\"Y\", dtype = \"uint8\")\n",
    "\n",
    "# The tag values are useful during debugging the creation of Theano graphs\n",
    "\n",
    "X_test_value, Y_test_value = next(cifar10_train_stream.get_epoch_iterator())\n",
    "\n",
    "# Unfortunately, test tags don't work with convolutions with newest Theano :(\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values\n",
    "\n",
    "\n",
    "X.tag.test_value = X_test_value[: 3]\n",
    "Y.tag.test_value = Y_test_value[: 3]\n",
    "\n",
    "print \"X shape: %s\" % (X.tag.test_value.shape,)\n",
    "\n",
    "# this list will hold all parameters of the network\n",
    "model_parameters = []\n",
    "\n",
    "# The first convolutional layer\n",
    "# The shape is: num_out_filters x num_in_filters x filter_height x filter_width\n",
    "num_filters_1 = 10 # we will apply that many convolution filters in the first layer\n",
    "CW1 = theano.shared(np.zeros((num_filters_1, 3, 5, 5), dtype = \"float32\"),\n",
    "                    name = \"CW1\")\n",
    "# please note - this is somewhat non-standard\n",
    "CW1.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB1 = theano.shared(np.zeros((num_filters_1,), dtype = \"float32\"),\n",
    "                    name = \"CB1\")\n",
    "CB1.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW1, CB1]\n",
    "\n",
    "after_C1 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(X, CW1) + CB1.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C1 shape: %s\" % (after_C1.tag.test_value.shape,)\n",
    "after_P1 = theano.tensor.signal.downsample.max_pool_2d(after_C1, (2, 2), ignore_border = True)\n",
    "# print \"after_P1 shape: %s\" % (after_P1.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_filters_2 = 25 # we will compute ten convolution filters in the first layer\n",
    "CW2 = theano.shared(np.zeros((num_filters_2, num_filters_1, 5, 5), dtype = \"float32\"),\n",
    "                   name = \"CW2\")\n",
    "CW2.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "CB2 = theano.shared(np.zeros((num_filters_2,), dtype = \"float32\"),\n",
    "                    name = \"CB2\")\n",
    "CB2.tag.initializer = Constant(0.0)\n",
    "model_parameters += [CW2, CB2]\n",
    "\n",
    "after_C2 = theano.tensor.maximum(0.0,\n",
    "                                 theano.tensor.nnet.conv2d(after_P1, CW2) + CB2.dimshuffle(\"x\", 0, \"x\", \"x\"))\n",
    "# print \"after_C2 shape: %s\" % (after_C2.tag.test_value.shape,)\n",
    "after_P2 = theano.tensor.signal.downsample.max_pool_2d(after_C2, (2, 2), ignore_border = True)\n",
    "# print \"after_P2 shape: %s\" % (after_P2.tag.test_value.shape,)\n",
    "\n",
    "# Fully connected layers - we just flatten all filter maps\n",
    "num_fw3_hidden = 500\n",
    "FW3 = theano.shared(np.zeros((num_filters_2 * 5 * 5, num_fw3_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW3\")\n",
    "FW3.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB3 = theano.shared(np.zeros((num_fw3_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB3\")\n",
    "FB3.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW3, FB3]\n",
    "\n",
    "after_F3 = theano.tensor.maximum(0.0, \n",
    "                                 theano.tensor.dot(after_P2.flatten(2), FW3) + FB3.dimshuffle(\"x\", 0))\n",
    "# print \"after_F3 shape: %s\" % (after_F3.tag.test_value.shape,)\n",
    "\n",
    "\n",
    "num_fw4_hidden = 10\n",
    "FW4 = theano.shared(np.zeros((num_fw3_hidden, num_fw4_hidden), dtype = \"float32\"),\n",
    "                    name = \"FW4\")\n",
    "FW4.tag.initializer = IsotropicGaussian(0.05)\n",
    "\n",
    "FB4 = theano.shared(np.zeros((num_fw4_hidden,), dtype = \"float32\"),\n",
    "                    name = \"FB4\")\n",
    "FB4.tag.initializer = Constant(0.0)\n",
    "model_parameters += [FW4, FB4]\n",
    "\n",
    "after_F4 = theano.tensor.dot(after_F3, FW4) + FB4.dimshuffle(\"x\", 0)\n",
    "# print \"after_F4 shape: %s\" % (after_F4.tag.test_value.shape,)\n",
    "\n",
    "log_probs = theano.tensor.nnet.softmax(after_F4)\n",
    "\n",
    "predictions = theano.tensor.argmax(log_probs, axis = 1)\n",
    "\n",
    "error_rate = theano.tensor.neq(predictions, Y.ravel()).mean()\n",
    "nll = -theano.tensor.log(log_probs[theano.tensor.arange(Y.shape[0]), Y.ravel()]).mean()\n",
    "\n",
    "weight_decay = 0.0\n",
    "for p in model_parameters:\n",
    "    if p.name[1] == \"W\":\n",
    "        weight_decay = weight_decay + 1e-3 * (p ** 2).sum()\n",
    "\n",
    "cost = nll + weight_decay\n",
    "\n",
    "# At this point stop computing test values\n",
    "theano.config.compute_test_value = \"off\" # Enable the computation of test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# We have built a computation graph for computing the error_rate, predictions and cost\n",
    "#\n",
    "# svgdotprint(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The updates will update our shared values\n",
    "updates = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrate = theano.tensor.scalar(\"lrate\", dtype = \"float32\")\n",
    "momentum = theano.tensor.scalar(\"momentum\", dtype = \"float32\")\n",
    "\n",
    "# Theano will compute the gradients for us\n",
    "gradients = theano.grad(cost, model_parameters)\n",
    "\n",
    "#initialize storage for momentum\n",
    "velocities = [theano.shared(np.zeros_like(p.get_value()), name = \"V_%s\" % (p.name,)) for p in model_parameters]\n",
    "\n",
    "for p, g, v in zip(model_parameters, gradients, velocities):\n",
    "    v_new = momentum * v - lrate * g\n",
    "    p_new = p + v_new\n",
    "    updates += [(v, v_new), (p, p_new)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(V_CW1, Elemwise{sub,no_inplace}.0),\n",
       " (CW1, Elemwise{add,no_inplace}.0),\n",
       " (V_CB1, Elemwise{sub,no_inplace}.0),\n",
       " (CB1, Elemwise{add,no_inplace}.0),\n",
       " (V_CW2, Elemwise{sub,no_inplace}.0),\n",
       " (CW2, Elemwise{add,no_inplace}.0),\n",
       " (V_CB2, Elemwise{sub,no_inplace}.0),\n",
       " (CB2, Elemwise{add,no_inplace}.0),\n",
       " (V_FW3, Elemwise{sub,no_inplace}.0),\n",
       " (FW3, Elemwise{add,no_inplace}.0),\n",
       " (V_FB3, Elemwise{sub,no_inplace}.0),\n",
       " (FB3, Elemwise{add,no_inplace}.0),\n",
       " (V_FW4, Elemwise{sub,no_inplace}.0),\n",
       " (FW4, Elemwise{add,no_inplace}.0),\n",
       " (V_FB4, Elemwise{sub,no_inplace}.0),\n",
       " (FB4, Elemwise{add,no_inplace}.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compile theano functions\n",
    "\n",
    "# each call to train step will make one SGD step\n",
    "train_step = theano.function([X, Y, lrate, momentum], [cost, error_rate, nll, weight_decay], updates = updates)\n",
    "# each call to predict will return predictions on a batch of data\n",
    "predict = theano.function([X], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_error_rate(stream):\n",
    "    errs = 0.0\n",
    "    num_samples = 0.0\n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        errs += (predict(X) != Y.ravel()).sum()\n",
    "        num_samples += Y.shape[0]\n",
    "    return errs / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utilities to save values of parameters and to load them\n",
    "\n",
    "def init_parameters():\n",
    "    rng = np.random.RandomState(1234)\n",
    "    for p in model_parameters:\n",
    "        p.set_value(p.tag.initializer.generate(rng, p.get_value().shape))\n",
    "\n",
    "def snapshot_parameters():\n",
    "    return [p.get_value(borrow = False) for p in model_parameters]\n",
    "\n",
    "def load_parameters(snapshot):\n",
    "    for p, s in zip(model_parameters, snapshot):\n",
    "        p.set_value(s, borrow = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init training\n",
    "\n",
    "i = 0\n",
    "e = 0\n",
    "\n",
    "init_parameters()\n",
    "for v in velocities:\n",
    "    v.set_value(np.zeros_like(v.get_value()))\n",
    "\n",
    "best_valid_error_rate = np.inf\n",
    "best_params = snapshot_parameters()\n",
    "best_params_epoch = 0\n",
    "\n",
    "train_erros = []\n",
    "train_loss = []\n",
    "train_nll = []\n",
    "validation_errors = []\n",
    "\n",
    "number_of_epochs = 3\n",
    "patience_expansion = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At minibatch 100, batch loss 2.598649, batch nll 1.795059, batch error rate 68.000000%\n",
      "At minibatch 200, batch loss 2.639572, batch nll 1.846523, batch error rate 76.000000%\n",
      "At minibatch 300, batch loss 2.579576, batch nll 1.796930, batch error rate 68.000000%\n",
      "At minibatch 400, batch loss 2.497492, batch nll 1.725067, batch error rate 56.000000%\n",
      "At minibatch 500, batch loss 2.169885, batch nll 1.407095, batch error rate 52.000000%\n",
      "At minibatch 600, batch loss 2.251184, batch nll 1.498452, batch error rate 56.000000%\n",
      "At minibatch 700, batch loss 2.150898, batch nll 1.408156, batch error rate 52.000000%\n",
      "At minibatch 800, batch loss 2.376138, batch nll 1.643391, batch error rate 72.000000%\n",
      "At minibatch 900, batch loss 2.196768, batch nll 1.473386, batch error rate 52.000000%\n",
      "At minibatch 1000, batch loss 2.384565, batch nll 1.670486, batch error rate 60.000000%\n",
      "At minibatch 1100, batch loss 2.158177, batch nll 1.453521, batch error rate 60.000000%\n",
      "At minibatch 1200, batch loss 1.974849, batch nll 1.278994, batch error rate 56.000000%\n",
      "At minibatch 1300, batch loss 2.246030, batch nll 1.559151, batch error rate 56.000000%\n",
      "At minibatch 1400, batch loss 1.910049, batch nll 1.231688, batch error rate 44.000000%\n",
      "At minibatch 1500, batch loss 1.986524, batch nll 1.317256, batch error rate 48.000000%\n",
      "At minibatch 1600, batch loss 1.663326, batch nll 1.001924, batch error rate 28.000000%\n",
      "After epoch 1: valid_err_rate: 48.190000% currently going to do 3 epochs\n",
      "After epoch 1: averaged train_err_rate: 57.590000% averaged train nll: 1.588400 averaged train loss: 2.322931\n",
      "At minibatch 1700, batch loss 2.165269, batch nll 1.512062, batch error rate 56.000000%\n",
      "At minibatch 1800, batch loss 1.882330, batch nll 1.237244, batch error rate 40.000000%\n",
      "At minibatch 1900, batch loss 1.903960, batch nll 1.266734, batch error rate 40.000000%\n",
      "At minibatch 2000, batch loss 2.180788, batch nll 1.551104, batch error rate 44.000000%\n",
      "At minibatch 2100, batch loss 1.718396, batch nll 1.096386, batch error rate 52.000000%\n",
      "At minibatch 2200, batch loss 2.000684, batch nll 1.385163, batch error rate 60.000000%\n",
      "At minibatch 2300, batch loss 2.178049, batch nll 1.569898, batch error rate 60.000000%\n",
      "At minibatch 2400, batch loss 1.865560, batch nll 1.263582, batch error rate 48.000000%\n",
      "At minibatch 2500, batch loss 2.653653, batch nll 2.058042, batch error rate 64.000000%\n",
      "At minibatch 2600, batch loss 1.957780, batch nll 1.367602, batch error rate 44.000000%\n",
      "At minibatch 2700, batch loss 1.830175, batch nll 1.245453, batch error rate 48.000000%\n",
      "At minibatch 2800, batch loss 1.494928, batch nll 0.915394, batch error rate 32.000000%\n",
      "At minibatch 2900, batch loss 2.129297, batch nll 1.554665, batch error rate 44.000000%\n",
      "At minibatch 3000, batch loss 1.667355, batch nll 1.097500, batch error rate 48.000000%\n",
      "At minibatch 3100, batch loss 2.117250, batch nll 1.551454, batch error rate 44.000000%\n",
      "At minibatch 3200, batch loss 1.607157, batch nll 1.045866, batch error rate 40.000000%\n",
      "After epoch 2: valid_err_rate: 41.790000% currently going to do 4 epochs\n",
      "After epoch 2: averaged train_err_rate: 44.592500% averaged train nll: 1.251725 averaged train loss: 1.856961\n",
      "At minibatch 3300, batch loss 1.534973, batch nll 0.977338, batch error rate 40.000000%"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "while e < number_of_epochs: # This loop goes over epochs\n",
    "    e += 1\n",
    "    # First train on all data from this batch\n",
    "    epoch_start_i = i\n",
    "    for X_batch, Y_batch in cifar10_train_stream.get_epoch_iterator(): \n",
    "        i += 1\n",
    "        \n",
    "        K = 2000\n",
    "        lrate = 4e-3 * K / np.maximum(K, i)\n",
    "        momentum = 0.9\n",
    "        \n",
    "        L, err_rate, nll, wdec = train_step(X_batch, Y_batch, lrate, momentum)\n",
    "        \n",
    "        # print [p.get_value().ravel()[: 10] for p in model_parameters]\n",
    "        # print [p.get_value().ravel()[: 10] for p in velocities]\n",
    "        \n",
    "        train_loss.append((i, L))\n",
    "        train_erros.append((i, err_rate))\n",
    "        train_nll.append((i, nll))\n",
    "        if i % 100 == 0:\n",
    "            print \"At minibatch %d, batch loss %f, batch nll %f, batch error rate %f%%\" % (i, L, nll, err_rate * 100)\n",
    "        \n",
    "    # After an epoch compute validation error\n",
    "    val_error_rate = compute_error_rate(cifar10_validation_stream)\n",
    "    if val_error_rate < best_valid_error_rate:\n",
    "        number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion + 1)\n",
    "        best_valid_error_rate = val_error_rate\n",
    "        best_params = snapshot_parameters()\n",
    "        best_params_epoch = e\n",
    "    validation_errors.append((i, val_error_rate))\n",
    "    print \"After epoch %d: valid_err_rate: %f%% currently going to do %d epochs\" \\\n",
    "          % (e,val_error_rate * 100, number_of_epochs)\n",
    "    print \"After epoch %d: averaged train_err_rate: %f%% averaged train nll: %f averaged train loss: %f\" \\\n",
    "          % (e,\n",
    "             np.mean(np.asarray(train_erros)[epoch_start_i :, 1]) * 100, \n",
    "             np.mean(np.asarray(train_nll)[epoch_start_i :, 1]),\n",
    "             np.mean(np.asarray(train_loss)[epoch_start_i :, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting network parameters from after epoch 31\n",
      "Test error rate is 31.210000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa0440d8710>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEDCAYAAADayhiNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBkFwzAMI8T/B4YbtwsQgekIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa05e41a990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Setting network parameters from after epoch %d\" % (best_params_epoch)\n",
    "load_parameters(best_params)\n",
    "\n",
    "print \"Test error rate is %f%%\" % (compute_error_rate(cifar10_test_stream) * 100.0,)\n",
    "\n",
    "subplot(2, 1, 1)\n",
    "train_nll_a = np.array(train_nll)\n",
    "semilogy(train_nll_a[:, 0], train_nll_a[:, 1], label = \"batch train nll\")\n",
    "legend()\n",
    "\n",
    "subplot(2, 1, 2)\n",
    "train_erros_a = np.array(train_erros)\n",
    "plot(train_erros_a[:, 0], train_erros_a[:, 1], label = \"batch train error rate\")\n",
    "validation_errors_a = np.array(validation_errors)\n",
    "plot(validation_errors_a[:, 0], validation_errors_a[:, 1], label = \"validation error rate\", color = \"r\")\n",
    "ylim(0, 0.2)\n",
    "legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
